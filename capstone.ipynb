{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK2gE4IYpFqu",
        "outputId": "8a6c2810-2cb8-480a-ddb1-b8cf7176cfbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#metrics"
      ],
      "metadata": {
        "id": "eKetU_kznosl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OOyT5H-zmq5-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + 1e-15) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-15)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return dice_loss(y_true, y_pred) + tf.keras.losses.binary_crossentropy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sparse attention"
      ],
      "metadata": {
        "id": "oKC0lspJn3Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Dropout\n",
        "\n",
        "class SparseAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, key_dim, num_heads=1,regularization_coeff=0.01):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.layer_norm1 = None\n",
        "        self.layer_norm2 = None\n",
        "        self.output_dense = None\n",
        "        self.v_dense = None\n",
        "        self.k_dense = None\n",
        "        self.q_dense = None\n",
        "        self.key_dim = key_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.regularization_coeff = regularization_coeff\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.q_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.k_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.v_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.output_dense = tf.keras.layers.Dense(input_shape[-1], kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        # Apply Linear Transformation for Query, Key and Value\n",
        "        q = Dropout(0.1)(self.q_dense(x))\n",
        "        k = Dropout(0.1)(self.q_dense(x))\n",
        "        v = Dropout(0.1)(self.q_dense(x))\n",
        "\n",
        "\n",
        "        # Split the heads\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "        # Calculate Attention Score\n",
        "        attn_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_score = attn_score / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
        "\n",
        "        # Apply Sparsemax\n",
        "        attn_score = self.sparsemax(attn_score)\n",
        "\n",
        "        # Apply Dropout after the Dense layer\n",
        "        attn_score = Dropout(0.1)(attn_score)\n",
        "        # Calculate the output value using attention score\n",
        "        attn_values = tf.matmul(attn_score, v)\n",
        "\n",
        "        # Combine the heads back\n",
        "        attn_values = self.combine_heads(attn_values)\n",
        "\n",
        "        # Apply Layer Normalization (First LayerNorm)\n",
        "        attn_values = self.layer_norm1(attn_values)\n",
        "\n",
        "        # Final Linear Transformation\n",
        "        output = self.output_dense(attn_values)\n",
        "\n",
        "        # Apply Layer Normalization (Second LayerNorm, if needed)\n",
        "        output = self.layer_norm2(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def sparsemax(self, logits, axis=-1):\n",
        "        logits = tf.convert_to_tensor(logits)\n",
        "        ob_dim = tf.shape(logits)[axis]\n",
        "        z = tf.sort(logits, axis=axis, direction='DESCENDING')\n",
        "        z_cumsum = tf.cumsum(z, axis=axis)\n",
        "        k = tf.range(1, ob_dim + 1, dtype=tf.float32)\n",
        "        z_check = 1 + k * z >= z_cumsum\n",
        "        k_max = tf.reduce_sum(tf.cast(z_check, tf.float32), axis=axis, keepdims=True)\n",
        "        z_max = tf.gather(z, tf.cast(k_max - 1, tf.int32), batch_dims=len(logits.shape) - 1)\n",
        "        out = tf.nn.relu(logits - z_max)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.key_dim))\n",
        "        return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.transpose(x, [0, 2, 1, 3])\n",
        "        return tf.reshape(x, (batch_size, -1, self.key_dim * self.num_heads))\n"
      ],
      "metadata": {
        "id": "ArI0wua-n2ch"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Se"
      ],
      "metadata": {
        "id": "PM9XDaRgoCQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, Add, Permute, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def squeeze_excite_block(input, ratio=16):\n",
        "    ''' Create a channel-wise squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def spatial_squeeze_excite_block(input):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    se = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False,\n",
        "                kernel_initializer='he_normal')(input)\n",
        "\n",
        "    x = Multiply([input, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def channel_spatial_squeeze_excite(input, ratio=16):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    cse = squeeze_excite_block(input, ratio)\n",
        "    sse = spatial_squeeze_excite_block(input)\n",
        "\n",
        "    x = Add([cse, sse])\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "csdWOzRcoB5d"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "f1ZIULAUm6TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape\n",
        "os.environ['SSL_CERT_DIR'] = '/etc/ssl/certs'\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Multiply, Add, BatchNormalization, Activation\n",
        "from keras.layers import Cropping2D,UpSampling2D, Input, Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def residual_block(x, num_filters):\n",
        "    x_init = x\n",
        "    x = Conv2D(num_filters//4, (1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters//4, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\" )(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    s = Conv2D(num_filters, (1, 1), padding=\"same\")(x_init)\n",
        "    s = BatchNormalization()(x)\n",
        "\n",
        "    x = Add()([x, s])\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_excite_block(x)\n",
        "    return x\n",
        "\n",
        "def NanoNet_A_with_Transformer(input_shape):\n",
        "    f = [32, 64, 128]\n",
        "    inputs = Input(shape=input_shape, name=\"input_image\")\n",
        "\n",
        "    # Encoder: MobileNetV2\n",
        "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.75)\n",
        "    encoder_output = encoder.get_layer(name=\"block_6_expand_relu\").output\n",
        "    skip_connections_name = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\"]\n",
        "\n",
        "    x = residual_block(encoder_output, 192)  # Residual Block\n",
        "\n",
        "\n",
        "    # SparseAttention Layer\n",
        "    transformer_shape = (x.shape[1], x.shape[2], x.shape[3])\n",
        "    x = Reshape((transformer_shape[0] * transformer_shape[1], transformer_shape[2]))(x)\n",
        "    x = SparseAttention(key_dim=transformer_shape[2], num_heads=2)(x)\n",
        "    x = Reshape((transformer_shape[0], transformer_shape[1], transformer_shape[2]))(x)\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(1, len(skip_connections_name) + 1, 1):\n",
        "        x_skip = encoder.get_layer(skip_connections_name[-i]).output\n",
        "        x_skip = Conv2D(f[-i], (1, 1), padding=\"same\")(x_skip)\n",
        "        x_skip = BatchNormalization()(x_skip)\n",
        "        x_skip = Activation(\"relu\")(x_skip)\n",
        "\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "\n",
        "        try:\n",
        "            x = Concatenate()([x, x_skip])\n",
        "        except Exception as e:\n",
        "            x = Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "            x = Concatenate()([x, x_skip])\n",
        "\n",
        "        x = residual_block(x, f[-i])\n",
        "    # Output layer\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    params = {\"img_height\": 256, \"img_width\": 256, \"img_channels\": 3, \"mask_channels\": 1}\n",
        "    input_shape = (params[\"img_height\"], params[\"img_width\"], params[\"img_channels\"])\n",
        "    model = NanoNet_A_with_Transformer(input_shape)\n",
        "    model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyi9W__ngsl",
        "outputId": "02757ed2-d39c-4171-9936-adfcc641581d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 24)         648       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 24)         96        ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 24)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 24)         216       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 24)         96        ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 24)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         384       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 24)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 24)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)         (None, 32, 32, 48)           6960      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_190 (B  (None, 32, 32, 48)           192       ['conv2d_200[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_160 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_190[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)         (None, 32, 32, 48)           20784     ['activation_160[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_191 (B  (None, 32, 32, 48)           192       ['conv2d_201[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_161 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_191[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)         (None, 32, 32, 192)          83136     ['activation_161[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_192 (B  (None, 32, 32, 192)          768       ['conv2d_202[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_193 (B  (None, 32, 32, 192)          768       ['batch_normalization_192[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_40 (Add)                (None, 32, 32, 192)          0         ['batch_normalization_192[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_193[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_162 (Activation  (None, 32, 32, 192)          0         ['add_40[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 192)                  0         ['activation_162[0][0]']      \n",
            " 0 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_60 (Reshape)        (None, 1, 1, 192)            0         ['global_average_pooling2d_40[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_80 (Dense)            (None, 1, 1, 12)             2304      ['reshape_60[0][0]']          \n",
            "                                                                                                  \n",
            " dense_81 (Dense)            (None, 1, 1, 192)            2304      ['dense_80[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_40 (Multiply)      (None, 32, 32, 192)          0         ['activation_162[0][0]',      \n",
            "                                                                     'dense_81[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_61 (Reshape)        (None, 1024, 192)            0         ['multiply_40[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_14 (Spars  (None, None, 192)            148800    ['reshape_61[0][0]']          \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_62 (Reshape)        (None, 32, 32, 192)          0         ['sparse_attention_14[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_194 (B  (None, 64, 64, 128)          512       ['conv2d_204[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_30 (UpSampli  (None, 64, 64, 192)          0         ['reshape_62[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_163 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_194[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenat  (None, 64, 64, 320)          0         ['up_sampling2d_30[0][0]',    \n",
            " e)                                                                  'activation_163[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)         (None, 64, 64, 32)           10272     ['concatenate_30[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_195 (B  (None, 64, 64, 32)           128       ['conv2d_205[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_164 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_195[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_164[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_196 (B  (None, 64, 64, 32)           128       ['conv2d_206[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_165 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_196[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_165[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_197 (B  (None, 64, 64, 128)          512       ['conv2d_207[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_198 (B  (None, 64, 64, 128)          512       ['batch_normalization_197[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_41 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_197[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_198[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_166 (Activation  (None, 64, 64, 128)          0         ['add_41[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 128)                  0         ['activation_166[0][0]']      \n",
            " 1 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_63 (Reshape)        (None, 1, 1, 128)            0         ['global_average_pooling2d_41[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_82 (Dense)            (None, 1, 1, 8)              1024      ['reshape_63[0][0]']          \n",
            "                                                                                                  \n",
            " dense_83 (Dense)            (None, 1, 1, 128)            1024      ['dense_82[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_41 (Multiply)      (None, 64, 64, 128)          0         ['activation_166[0][0]',      \n",
            "                                                                     'dense_83[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_199 (B  (None, 128, 128, 64)         256       ['conv2d_209[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_31 (UpSampli  (None, 128, 128, 128)        0         ['multiply_41[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_167 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_199[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_31[0][0]',    \n",
            " e)                                                                  'activation_167[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_31[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_200 (B  (None, 128, 128, 16)         64        ['conv2d_210[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_168 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_200[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_168[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_201 (B  (None, 128, 128, 16)         64        ['conv2d_211[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_169 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_201[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_169[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_202 (B  (None, 128, 128, 64)         256       ['conv2d_212[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_203 (B  (None, 128, 128, 64)         256       ['batch_normalization_202[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_42 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_202[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_203[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_170 (Activation  (None, 128, 128, 64)         0         ['add_42[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 64)                   0         ['activation_170[0][0]']      \n",
            " 2 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_64 (Reshape)        (None, 1, 1, 64)             0         ['global_average_pooling2d_42[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_84 (Dense)            (None, 1, 1, 4)              256       ['reshape_64[0][0]']          \n",
            "                                                                                                  \n",
            " dense_85 (Dense)            (None, 1, 1, 64)             256       ['dense_84[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_42 (Multiply)      (None, 128, 128, 64)         0         ['activation_170[0][0]',      \n",
            "                                                                     'dense_85[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_204 (B  (None, 256, 256, 32)         128       ['conv2d_214[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_32 (UpSampli  (None, 256, 256, 64)         0         ['multiply_42[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_171 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_204[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_32[0][0]',    \n",
            " e)                                                                  'activation_171[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_32[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_205 (B  (None, 256, 256, 8)          32        ['conv2d_215[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_172 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_205[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)         (None, 256, 256, 8)          584       ['activation_172[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_206 (B  (None, 256, 256, 8)          32        ['conv2d_216[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_173 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_206[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_173[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_207 (B  (None, 256, 256, 32)         128       ['conv2d_217[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_208 (B  (None, 256, 256, 32)         128       ['batch_normalization_207[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_43 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_207[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_208[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_174 (Activation  (None, 256, 256, 32)         0         ['add_43[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 32)                   0         ['activation_174[0][0]']      \n",
            " 3 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_65 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_43[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_86 (Dense)            (None, 1, 1, 2)              64        ['reshape_65[0][0]']          \n",
            "                                                                                                  \n",
            " dense_87 (Dense)            (None, 1, 1, 32)             64        ['dense_86[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_43 (Multiply)      (None, 256, 256, 32)         0         ['activation_174[0][0]',      \n",
            "                                                                     'dense_87[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_43[0][0]']         \n",
            "                                                                                                  \n",
            " activation_175 (Activation  (None, 256, 256, 1)          0         ['conv2d_219[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 420785 (1.61 MB)\n",
            "Trainable params: 414913 (1.58 MB)\n",
            "Non-trainable params: 5872 (22.94 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "yaVq7EoOvQFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from dimensionality_reduction import apply_pca_to_image,reduce_mask_dimension,save_image\n",
        "\n",
        "H = 256\n",
        "W = 256\n",
        "\n",
        "def load_names(path, file_path):\n",
        "    f = open(file_path, \"r\")\n",
        "    data = f.read().split(\"\\n\")[:-1]\n",
        "    images = [os.path.join(path, \"images\", name) + \".jpg\" for name in data]\n",
        "    masks = [os.path.join(path, \"masks\", name) + \".jpg\" for name in data]\n",
        "    return images, masks\n",
        "\n",
        "def load_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    valid_names_path = f\"{path}/val.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    valid_x, valid_y = load_names(path, valid_names_path)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y)\n",
        "\n",
        "def load_test_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    test_names_path = f\"{path}/test.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    test_x, test_y = load_names(path, test_names_path)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "def read_image(path):\n",
        "\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_image2(img_path):\n",
        "    return np.load(img_path)\n",
        "\n",
        "def read_mask2(mask_path):\n",
        "    return np.load(mask_path)\n",
        "\n",
        "def augment_data(image, mask):\n",
        "    # Random horizontal flip\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "\n",
        "    # Random vertical flip\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_up_down(image)\n",
        "        mask = tf.image.flip_up_down(mask)\n",
        "\n",
        "    # RGB to HSV\n",
        "    image_hsv = tf.image.rgb_to_hsv(image)\n",
        "\n",
        "    # Do some operations in HSV space, adjust saturation\n",
        "\n",
        "    delta = 0.2\n",
        "    image_hsv = tf.stack([\n",
        "        image_hsv[:, :, 0],  # Hue\n",
        "        tf.clip_by_value(image_hsv[:, :, 1] + delta, 0, 1),  # Saturation\n",
        "        image_hsv[:, :, 2],  # Value\n",
        "    ], axis=-1)\n",
        "\n",
        "    # Convert back to RGB\n",
        "    image_rgb = tf.image.hsv_to_rgb(image_hsv)\n",
        "\n",
        "    return image_rgb, mask\n",
        "\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    x, y = augment_data(x, y)\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch_size=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1000)  # Scramble data\n",
        "    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.cache()  # Cache data\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def process_dataset(image_paths, mask_paths, save_image_dir, save_mask_dir):\n",
        "    processed_image_paths = []\n",
        "    processed_mask_paths = []\n",
        "\n",
        "    for i, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
        "        img = read_image2(img_path)\n",
        "        mask = read_mask2(mask_path)\n",
        "\n",
        "        processed_img = apply_pca_to_image(img)\n",
        "        processed_mask = reduce_mask_dimension(mask)\n",
        "\n",
        "        processed_img_path = os.path.join(save_image_dir, f\"processed_image_{i}.jpg\")\n",
        "        processed_mask_path = os.path.join(save_mask_dir, f\"processed_mask_{i}.jpg\")\n",
        "\n",
        "        save_image(processed_img, processed_img_path)\n",
        "        save_image(processed_mask, processed_mask_path)\n",
        "\n",
        "        processed_image_paths.append(processed_img_path)\n",
        "        processed_mask_paths.append(processed_mask_path)\n",
        "\n",
        "    return processed_image_paths, processed_mask_paths\n",
        "\n"
      ],
      "metadata": {
        "id": "IYnZY6mivWl3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "OHOMge78ve6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import CustomObjectScope\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import load_model\n",
        "from keras.utils import custom_object_scope\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "    except OSError:\n",
        "        print(f\"Error: creating directory with name {path}\")\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def load_model_file(path):\n",
        "    with CustomObjectScope({\n",
        "            'iou': iou,\n",
        "            'dice_coef': dice_coef,\n",
        "            'dice_loss': dice_loss,\n",
        "            'bce_dice_loss': bce_dice_loss,\n",
        "            'SparseAttention': SparseAttention  #  SparseAttention\n",
        "        }):\n",
        "        model = tf.keras.models.load_model(path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "FkXytfLvvkPW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sgdr\n"
      ],
      "metadata": {
        "id": "URswsGAAv4pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "class SGDRScheduler(Callback):\n",
        "\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)\n"
      ],
      "metadata": {
        "id": "DFBfW-Gev7Lo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "4DLQuj1MvtK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from keras.metrics import Recall, Precision, MeanIoU\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    \"\"\" Remove folders and files \"\"\"\n",
        "    # os.system(\"rm files/files.csv\")\n",
        "    # os.system(\"rm -r logs\")\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    input_shape = (256, 256, 3)\n",
        "    batch_size = 8\n",
        "    lr = 1e-4\n",
        "    epochs = 200\n",
        "    model_name = \"A\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "    csv_path = f\"files/{model_name}/model.csv\"\n",
        "    log_path = f\"logs/{model_name}/\"\n",
        "    \"\"\" Creating folders \"\"\"\n",
        "    create_dir(f\"files/{model_name}\")\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    path = '/content/drive/MyDrive/capstone/Kvasir-SEG'\n",
        "\n",
        "    (train_x, train_y), (valid_x, valid_y) = load_data(path)\n",
        "\n",
        "    # processed_train_x, processed_train_y = process_dataset(\n",
        "    #     train_x, train_y,\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/train/images\",\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/train/masks\"\n",
        "    # )\n",
        "    # processed_valid_x, processed_valid_y = process_dataset(\n",
        "    #     valid_x, valid_y,\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/valid/images\",\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/valid/masks\"\n",
        "    # )\n",
        "\n",
        "    # train_dataset = tf_dataset(processed_train_x, processed_train_y, batch)\n",
        "    # valid_dataset = tf_dataset(processed_valid_x, processed_valid_y, batch)\n",
        "    #\n",
        "    # extractor = feature_extractor(input_shape)\n",
        "    # train_dataset = feature_extracted_tf_dataset(train_x, train_y, extractor, batch)\n",
        "    # valid_dataset = feature_extracted_tf_dataset(valid_x, valid_y, extractor, batch)\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
        "\n",
        "    # \"\"\" Model \"\"\"\n",
        "    model = NanoNet_A_with_Transformer(input_shape)\n",
        "\n",
        "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
        "    model.summary()\n",
        "\n",
        "    #\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(log_dir=log_path),\n",
        "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
        "    ]\n",
        "\n",
        "    train_steps = (len(train_x)//batch_size)\n",
        "    valid_steps = (len(valid_x)//batch_size)\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_steps += 1\n",
        "\n",
        "    model.fit(train_dataset,\n",
        "            epochs=epochs,\n",
        "            validation_data=valid_dataset,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_steps=valid_steps,\n",
        "            callbacks=callbacks,\n",
        "            shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejrVNs_vzm6",
        "outputId": "65249908-4e5a-4e1a-b035-61a045fc58f6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 24)         648       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 24)         96        ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 24)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 24)         216       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 24)         96        ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 24)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         384       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 24)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 24)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)         (None, 32, 32, 48)           6960      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_209 (B  (None, 32, 32, 48)           192       ['conv2d_220[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_176 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_209[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)         (None, 32, 32, 48)           20784     ['activation_176[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_210 (B  (None, 32, 32, 48)           192       ['conv2d_221[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_177 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_210[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)         (None, 32, 32, 192)          83136     ['activation_177[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_211 (B  (None, 32, 32, 192)          768       ['conv2d_222[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_212 (B  (None, 32, 32, 192)          768       ['batch_normalization_211[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_44 (Add)                (None, 32, 32, 192)          0         ['batch_normalization_211[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_212[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_178 (Activation  (None, 32, 32, 192)          0         ['add_44[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 192)                  0         ['activation_178[0][0]']      \n",
            " 4 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_66 (Reshape)        (None, 1, 1, 192)            0         ['global_average_pooling2d_44[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_88 (Dense)            (None, 1, 1, 12)             2304      ['reshape_66[0][0]']          \n",
            "                                                                                                  \n",
            " dense_89 (Dense)            (None, 1, 1, 192)            2304      ['dense_88[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_44 (Multiply)      (None, 32, 32, 192)          0         ['activation_178[0][0]',      \n",
            "                                                                     'dense_89[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_67 (Reshape)        (None, 1024, 192)            0         ['multiply_44[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_15 (Spars  (None, None, 192)            148800    ['reshape_67[0][0]']          \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_68 (Reshape)        (None, 32, 32, 192)          0         ['sparse_attention_15[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_213 (B  (None, 64, 64, 128)          512       ['conv2d_224[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_33 (UpSampli  (None, 64, 64, 192)          0         ['reshape_68[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_179 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_213[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenat  (None, 64, 64, 320)          0         ['up_sampling2d_33[0][0]',    \n",
            " e)                                                                  'activation_179[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)         (None, 64, 64, 32)           10272     ['concatenate_33[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_214 (B  (None, 64, 64, 32)           128       ['conv2d_225[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_180 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_214[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_180[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_215 (B  (None, 64, 64, 32)           128       ['conv2d_226[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_181 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_215[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_181[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_216 (B  (None, 64, 64, 128)          512       ['conv2d_227[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_217 (B  (None, 64, 64, 128)          512       ['batch_normalization_216[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_45 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_216[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_217[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_182 (Activation  (None, 64, 64, 128)          0         ['add_45[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 128)                  0         ['activation_182[0][0]']      \n",
            " 5 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_69 (Reshape)        (None, 1, 1, 128)            0         ['global_average_pooling2d_45[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_90 (Dense)            (None, 1, 1, 8)              1024      ['reshape_69[0][0]']          \n",
            "                                                                                                  \n",
            " dense_91 (Dense)            (None, 1, 1, 128)            1024      ['dense_90[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_229 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_45 (Multiply)      (None, 64, 64, 128)          0         ['activation_182[0][0]',      \n",
            "                                                                     'dense_91[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_218 (B  (None, 128, 128, 64)         256       ['conv2d_229[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_34 (UpSampli  (None, 128, 128, 128)        0         ['multiply_45[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_183 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_218[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_34[0][0]',    \n",
            " e)                                                                  'activation_183[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_230 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_34[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_219 (B  (None, 128, 128, 16)         64        ['conv2d_230[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_184 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_219[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_231 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_184[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_220 (B  (None, 128, 128, 16)         64        ['conv2d_231[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_185 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_220[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_232 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_185[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_221 (B  (None, 128, 128, 64)         256       ['conv2d_232[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_222 (B  (None, 128, 128, 64)         256       ['batch_normalization_221[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_46 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_221[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_222[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_186 (Activation  (None, 128, 128, 64)         0         ['add_46[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 64)                   0         ['activation_186[0][0]']      \n",
            " 6 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_70 (Reshape)        (None, 1, 1, 64)             0         ['global_average_pooling2d_46[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_92 (Dense)            (None, 1, 1, 4)              256       ['reshape_70[0][0]']          \n",
            "                                                                                                  \n",
            " dense_93 (Dense)            (None, 1, 1, 64)             256       ['dense_92[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_234 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_46 (Multiply)      (None, 128, 128, 64)         0         ['activation_186[0][0]',      \n",
            "                                                                     'dense_93[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_223 (B  (None, 256, 256, 32)         128       ['conv2d_234[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_35 (UpSampli  (None, 256, 256, 64)         0         ['multiply_46[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_187 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_223[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_35[0][0]',    \n",
            " e)                                                                  'activation_187[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_235 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_35[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_224 (B  (None, 256, 256, 8)          32        ['conv2d_235[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_188 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_224[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_236 (Conv2D)         (None, 256, 256, 8)          584       ['activation_188[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_225 (B  (None, 256, 256, 8)          32        ['conv2d_236[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_189 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_225[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_237 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_189[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_226 (B  (None, 256, 256, 32)         128       ['conv2d_237[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_227 (B  (None, 256, 256, 32)         128       ['batch_normalization_226[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_47 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_226[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_227[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_190 (Activation  (None, 256, 256, 32)         0         ['add_47[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 32)                   0         ['activation_190[0][0]']      \n",
            " 7 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_71 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_47[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_94 (Dense)            (None, 1, 1, 2)              64        ['reshape_71[0][0]']          \n",
            "                                                                                                  \n",
            " dense_95 (Dense)            (None, 1, 1, 32)             64        ['dense_94[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_47 (Multiply)      (None, 256, 256, 32)         0         ['activation_190[0][0]',      \n",
            "                                                                     'dense_95[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_239 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_47[0][0]']         \n",
            "                                                                                                  \n",
            " activation_191 (Activation  (None, 256, 256, 1)          0         ['conv2d_239[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 420785 (1.61 MB)\n",
            "Trainable params: 414913 (1.58 MB)\n",
            "Non-trainable params: 5872 (22.94 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.1813 - dice_coef: 0.3050 - iou: 0.1824 - recall_5: 0.6786 - precision_5: 0.2752\n",
            "Epoch 1: val_loss improved from inf to 4.65825, saving model to files/A/model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r93/93 [==============================] - 49s 238ms/step - loss: 5.1813 - dice_coef: 0.3050 - iou: 0.1824 - recall_5: 0.6786 - precision_5: 0.2752 - val_loss: 4.6583 - val_dice_coef: 0.2560 - val_iou: 0.1485 - val_recall_5: 0.9828 - val_precision_5: 0.1937 - lr: 1.0000e-04\n",
            "Epoch 2/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.0102 - dice_coef: 0.4455 - iou: 0.2907 - recall_5: 0.8262 - precision_5: 0.3701\n",
            "Epoch 2: val_loss improved from 4.65825 to 3.67758, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 4.0102 - dice_coef: 0.4455 - iou: 0.2907 - recall_5: 0.8262 - precision_5: 0.3701 - val_loss: 3.6776 - val_dice_coef: 0.3546 - val_iou: 0.2184 - val_recall_5: 0.7559 - val_precision_5: 0.3439 - lr: 1.0000e-04\n",
            "Epoch 3/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.1587 - dice_coef: 0.5375 - iou: 0.3726 - recall_5: 0.8136 - precision_5: 0.4890\n",
            "Epoch 3: val_loss improved from 3.67758 to 2.97717, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 3.1587 - dice_coef: 0.5375 - iou: 0.3726 - recall_5: 0.8136 - precision_5: 0.4890 - val_loss: 2.9772 - val_dice_coef: 0.4086 - val_iou: 0.2600 - val_recall_5: 0.6732 - val_precision_5: 0.3766 - lr: 1.0000e-04\n",
            "Epoch 4/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 2.5417 - dice_coef: 0.5935 - iou: 0.4275 - recall_5: 0.8105 - precision_5: 0.5682\n",
            "Epoch 4: val_loss improved from 2.97717 to 2.48667, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 2.5417 - dice_coef: 0.5935 - iou: 0.4275 - recall_5: 0.8105 - precision_5: 0.5682 - val_loss: 2.4867 - val_dice_coef: 0.4137 - val_iou: 0.2648 - val_recall_5: 0.7996 - val_precision_5: 0.3165 - lr: 1.0000e-04\n",
            "Epoch 5/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 2.0605 - dice_coef: 0.6447 - iou: 0.4816 - recall_5: 0.8155 - precision_5: 0.6369\n",
            "Epoch 5: val_loss improved from 2.48667 to 2.03958, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 2.0605 - dice_coef: 0.6447 - iou: 0.4816 - recall_5: 0.8155 - precision_5: 0.6369 - val_loss: 2.0396 - val_dice_coef: 0.4846 - val_iou: 0.3248 - val_recall_5: 0.6860 - val_precision_5: 0.4363 - lr: 1.0000e-04\n",
            "Epoch 6/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.6884 - dice_coef: 0.6836 - iou: 0.5252 - recall_5: 0.8172 - precision_5: 0.6861\n",
            "Epoch 6: val_loss improved from 2.03958 to 1.73719, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 1.6884 - dice_coef: 0.6836 - iou: 0.5252 - recall_5: 0.8172 - precision_5: 0.6861 - val_loss: 1.7372 - val_dice_coef: 0.4954 - val_iou: 0.3345 - val_recall_5: 0.6246 - val_precision_5: 0.4771 - lr: 1.0000e-04\n",
            "Epoch 7/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3977 - dice_coef: 0.7156 - iou: 0.5630 - recall_5: 0.8178 - precision_5: 0.7259\n",
            "Epoch 7: val_loss improved from 1.73719 to 1.51240, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 1.3977 - dice_coef: 0.7156 - iou: 0.5630 - recall_5: 0.8178 - precision_5: 0.7259 - val_loss: 1.5124 - val_dice_coef: 0.4926 - val_iou: 0.3314 - val_recall_5: 0.7084 - val_precision_5: 0.4212 - lr: 1.0000e-04\n",
            "Epoch 8/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.1707 - dice_coef: 0.7404 - iou: 0.5939 - recall_5: 0.8207 - precision_5: 0.7530\n",
            "Epoch 8: val_loss improved from 1.51240 to 1.34844, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 1.1707 - dice_coef: 0.7404 - iou: 0.5939 - recall_5: 0.8207 - precision_5: 0.7530 - val_loss: 1.3484 - val_dice_coef: 0.4779 - val_iou: 0.3181 - val_recall_5: 0.7850 - val_precision_5: 0.3752 - lr: 1.0000e-04\n",
            "Epoch 9/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.9853 - dice_coef: 0.7664 - iou: 0.6263 - recall_5: 0.8202 - precision_5: 0.7881\n",
            "Epoch 9: val_loss improved from 1.34844 to 1.11087, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.9853 - dice_coef: 0.7664 - iou: 0.6263 - recall_5: 0.8202 - precision_5: 0.7881 - val_loss: 1.1109 - val_dice_coef: 0.5725 - val_iou: 0.4060 - val_recall_5: 0.7136 - val_precision_5: 0.5257 - lr: 1.0000e-04\n",
            "Epoch 10/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.8443 - dice_coef: 0.7815 - iou: 0.6465 - recall_5: 0.8177 - precision_5: 0.8060\n",
            "Epoch 10: val_loss improved from 1.11087 to 1.01566, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.8443 - dice_coef: 0.7815 - iou: 0.6465 - recall_5: 0.8177 - precision_5: 0.8060 - val_loss: 1.0157 - val_dice_coef: 0.5582 - val_iou: 0.3919 - val_recall_5: 0.6597 - val_precision_5: 0.5301 - lr: 1.0000e-04\n",
            "Epoch 11/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7304 - dice_coef: 0.7966 - iou: 0.6663 - recall_5: 0.8234 - precision_5: 0.8184\n",
            "Epoch 11: val_loss improved from 1.01566 to 0.88218, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.7304 - dice_coef: 0.7966 - iou: 0.6663 - recall_5: 0.8234 - precision_5: 0.8184 - val_loss: 0.8822 - val_dice_coef: 0.6049 - val_iou: 0.4422 - val_recall_5: 0.6272 - val_precision_5: 0.6313 - lr: 1.0000e-04\n",
            "Epoch 12/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.6352 - dice_coef: 0.8137 - iou: 0.6894 - recall_5: 0.8252 - precision_5: 0.8411\n",
            "Epoch 12: val_loss improved from 0.88218 to 0.78448, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.6352 - dice_coef: 0.8137 - iou: 0.6894 - recall_5: 0.8252 - precision_5: 0.8411 - val_loss: 0.7845 - val_dice_coef: 0.6331 - val_iou: 0.4721 - val_recall_5: 0.7188 - val_precision_5: 0.6082 - lr: 1.0000e-04\n",
            "Epoch 13/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.5640 - dice_coef: 0.8225 - iou: 0.7023 - recall_5: 0.8232 - precision_5: 0.8537\n",
            "Epoch 13: val_loss improved from 0.78448 to 0.77473, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.5640 - dice_coef: 0.8225 - iou: 0.7023 - recall_5: 0.8232 - precision_5: 0.8537 - val_loss: 0.7747 - val_dice_coef: 0.5867 - val_iou: 0.4212 - val_recall_5: 0.8099 - val_precision_5: 0.4882 - lr: 1.0000e-04\n",
            "Epoch 14/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.5018 - dice_coef: 0.8338 - iou: 0.7185 - recall_5: 0.8274 - precision_5: 0.8652\n",
            "Epoch 14: val_loss improved from 0.77473 to 0.72776, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.5018 - dice_coef: 0.8338 - iou: 0.7185 - recall_5: 0.8274 - precision_5: 0.8652 - val_loss: 0.7278 - val_dice_coef: 0.5881 - val_iou: 0.4232 - val_recall_5: 0.8200 - val_precision_5: 0.4858 - lr: 1.0000e-04\n",
            "Epoch 15/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4475 - dice_coef: 0.8460 - iou: 0.7363 - recall_5: 0.8350 - precision_5: 0.8777\n",
            "Epoch 15: val_loss improved from 0.72776 to 0.66960, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.4475 - dice_coef: 0.8460 - iou: 0.7363 - recall_5: 0.8350 - precision_5: 0.8777 - val_loss: 0.6696 - val_dice_coef: 0.6077 - val_iou: 0.4429 - val_recall_5: 0.8149 - val_precision_5: 0.5115 - lr: 1.0000e-04\n",
            "Epoch 16/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.4016 - dice_coef: 0.8568 - iou: 0.7520 - recall_5: 0.8406 - precision_5: 0.8881\n",
            "Epoch 16: val_loss improved from 0.66960 to 0.65609, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.4016 - dice_coef: 0.8568 - iou: 0.7520 - recall_5: 0.8406 - precision_5: 0.8881 - val_loss: 0.6561 - val_dice_coef: 0.5895 - val_iou: 0.4235 - val_recall_5: 0.8514 - val_precision_5: 0.4752 - lr: 1.0000e-04\n",
            "Epoch 17/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3643 - dice_coef: 0.8644 - iou: 0.7637 - recall_5: 0.8480 - precision_5: 0.8940\n",
            "Epoch 17: val_loss improved from 0.65609 to 0.61211, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.3643 - dice_coef: 0.8644 - iou: 0.7637 - recall_5: 0.8480 - precision_5: 0.8940 - val_loss: 0.6121 - val_dice_coef: 0.6059 - val_iou: 0.4436 - val_recall_5: 0.8415 - val_precision_5: 0.4993 - lr: 1.0000e-04\n",
            "Epoch 18/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3409 - dice_coef: 0.8628 - iou: 0.7615 - recall_5: 0.8425 - precision_5: 0.8944\n",
            "Epoch 18: val_loss improved from 0.61211 to 0.52592, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 103ms/step - loss: 0.3409 - dice_coef: 0.8628 - iou: 0.7615 - recall_5: 0.8425 - precision_5: 0.8944 - val_loss: 0.5259 - val_dice_coef: 0.6694 - val_iou: 0.5126 - val_recall_5: 0.7009 - val_precision_5: 0.6651 - lr: 1.0000e-04\n",
            "Epoch 19/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.3128 - dice_coef: 0.8703 - iou: 0.7727 - recall_5: 0.8494 - precision_5: 0.8999\n",
            "Epoch 19: val_loss did not improve from 0.52592\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.3128 - dice_coef: 0.8703 - iou: 0.7727 - recall_5: 0.8494 - precision_5: 0.8999 - val_loss: 0.6109 - val_dice_coef: 0.5612 - val_iou: 0.4036 - val_recall_5: 0.4440 - val_precision_5: 0.7814 - lr: 1.0000e-04\n",
            "Epoch 20/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2918 - dice_coef: 0.8737 - iou: 0.7781 - recall_5: 0.8478 - precision_5: 0.9065\n",
            "Epoch 20: val_loss improved from 0.52592 to 0.47571, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.2918 - dice_coef: 0.8737 - iou: 0.7781 - recall_5: 0.8478 - precision_5: 0.9065 - val_loss: 0.4757 - val_dice_coef: 0.6825 - val_iou: 0.5302 - val_recall_5: 0.6240 - val_precision_5: 0.7751 - lr: 1.0000e-04\n",
            "Epoch 21/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2707 - dice_coef: 0.8793 - iou: 0.7870 - recall_5: 0.8553 - precision_5: 0.9092\n",
            "Epoch 21: val_loss improved from 0.47571 to 0.46622, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.2707 - dice_coef: 0.8793 - iou: 0.7870 - recall_5: 0.8553 - precision_5: 0.9092 - val_loss: 0.4662 - val_dice_coef: 0.6791 - val_iou: 0.5229 - val_recall_5: 0.7473 - val_precision_5: 0.6507 - lr: 1.0000e-04\n",
            "Epoch 22/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2463 - dice_coef: 0.8905 - iou: 0.8046 - recall_5: 0.8631 - precision_5: 0.9203\n",
            "Epoch 22: val_loss improved from 0.46622 to 0.44044, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.2463 - dice_coef: 0.8905 - iou: 0.8046 - recall_5: 0.8631 - precision_5: 0.9203 - val_loss: 0.4404 - val_dice_coef: 0.6918 - val_iou: 0.5392 - val_recall_5: 0.7079 - val_precision_5: 0.6939 - lr: 1.0000e-04\n",
            "Epoch 23/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2268 - dice_coef: 0.8980 - iou: 0.8166 - recall_5: 0.8681 - precision_5: 0.9290\n",
            "Epoch 23: val_loss improved from 0.44044 to 0.43699, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.2268 - dice_coef: 0.8980 - iou: 0.8166 - recall_5: 0.8681 - precision_5: 0.9290 - val_loss: 0.4370 - val_dice_coef: 0.6845 - val_iou: 0.5333 - val_recall_5: 0.6751 - val_precision_5: 0.7129 - lr: 1.0000e-04\n",
            "Epoch 24/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.2114 - dice_coef: 0.9026 - iou: 0.8242 - recall_5: 0.8719 - precision_5: 0.9318\n",
            "Epoch 24: val_loss improved from 0.43699 to 0.42652, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.2114 - dice_coef: 0.9026 - iou: 0.8242 - recall_5: 0.8719 - precision_5: 0.9318 - val_loss: 0.4265 - val_dice_coef: 0.6847 - val_iou: 0.5298 - val_recall_5: 0.7377 - val_precision_5: 0.6530 - lr: 1.0000e-04\n",
            "Epoch 25/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1984 - dice_coef: 0.9061 - iou: 0.8298 - recall_5: 0.8745 - precision_5: 0.9355\n",
            "Epoch 25: val_loss improved from 0.42652 to 0.40276, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.1984 - dice_coef: 0.9061 - iou: 0.8298 - recall_5: 0.8745 - precision_5: 0.9355 - val_loss: 0.4028 - val_dice_coef: 0.6997 - val_iou: 0.5474 - val_recall_5: 0.7263 - val_precision_5: 0.6916 - lr: 1.0000e-04\n",
            "Epoch 26/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1966 - dice_coef: 0.8996 - iou: 0.8190 - recall_5: 0.8684 - precision_5: 0.9279\n",
            "Epoch 26: val_loss improved from 0.40276 to 0.38865, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.1966 - dice_coef: 0.8996 - iou: 0.8190 - recall_5: 0.8684 - precision_5: 0.9279 - val_loss: 0.3886 - val_dice_coef: 0.7061 - val_iou: 0.5533 - val_recall_5: 0.7297 - val_precision_5: 0.6929 - lr: 1.0000e-04\n",
            "Epoch 27/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1820 - dice_coef: 0.9073 - iou: 0.8316 - recall_5: 0.8737 - precision_5: 0.9369\n",
            "Epoch 27: val_loss improved from 0.38865 to 0.36741, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.1820 - dice_coef: 0.9073 - iou: 0.8316 - recall_5: 0.8737 - precision_5: 0.9369 - val_loss: 0.3674 - val_dice_coef: 0.7209 - val_iou: 0.5737 - val_recall_5: 0.6998 - val_precision_5: 0.7524 - lr: 1.0000e-04\n",
            "Epoch 28/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1722 - dice_coef: 0.9103 - iou: 0.8369 - recall_5: 0.8799 - precision_5: 0.9369\n",
            "Epoch 28: val_loss did not improve from 0.36741\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.1722 - dice_coef: 0.9103 - iou: 0.8369 - recall_5: 0.8799 - precision_5: 0.9369 - val_loss: 0.4033 - val_dice_coef: 0.6773 - val_iou: 0.5226 - val_recall_5: 0.6167 - val_precision_5: 0.7548 - lr: 1.0000e-04\n",
            "Epoch 29/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1620 - dice_coef: 0.9152 - iou: 0.8447 - recall_5: 0.8816 - precision_5: 0.9436\n",
            "Epoch 29: val_loss did not improve from 0.36741\n",
            "93/93 [==============================] - 9s 100ms/step - loss: 0.1620 - dice_coef: 0.9152 - iou: 0.8447 - recall_5: 0.8816 - precision_5: 0.9436 - val_loss: 0.3790 - val_dice_coef: 0.6988 - val_iou: 0.5456 - val_recall_5: 0.7751 - val_precision_5: 0.6462 - lr: 1.0000e-04\n",
            "Epoch 30/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1521 - dice_coef: 0.9197 - iou: 0.8523 - recall_5: 0.8857 - precision_5: 0.9462\n",
            "Epoch 30: val_loss improved from 0.36741 to 0.36684, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.1521 - dice_coef: 0.9197 - iou: 0.8523 - recall_5: 0.8857 - precision_5: 0.9462 - val_loss: 0.3668 - val_dice_coef: 0.7057 - val_iou: 0.5565 - val_recall_5: 0.6883 - val_precision_5: 0.7371 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1484 - dice_coef: 0.9185 - iou: 0.8504 - recall_5: 0.8858 - precision_5: 0.9446\n",
            "Epoch 31: val_loss improved from 0.36684 to 0.34793, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.1484 - dice_coef: 0.9185 - iou: 0.8504 - recall_5: 0.8858 - precision_5: 0.9446 - val_loss: 0.3479 - val_dice_coef: 0.7189 - val_iou: 0.5696 - val_recall_5: 0.6930 - val_precision_5: 0.7525 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1416 - dice_coef: 0.9210 - iou: 0.8547 - recall_5: 0.8869 - precision_5: 0.9468\n",
            "Epoch 32: val_loss did not improve from 0.34793\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.1416 - dice_coef: 0.9210 - iou: 0.8547 - recall_5: 0.8869 - precision_5: 0.9468 - val_loss: 0.3567 - val_dice_coef: 0.7053 - val_iou: 0.5531 - val_recall_5: 0.7081 - val_precision_5: 0.7064 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1378 - dice_coef: 0.9210 - iou: 0.8546 - recall_5: 0.8860 - precision_5: 0.9485\n",
            "Epoch 33: val_loss did not improve from 0.34793\n",
            "93/93 [==============================] - 9s 100ms/step - loss: 0.1378 - dice_coef: 0.9210 - iou: 0.8546 - recall_5: 0.8860 - precision_5: 0.9485 - val_loss: 0.3652 - val_dice_coef: 0.6925 - val_iou: 0.5412 - val_recall_5: 0.6293 - val_precision_5: 0.7572 - lr: 1.0000e-04\n",
            "Epoch 34/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1304 - dice_coef: 0.9249 - iou: 0.8611 - recall_5: 0.8889 - precision_5: 0.9513\n",
            "Epoch 34: val_loss did not improve from 0.34793\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.1304 - dice_coef: 0.9249 - iou: 0.8611 - recall_5: 0.8889 - precision_5: 0.9513 - val_loss: 0.3647 - val_dice_coef: 0.6898 - val_iou: 0.5342 - val_recall_5: 0.7522 - val_precision_5: 0.6449 - lr: 1.0000e-04\n",
            "Epoch 35/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1269 - dice_coef: 0.9252 - iou: 0.8620 - recall_5: 0.8899 - precision_5: 0.9502\n",
            "Epoch 35: val_loss improved from 0.34793 to 0.34733, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.1269 - dice_coef: 0.9252 - iou: 0.8620 - recall_5: 0.8899 - precision_5: 0.9502 - val_loss: 0.3473 - val_dice_coef: 0.7044 - val_iou: 0.5575 - val_recall_5: 0.6223 - val_precision_5: 0.8142 - lr: 1.0000e-04\n",
            "Epoch 36/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1194 - dice_coef: 0.9296 - iou: 0.8693 - recall_5: 0.8962 - precision_5: 0.9535\n",
            "Epoch 36: val_loss improved from 0.34733 to 0.32780, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.1194 - dice_coef: 0.9296 - iou: 0.8693 - recall_5: 0.8962 - precision_5: 0.9535 - val_loss: 0.3278 - val_dice_coef: 0.7222 - val_iou: 0.5739 - val_recall_5: 0.7291 - val_precision_5: 0.7193 - lr: 1.0000e-04\n",
            "Epoch 37/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1168 - dice_coef: 0.9296 - iou: 0.8693 - recall_5: 0.8948 - precision_5: 0.9540\n",
            "Epoch 37: val_loss did not improve from 0.32780\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.1168 - dice_coef: 0.9296 - iou: 0.8693 - recall_5: 0.8948 - precision_5: 0.9540 - val_loss: 0.3883 - val_dice_coef: 0.6576 - val_iou: 0.4966 - val_recall_5: 0.8349 - val_precision_5: 0.5583 - lr: 1.0000e-04\n",
            "Epoch 38/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1188 - dice_coef: 0.9254 - iou: 0.8622 - recall_5: 0.8874 - precision_5: 0.9536\n",
            "Epoch 38: val_loss did not improve from 0.32780\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.1188 - dice_coef: 0.9254 - iou: 0.8622 - recall_5: 0.8874 - precision_5: 0.9536 - val_loss: 0.3550 - val_dice_coef: 0.6886 - val_iou: 0.5340 - val_recall_5: 0.6311 - val_precision_5: 0.7460 - lr: 1.0000e-04\n",
            "Epoch 39/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1157 - dice_coef: 0.9265 - iou: 0.8641 - recall_5: 0.8920 - precision_5: 0.9508\n",
            "Epoch 39: val_loss did not improve from 0.32780\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.1157 - dice_coef: 0.9265 - iou: 0.8641 - recall_5: 0.8920 - precision_5: 0.9508 - val_loss: 0.3614 - val_dice_coef: 0.6823 - val_iou: 0.5267 - val_recall_5: 0.5916 - val_precision_5: 0.7944 - lr: 1.0000e-04\n",
            "Epoch 40/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1118 - dice_coef: 0.9284 - iou: 0.8673 - recall_5: 0.8920 - precision_5: 0.9531\n",
            "Epoch 40: val_loss did not improve from 0.32780\n",
            "93/93 [==============================] - 9s 100ms/step - loss: 0.1118 - dice_coef: 0.9284 - iou: 0.8673 - recall_5: 0.8920 - precision_5: 0.9531 - val_loss: 0.3612 - val_dice_coef: 0.6794 - val_iou: 0.5271 - val_recall_5: 0.5788 - val_precision_5: 0.8206 - lr: 1.0000e-04\n",
            "Epoch 41/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1061 - dice_coef: 0.9322 - iou: 0.8736 - recall_5: 0.8972 - precision_5: 0.9556\n",
            "Epoch 41: val_loss did not improve from 0.32780\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.1061 - dice_coef: 0.9322 - iou: 0.8736 - recall_5: 0.8972 - precision_5: 0.9556 - val_loss: 0.4034 - val_dice_coef: 0.6335 - val_iou: 0.4784 - val_recall_5: 0.4849 - val_precision_5: 0.8918 - lr: 1.0000e-04\n",
            "Epoch 42/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0966 - dice_coef: 0.9398 - iou: 0.8870 - recall_5: 0.9048 - precision_5: 0.9633\n",
            "Epoch 42: val_loss improved from 0.32780 to 0.30935, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0966 - dice_coef: 0.9398 - iou: 0.8870 - recall_5: 0.9048 - precision_5: 0.9633 - val_loss: 0.3093 - val_dice_coef: 0.7289 - val_iou: 0.5844 - val_recall_5: 0.6730 - val_precision_5: 0.7923 - lr: 1.0000e-04\n",
            "Epoch 43/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0981 - dice_coef: 0.9366 - iou: 0.8813 - recall_5: 0.9005 - precision_5: 0.9614\n",
            "Epoch 43: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 103ms/step - loss: 0.0981 - dice_coef: 0.9366 - iou: 0.8813 - recall_5: 0.9005 - precision_5: 0.9614 - val_loss: 0.3701 - val_dice_coef: 0.6649 - val_iou: 0.5102 - val_recall_5: 0.5268 - val_precision_5: 0.8803 - lr: 1.0000e-04\n",
            "Epoch 44/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0949 - dice_coef: 0.9382 - iou: 0.8841 - recall_5: 0.9013 - precision_5: 0.9634\n",
            "Epoch 44: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0949 - dice_coef: 0.9382 - iou: 0.8841 - recall_5: 0.9013 - precision_5: 0.9634 - val_loss: 0.3910 - val_dice_coef: 0.6441 - val_iou: 0.4939 - val_recall_5: 0.5061 - val_precision_5: 0.8784 - lr: 1.0000e-04\n",
            "Epoch 45/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0930 - dice_coef: 0.9388 - iou: 0.8852 - recall_5: 0.9028 - precision_5: 0.9617\n",
            "Epoch 45: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0930 - dice_coef: 0.9388 - iou: 0.8852 - recall_5: 0.9028 - precision_5: 0.9617 - val_loss: 0.4754 - val_dice_coef: 0.5541 - val_iou: 0.4008 - val_recall_5: 0.3846 - val_precision_5: 0.9406 - lr: 1.0000e-04\n",
            "Epoch 46/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0918 - dice_coef: 0.9386 - iou: 0.8851 - recall_5: 0.9029 - precision_5: 0.9627\n",
            "Epoch 46: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0918 - dice_coef: 0.9386 - iou: 0.8851 - recall_5: 0.9029 - precision_5: 0.9627 - val_loss: 0.4316 - val_dice_coef: 0.5994 - val_iou: 0.4500 - val_recall_5: 0.4448 - val_precision_5: 0.9154 - lr: 1.0000e-04\n",
            "Epoch 47/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0876 - dice_coef: 0.9416 - iou: 0.8903 - recall_5: 0.9054 - precision_5: 0.9651\n",
            "Epoch 47: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 9s 100ms/step - loss: 0.0876 - dice_coef: 0.9416 - iou: 0.8903 - recall_5: 0.9054 - precision_5: 0.9651 - val_loss: 0.3534 - val_dice_coef: 0.6766 - val_iou: 0.5305 - val_recall_5: 0.5448 - val_precision_5: 0.8974 - lr: 1.0000e-04\n",
            "Epoch 48/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0897 - dice_coef: 0.9384 - iou: 0.8844 - recall_5: 0.9012 - precision_5: 0.9625\n",
            "Epoch 48: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0897 - dice_coef: 0.9384 - iou: 0.8844 - recall_5: 0.9012 - precision_5: 0.9625 - val_loss: 0.4519 - val_dice_coef: 0.5727 - val_iou: 0.4201 - val_recall_5: 0.4073 - val_precision_5: 0.9418 - lr: 1.0000e-04\n",
            "Epoch 49/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0892 - dice_coef: 0.9382 - iou: 0.8842 - recall_5: 0.9015 - precision_5: 0.9617\n",
            "Epoch 49: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.0892 - dice_coef: 0.9382 - iou: 0.8842 - recall_5: 0.9015 - precision_5: 0.9617 - val_loss: 0.3369 - val_dice_coef: 0.6929 - val_iou: 0.5495 - val_recall_5: 0.5942 - val_precision_5: 0.8413 - lr: 1.0000e-04\n",
            "Epoch 50/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0888 - dice_coef: 0.9379 - iou: 0.8837 - recall_5: 0.9000 - precision_5: 0.9616\n",
            "Epoch 50: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 9s 102ms/step - loss: 0.0888 - dice_coef: 0.9379 - iou: 0.8837 - recall_5: 0.9000 - precision_5: 0.9616 - val_loss: 0.3815 - val_dice_coef: 0.6461 - val_iou: 0.4969 - val_recall_5: 0.4924 - val_precision_5: 0.9270 - lr: 1.0000e-04\n",
            "Epoch 51/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0876 - dice_coef: 0.9384 - iou: 0.8847 - recall_5: 0.9018 - precision_5: 0.9617\n",
            "Epoch 51: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 103ms/step - loss: 0.0876 - dice_coef: 0.9384 - iou: 0.8847 - recall_5: 0.9018 - precision_5: 0.9617 - val_loss: 0.3419 - val_dice_coef: 0.6872 - val_iou: 0.5368 - val_recall_5: 0.5886 - val_precision_5: 0.8296 - lr: 1.0000e-04\n",
            "Epoch 52/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0828 - dice_coef: 0.9423 - iou: 0.8915 - recall_5: 0.9056 - precision_5: 0.9656\n",
            "Epoch 52: val_loss did not improve from 0.30935\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0828 - dice_coef: 0.9423 - iou: 0.8915 - recall_5: 0.9056 - precision_5: 0.9656 - val_loss: 0.3479 - val_dice_coef: 0.6777 - val_iou: 0.5282 - val_recall_5: 0.5574 - val_precision_5: 0.8669 - lr: 1.0000e-04\n",
            "Epoch 53/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0777 - dice_coef: 0.9468 - iou: 0.8994 - recall_5: 0.9138 - precision_5: 0.9664\n",
            "Epoch 53: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0777 - dice_coef: 0.9468 - iou: 0.8994 - recall_5: 0.9138 - precision_5: 0.9664 - val_loss: 0.3127 - val_dice_coef: 0.7138 - val_iou: 0.5699 - val_recall_5: 0.6335 - val_precision_5: 0.8243 - lr: 1.0000e-05\n",
            "Epoch 54/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0727 - dice_coef: 0.9516 - iou: 0.9080 - recall_5: 0.9177 - precision_5: 0.9710\n",
            "Epoch 54: val_loss did not improve from 0.30935\n",
            "93/93 [==============================] - 9s 102ms/step - loss: 0.0727 - dice_coef: 0.9516 - iou: 0.9080 - recall_5: 0.9177 - precision_5: 0.9710 - val_loss: 0.3101 - val_dice_coef: 0.7163 - val_iou: 0.5728 - val_recall_5: 0.6300 - val_precision_5: 0.8341 - lr: 1.0000e-05\n",
            "Epoch 55/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0704 - dice_coef: 0.9538 - iou: 0.9120 - recall_5: 0.9195 - precision_5: 0.9734\n",
            "Epoch 55: val_loss improved from 0.30935 to 0.30828, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 112ms/step - loss: 0.0704 - dice_coef: 0.9538 - iou: 0.9120 - recall_5: 0.9195 - precision_5: 0.9734 - val_loss: 0.3083 - val_dice_coef: 0.7179 - val_iou: 0.5748 - val_recall_5: 0.6338 - val_precision_5: 0.8335 - lr: 1.0000e-05\n",
            "Epoch 56/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0688 - dice_coef: 0.9552 - iou: 0.9146 - recall_5: 0.9207 - precision_5: 0.9748\n",
            "Epoch 56: val_loss did not improve from 0.30828\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0688 - dice_coef: 0.9552 - iou: 0.9146 - recall_5: 0.9207 - precision_5: 0.9748 - val_loss: 0.3088 - val_dice_coef: 0.7171 - val_iou: 0.5736 - val_recall_5: 0.6304 - val_precision_5: 0.8360 - lr: 1.0000e-05\n",
            "Epoch 57/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0679 - dice_coef: 0.9560 - iou: 0.9159 - recall_5: 0.9213 - precision_5: 0.9758\n",
            "Epoch 57: val_loss improved from 0.30828 to 0.30600, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0679 - dice_coef: 0.9560 - iou: 0.9159 - recall_5: 0.9213 - precision_5: 0.9758 - val_loss: 0.3060 - val_dice_coef: 0.7198 - val_iou: 0.5763 - val_recall_5: 0.6393 - val_precision_5: 0.8274 - lr: 1.0000e-05\n",
            "Epoch 58/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0674 - dice_coef: 0.9562 - iou: 0.9164 - recall_5: 0.9218 - precision_5: 0.9758\n",
            "Epoch 58: val_loss improved from 0.30600 to 0.30490, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0674 - dice_coef: 0.9562 - iou: 0.9164 - recall_5: 0.9218 - precision_5: 0.9758 - val_loss: 0.3049 - val_dice_coef: 0.7205 - val_iou: 0.5768 - val_recall_5: 0.6406 - val_precision_5: 0.8268 - lr: 1.0000e-05\n",
            "Epoch 59/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0663 - dice_coef: 0.9571 - iou: 0.9181 - recall_5: 0.9229 - precision_5: 0.9765\n",
            "Epoch 59: val_loss improved from 0.30490 to 0.30019, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0663 - dice_coef: 0.9571 - iou: 0.9181 - recall_5: 0.9229 - precision_5: 0.9765 - val_loss: 0.3002 - val_dice_coef: 0.7252 - val_iou: 0.5820 - val_recall_5: 0.6581 - val_precision_5: 0.8125 - lr: 1.0000e-05\n",
            "Epoch 60/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0656 - dice_coef: 0.9577 - iou: 0.9190 - recall_5: 0.9232 - precision_5: 0.9770\n",
            "Epoch 60: val_loss did not improve from 0.30019\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0656 - dice_coef: 0.9577 - iou: 0.9190 - recall_5: 0.9232 - precision_5: 0.9770 - val_loss: 0.3002 - val_dice_coef: 0.7250 - val_iou: 0.5817 - val_recall_5: 0.6539 - val_precision_5: 0.8170 - lr: 1.0000e-05\n",
            "Epoch 61/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0649 - dice_coef: 0.9582 - iou: 0.9199 - recall_5: 0.9236 - precision_5: 0.9776\n",
            "Epoch 61: val_loss did not improve from 0.30019\n",
            "93/93 [==============================] - 9s 99ms/step - loss: 0.0649 - dice_coef: 0.9582 - iou: 0.9199 - recall_5: 0.9236 - precision_5: 0.9776 - val_loss: 0.3003 - val_dice_coef: 0.7246 - val_iou: 0.5810 - val_recall_5: 0.6534 - val_precision_5: 0.8168 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0643 - dice_coef: 0.9586 - iou: 0.9207 - recall_5: 0.9243 - precision_5: 0.9778\n",
            "Epoch 62: val_loss did not improve from 0.30019\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0643 - dice_coef: 0.9586 - iou: 0.9207 - recall_5: 0.9243 - precision_5: 0.9778 - val_loss: 0.3014 - val_dice_coef: 0.7232 - val_iou: 0.5795 - val_recall_5: 0.6459 - val_precision_5: 0.8246 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0635 - dice_coef: 0.9591 - iou: 0.9217 - recall_5: 0.9249 - precision_5: 0.9783\n",
            "Epoch 63: val_loss improved from 0.30019 to 0.29799, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0635 - dice_coef: 0.9591 - iou: 0.9217 - recall_5: 0.9249 - precision_5: 0.9783 - val_loss: 0.2980 - val_dice_coef: 0.7265 - val_iou: 0.5829 - val_recall_5: 0.6567 - val_precision_5: 0.8163 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0630 - dice_coef: 0.9594 - iou: 0.9222 - recall_5: 0.9256 - precision_5: 0.9782\n",
            "Epoch 64: val_loss did not improve from 0.29799\n",
            "93/93 [==============================] - 10s 102ms/step - loss: 0.0630 - dice_coef: 0.9594 - iou: 0.9222 - recall_5: 0.9256 - precision_5: 0.9782 - val_loss: 0.2982 - val_dice_coef: 0.7261 - val_iou: 0.5825 - val_recall_5: 0.6568 - val_precision_5: 0.8154 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0625 - dice_coef: 0.9597 - iou: 0.9228 - recall_5: 0.9259 - precision_5: 0.9786\n",
            "Epoch 65: val_loss improved from 0.29799 to 0.29711, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0625 - dice_coef: 0.9597 - iou: 0.9228 - recall_5: 0.9259 - precision_5: 0.9786 - val_loss: 0.2971 - val_dice_coef: 0.7268 - val_iou: 0.5831 - val_recall_5: 0.6573 - val_precision_5: 0.8161 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0620 - dice_coef: 0.9600 - iou: 0.9233 - recall_5: 0.9257 - precision_5: 0.9792\n",
            "Epoch 66: val_loss improved from 0.29711 to 0.29415, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0620 - dice_coef: 0.9600 - iou: 0.9233 - recall_5: 0.9257 - precision_5: 0.9792 - val_loss: 0.2942 - val_dice_coef: 0.7295 - val_iou: 0.5860 - val_recall_5: 0.6658 - val_precision_5: 0.8099 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0614 - dice_coef: 0.9604 - iou: 0.9240 - recall_5: 0.9263 - precision_5: 0.9794\n",
            "Epoch 67: val_loss did not improve from 0.29415\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0614 - dice_coef: 0.9604 - iou: 0.9240 - recall_5: 0.9263 - precision_5: 0.9794 - val_loss: 0.2947 - val_dice_coef: 0.7287 - val_iou: 0.5850 - val_recall_5: 0.6630 - val_precision_5: 0.8120 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0608 - dice_coef: 0.9608 - iou: 0.9248 - recall_5: 0.9264 - precision_5: 0.9799\n",
            "Epoch 68: val_loss improved from 0.29415 to 0.29238, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0608 - dice_coef: 0.9608 - iou: 0.9248 - recall_5: 0.9264 - precision_5: 0.9799 - val_loss: 0.2924 - val_dice_coef: 0.7307 - val_iou: 0.5874 - val_recall_5: 0.6658 - val_precision_5: 0.8127 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0604 - dice_coef: 0.9610 - iou: 0.9251 - recall_5: 0.9270 - precision_5: 0.9798\n",
            "Epoch 69: val_loss improved from 0.29238 to 0.29078, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0604 - dice_coef: 0.9610 - iou: 0.9251 - recall_5: 0.9270 - precision_5: 0.9798 - val_loss: 0.2908 - val_dice_coef: 0.7321 - val_iou: 0.5889 - val_recall_5: 0.6717 - val_precision_5: 0.8085 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0599 - dice_coef: 0.9612 - iou: 0.9255 - recall_5: 0.9272 - precision_5: 0.9800\n",
            "Epoch 70: val_loss improved from 0.29078 to 0.28698, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 112ms/step - loss: 0.0599 - dice_coef: 0.9612 - iou: 0.9255 - recall_5: 0.9272 - precision_5: 0.9800 - val_loss: 0.2870 - val_dice_coef: 0.7356 - val_iou: 0.5924 - val_recall_5: 0.6820 - val_precision_5: 0.8020 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0593 - dice_coef: 0.9616 - iou: 0.9262 - recall_5: 0.9276 - precision_5: 0.9802\n",
            "Epoch 71: val_loss did not improve from 0.28698\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0593 - dice_coef: 0.9616 - iou: 0.9262 - recall_5: 0.9276 - precision_5: 0.9802 - val_loss: 0.2891 - val_dice_coef: 0.7335 - val_iou: 0.5903 - val_recall_5: 0.6729 - val_precision_5: 0.8087 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0590 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9277 - precision_5: 0.9804\n",
            "Epoch 72: val_loss did not improve from 0.28698\n",
            "93/93 [==============================] - 9s 102ms/step - loss: 0.0590 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9277 - precision_5: 0.9804 - val_loss: 0.2876 - val_dice_coef: 0.7346 - val_iou: 0.5914 - val_recall_5: 0.6806 - val_precision_5: 0.8015 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0584 - dice_coef: 0.9620 - iou: 0.9270 - recall_5: 0.9281 - precision_5: 0.9806\n",
            "Epoch 73: val_loss did not improve from 0.28698\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0584 - dice_coef: 0.9620 - iou: 0.9270 - recall_5: 0.9281 - precision_5: 0.9806 - val_loss: 0.2891 - val_dice_coef: 0.7328 - val_iou: 0.5891 - val_recall_5: 0.6755 - val_precision_5: 0.8037 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0579 - dice_coef: 0.9623 - iou: 0.9276 - recall_5: 0.9283 - precision_5: 0.9809\n",
            "Epoch 74: val_loss improved from 0.28698 to 0.28686, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 112ms/step - loss: 0.0579 - dice_coef: 0.9623 - iou: 0.9276 - recall_5: 0.9283 - precision_5: 0.9809 - val_loss: 0.2869 - val_dice_coef: 0.7347 - val_iou: 0.5909 - val_recall_5: 0.6826 - val_precision_5: 0.7986 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0573 - dice_coef: 0.9626 - iou: 0.9281 - recall_5: 0.9284 - precision_5: 0.9812\n",
            "Epoch 75: val_loss improved from 0.28686 to 0.28663, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 112ms/step - loss: 0.0573 - dice_coef: 0.9626 - iou: 0.9281 - recall_5: 0.9284 - precision_5: 0.9812 - val_loss: 0.2866 - val_dice_coef: 0.7349 - val_iou: 0.5913 - val_recall_5: 0.6803 - val_precision_5: 0.8013 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0570 - dice_coef: 0.9627 - iou: 0.9283 - recall_5: 0.9289 - precision_5: 0.9809\n",
            "Epoch 76: val_loss improved from 0.28663 to 0.28582, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0570 - dice_coef: 0.9627 - iou: 0.9283 - recall_5: 0.9289 - precision_5: 0.9809 - val_loss: 0.2858 - val_dice_coef: 0.7356 - val_iou: 0.5922 - val_recall_5: 0.6924 - val_precision_5: 0.7896 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0565 - dice_coef: 0.9630 - iou: 0.9289 - recall_5: 0.9290 - precision_5: 0.9814\n",
            "Epoch 77: val_loss did not improve from 0.28582\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0565 - dice_coef: 0.9630 - iou: 0.9289 - recall_5: 0.9290 - precision_5: 0.9814 - val_loss: 0.2863 - val_dice_coef: 0.7347 - val_iou: 0.5911 - val_recall_5: 0.6788 - val_precision_5: 0.8028 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0558 - dice_coef: 0.9634 - iou: 0.9297 - recall_5: 0.9294 - precision_5: 0.9817\n",
            "Epoch 78: val_loss improved from 0.28582 to 0.28467, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.0558 - dice_coef: 0.9634 - iou: 0.9297 - recall_5: 0.9294 - precision_5: 0.9817 - val_loss: 0.2847 - val_dice_coef: 0.7363 - val_iou: 0.5931 - val_recall_5: 0.6840 - val_precision_5: 0.8003 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0555 - dice_coef: 0.9635 - iou: 0.9297 - recall_5: 0.9299 - precision_5: 0.9816\n",
            "Epoch 79: val_loss did not improve from 0.28467\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0555 - dice_coef: 0.9635 - iou: 0.9297 - recall_5: 0.9299 - precision_5: 0.9816 - val_loss: 0.2863 - val_dice_coef: 0.7342 - val_iou: 0.5901 - val_recall_5: 0.6775 - val_precision_5: 0.8023 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0551 - dice_coef: 0.9637 - iou: 0.9301 - recall_5: 0.9297 - precision_5: 0.9818\n",
            "Epoch 80: val_loss improved from 0.28467 to 0.28245, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0551 - dice_coef: 0.9637 - iou: 0.9301 - recall_5: 0.9297 - precision_5: 0.9818 - val_loss: 0.2824 - val_dice_coef: 0.7378 - val_iou: 0.5945 - val_recall_5: 0.6859 - val_precision_5: 0.8007 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0545 - dice_coef: 0.9639 - iou: 0.9306 - recall_5: 0.9302 - precision_5: 0.9820\n",
            "Epoch 81: val_loss improved from 0.28245 to 0.28196, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0545 - dice_coef: 0.9639 - iou: 0.9306 - recall_5: 0.9302 - precision_5: 0.9820 - val_loss: 0.2820 - val_dice_coef: 0.7380 - val_iou: 0.5952 - val_recall_5: 0.7021 - val_precision_5: 0.7843 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0541 - dice_coef: 0.9641 - iou: 0.9310 - recall_5: 0.9304 - precision_5: 0.9822\n",
            "Epoch 82: val_loss improved from 0.28196 to 0.28163, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0541 - dice_coef: 0.9641 - iou: 0.9310 - recall_5: 0.9304 - precision_5: 0.9822 - val_loss: 0.2816 - val_dice_coef: 0.7381 - val_iou: 0.5951 - val_recall_5: 0.6813 - val_precision_5: 0.8067 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0537 - dice_coef: 0.9644 - iou: 0.9314 - recall_5: 0.9306 - precision_5: 0.9823\n",
            "Epoch 83: val_loss improved from 0.28163 to 0.27996, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0537 - dice_coef: 0.9644 - iou: 0.9314 - recall_5: 0.9306 - precision_5: 0.9823 - val_loss: 0.2800 - val_dice_coef: 0.7394 - val_iou: 0.5966 - val_recall_5: 0.6871 - val_precision_5: 0.8025 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0533 - dice_coef: 0.9645 - iou: 0.9316 - recall_5: 0.9309 - precision_5: 0.9822\n",
            "Epoch 84: val_loss did not improve from 0.27996\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0533 - dice_coef: 0.9645 - iou: 0.9316 - recall_5: 0.9309 - precision_5: 0.9822 - val_loss: 0.2816 - val_dice_coef: 0.7379 - val_iou: 0.5950 - val_recall_5: 0.6871 - val_precision_5: 0.7998 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0527 - dice_coef: 0.9648 - iou: 0.9323 - recall_5: 0.9310 - precision_5: 0.9827\n",
            "Epoch 85: val_loss did not improve from 0.27996\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0527 - dice_coef: 0.9648 - iou: 0.9323 - recall_5: 0.9310 - precision_5: 0.9827 - val_loss: 0.2802 - val_dice_coef: 0.7386 - val_iou: 0.5953 - val_recall_5: 0.6907 - val_precision_5: 0.7964 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0525 - dice_coef: 0.9648 - iou: 0.9322 - recall_5: 0.9314 - precision_5: 0.9824\n",
            "Epoch 86: val_loss did not improve from 0.27996\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0525 - dice_coef: 0.9648 - iou: 0.9322 - recall_5: 0.9314 - precision_5: 0.9824 - val_loss: 0.2810 - val_dice_coef: 0.7376 - val_iou: 0.5944 - val_recall_5: 0.6864 - val_precision_5: 0.7997 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0520 - dice_coef: 0.9650 - iou: 0.9326 - recall_5: 0.9311 - precision_5: 0.9830\n",
            "Epoch 87: val_loss improved from 0.27996 to 0.27960, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0520 - dice_coef: 0.9650 - iou: 0.9326 - recall_5: 0.9311 - precision_5: 0.9830 - val_loss: 0.2796 - val_dice_coef: 0.7388 - val_iou: 0.5960 - val_recall_5: 0.6946 - val_precision_5: 0.7930 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0514 - dice_coef: 0.9654 - iou: 0.9334 - recall_5: 0.9321 - precision_5: 0.9827\n",
            "Epoch 88: val_loss improved from 0.27960 to 0.27866, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0514 - dice_coef: 0.9654 - iou: 0.9334 - recall_5: 0.9321 - precision_5: 0.9827 - val_loss: 0.2787 - val_dice_coef: 0.7399 - val_iou: 0.5973 - val_recall_5: 0.6939 - val_precision_5: 0.7954 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0511 - dice_coef: 0.9656 - iou: 0.9336 - recall_5: 0.9320 - precision_5: 0.9830\n",
            "Epoch 89: val_loss did not improve from 0.27866\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0511 - dice_coef: 0.9656 - iou: 0.9336 - recall_5: 0.9320 - precision_5: 0.9830 - val_loss: 0.2790 - val_dice_coef: 0.7391 - val_iou: 0.5965 - val_recall_5: 0.6943 - val_precision_5: 0.7954 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0505 - dice_coef: 0.9658 - iou: 0.9341 - recall_5: 0.9326 - precision_5: 0.9830\n",
            "Epoch 90: val_loss improved from 0.27866 to 0.27859, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 11s 120ms/step - loss: 0.0505 - dice_coef: 0.9658 - iou: 0.9341 - recall_5: 0.9326 - precision_5: 0.9830 - val_loss: 0.2786 - val_dice_coef: 0.7391 - val_iou: 0.5965 - val_recall_5: 0.6874 - val_precision_5: 0.8024 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0500 - dice_coef: 0.9661 - iou: 0.9346 - recall_5: 0.9325 - precision_5: 0.9835\n",
            "Epoch 91: val_loss did not improve from 0.27859\n",
            "93/93 [==============================] - 10s 102ms/step - loss: 0.0500 - dice_coef: 0.9661 - iou: 0.9346 - recall_5: 0.9325 - precision_5: 0.9835 - val_loss: 0.2816 - val_dice_coef: 0.7362 - val_iou: 0.5924 - val_recall_5: 0.6843 - val_precision_5: 0.7982 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0496 - dice_coef: 0.9663 - iou: 0.9350 - recall_5: 0.9331 - precision_5: 0.9835\n",
            "Epoch 92: val_loss improved from 0.27859 to 0.27793, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 113ms/step - loss: 0.0496 - dice_coef: 0.9663 - iou: 0.9350 - recall_5: 0.9331 - precision_5: 0.9835 - val_loss: 0.2779 - val_dice_coef: 0.7396 - val_iou: 0.5966 - val_recall_5: 0.7123 - val_precision_5: 0.7753 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0494 - dice_coef: 0.9663 - iou: 0.9351 - recall_5: 0.9328 - precision_5: 0.9837\n",
            "Epoch 93: val_loss did not improve from 0.27793\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0494 - dice_coef: 0.9663 - iou: 0.9351 - recall_5: 0.9328 - precision_5: 0.9837 - val_loss: 0.2792 - val_dice_coef: 0.7380 - val_iou: 0.5949 - val_recall_5: 0.6784 - val_precision_5: 0.8099 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0490 - dice_coef: 0.9665 - iou: 0.9354 - recall_5: 0.9332 - precision_5: 0.9836\n",
            "Epoch 94: val_loss improved from 0.27793 to 0.27587, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.0490 - dice_coef: 0.9665 - iou: 0.9354 - recall_5: 0.9332 - precision_5: 0.9836 - val_loss: 0.2759 - val_dice_coef: 0.7414 - val_iou: 0.5993 - val_recall_5: 0.7222 - val_precision_5: 0.7683 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0485 - dice_coef: 0.9668 - iou: 0.9359 - recall_5: 0.9335 - precision_5: 0.9837\n",
            "Epoch 95: val_loss improved from 0.27587 to 0.27495, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0485 - dice_coef: 0.9668 - iou: 0.9359 - recall_5: 0.9335 - precision_5: 0.9837 - val_loss: 0.2750 - val_dice_coef: 0.7420 - val_iou: 0.5998 - val_recall_5: 0.7129 - val_precision_5: 0.7805 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0481 - dice_coef: 0.9669 - iou: 0.9362 - recall_5: 0.9334 - precision_5: 0.9841\n",
            "Epoch 96: val_loss did not improve from 0.27495\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0481 - dice_coef: 0.9669 - iou: 0.9362 - recall_5: 0.9334 - precision_5: 0.9841 - val_loss: 0.2825 - val_dice_coef: 0.7345 - val_iou: 0.5918 - val_recall_5: 0.6679 - val_precision_5: 0.8181 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0478 - dice_coef: 0.9671 - iou: 0.9365 - recall_5: 0.9337 - precision_5: 0.9841\n",
            "Epoch 97: val_loss did not improve from 0.27495\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0478 - dice_coef: 0.9671 - iou: 0.9365 - recall_5: 0.9337 - precision_5: 0.9841 - val_loss: 0.2790 - val_dice_coef: 0.7373 - val_iou: 0.5945 - val_recall_5: 0.6651 - val_precision_5: 0.8249 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0475 - dice_coef: 0.9671 - iou: 0.9365 - recall_5: 0.9339 - precision_5: 0.9842\n",
            "Epoch 98: val_loss improved from 0.27495 to 0.27454, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0475 - dice_coef: 0.9671 - iou: 0.9365 - recall_5: 0.9339 - precision_5: 0.9842 - val_loss: 0.2745 - val_dice_coef: 0.7417 - val_iou: 0.5993 - val_recall_5: 0.7099 - val_precision_5: 0.7813 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0471 - dice_coef: 0.9674 - iou: 0.9370 - recall_5: 0.9344 - precision_5: 0.9842\n",
            "Epoch 99: val_loss did not improve from 0.27454\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0471 - dice_coef: 0.9674 - iou: 0.9370 - recall_5: 0.9344 - precision_5: 0.9842 - val_loss: 0.2778 - val_dice_coef: 0.7385 - val_iou: 0.5958 - val_recall_5: 0.6764 - val_precision_5: 0.8109 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0466 - dice_coef: 0.9677 - iou: 0.9375 - recall_5: 0.9343 - precision_5: 0.9845\n",
            "Epoch 100: val_loss did not improve from 0.27454\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0466 - dice_coef: 0.9677 - iou: 0.9375 - recall_5: 0.9343 - precision_5: 0.9845 - val_loss: 0.2751 - val_dice_coef: 0.7407 - val_iou: 0.5976 - val_recall_5: 0.7005 - val_precision_5: 0.7874 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0464 - dice_coef: 0.9677 - iou: 0.9375 - recall_5: 0.9346 - precision_5: 0.9843\n",
            "Epoch 101: val_loss did not improve from 0.27454\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0464 - dice_coef: 0.9677 - iou: 0.9375 - recall_5: 0.9346 - precision_5: 0.9843 - val_loss: 0.2890 - val_dice_coef: 0.7266 - val_iou: 0.5825 - val_recall_5: 0.6380 - val_precision_5: 0.8380 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0459 - dice_coef: 0.9681 - iou: 0.9383 - recall_5: 0.9351 - precision_5: 0.9845\n",
            "Epoch 102: val_loss did not improve from 0.27454\n",
            "93/93 [==============================] - 9s 100ms/step - loss: 0.0459 - dice_coef: 0.9681 - iou: 0.9383 - recall_5: 0.9351 - precision_5: 0.9845 - val_loss: 0.2800 - val_dice_coef: 0.7358 - val_iou: 0.5935 - val_recall_5: 0.6954 - val_precision_5: 0.7885 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0454 - dice_coef: 0.9684 - iou: 0.9388 - recall_5: 0.9354 - precision_5: 0.9848\n",
            "Epoch 103: val_loss improved from 0.27454 to 0.27365, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0454 - dice_coef: 0.9684 - iou: 0.9388 - recall_5: 0.9354 - precision_5: 0.9848 - val_loss: 0.2736 - val_dice_coef: 0.7415 - val_iou: 0.5994 - val_recall_5: 0.6979 - val_precision_5: 0.7968 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0452 - dice_coef: 0.9684 - iou: 0.9388 - recall_5: 0.9353 - precision_5: 0.9848\n",
            "Epoch 104: val_loss improved from 0.27365 to 0.27303, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0452 - dice_coef: 0.9684 - iou: 0.9388 - recall_5: 0.9353 - precision_5: 0.9848 - val_loss: 0.2730 - val_dice_coef: 0.7419 - val_iou: 0.5990 - val_recall_5: 0.7084 - val_precision_5: 0.7831 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0447 - dice_coef: 0.9687 - iou: 0.9395 - recall_5: 0.9357 - precision_5: 0.9852\n",
            "Epoch 105: val_loss improved from 0.27303 to 0.27294, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 111ms/step - loss: 0.0447 - dice_coef: 0.9687 - iou: 0.9395 - recall_5: 0.9357 - precision_5: 0.9852 - val_loss: 0.2729 - val_dice_coef: 0.7417 - val_iou: 0.5991 - val_recall_5: 0.6786 - val_precision_5: 0.8163 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0446 - dice_coef: 0.9687 - iou: 0.9394 - recall_5: 0.9356 - precision_5: 0.9851\n",
            "Epoch 106: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 102ms/step - loss: 0.0446 - dice_coef: 0.9687 - iou: 0.9394 - recall_5: 0.9356 - precision_5: 0.9851 - val_loss: 0.2754 - val_dice_coef: 0.7398 - val_iou: 0.5974 - val_recall_5: 0.6875 - val_precision_5: 0.8046 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0444 - dice_coef: 0.9687 - iou: 0.9395 - recall_5: 0.9357 - precision_5: 0.9849\n",
            "Epoch 107: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0444 - dice_coef: 0.9687 - iou: 0.9395 - recall_5: 0.9357 - precision_5: 0.9849 - val_loss: 0.2760 - val_dice_coef: 0.7386 - val_iou: 0.5961 - val_recall_5: 0.6858 - val_precision_5: 0.8018 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0441 - dice_coef: 0.9689 - iou: 0.9398 - recall_5: 0.9357 - precision_5: 0.9853\n",
            "Epoch 108: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0441 - dice_coef: 0.9689 - iou: 0.9398 - recall_5: 0.9357 - precision_5: 0.9853 - val_loss: 0.2735 - val_dice_coef: 0.7408 - val_iou: 0.5988 - val_recall_5: 0.6913 - val_precision_5: 0.8016 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0436 - dice_coef: 0.9692 - iou: 0.9404 - recall_5: 0.9366 - precision_5: 0.9853\n",
            "Epoch 109: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 103ms/step - loss: 0.0436 - dice_coef: 0.9692 - iou: 0.9404 - recall_5: 0.9366 - precision_5: 0.9853 - val_loss: 0.2777 - val_dice_coef: 0.7368 - val_iou: 0.5941 - val_recall_5: 0.6790 - val_precision_5: 0.8074 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0432 - dice_coef: 0.9694 - iou: 0.9408 - recall_5: 0.9364 - precision_5: 0.9856\n",
            "Epoch 110: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0432 - dice_coef: 0.9694 - iou: 0.9408 - recall_5: 0.9364 - precision_5: 0.9856 - val_loss: 0.2746 - val_dice_coef: 0.7403 - val_iou: 0.5970 - val_recall_5: 0.6894 - val_precision_5: 0.8001 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0428 - dice_coef: 0.9697 - iou: 0.9412 - recall_5: 0.9366 - precision_5: 0.9857\n",
            "Epoch 111: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0428 - dice_coef: 0.9697 - iou: 0.9412 - recall_5: 0.9366 - precision_5: 0.9857 - val_loss: 0.2790 - val_dice_coef: 0.7351 - val_iou: 0.5923 - val_recall_5: 0.6803 - val_precision_5: 0.8034 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0425 - dice_coef: 0.9698 - iou: 0.9415 - recall_5: 0.9369 - precision_5: 0.9858\n",
            "Epoch 112: val_loss did not improve from 0.27294\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0425 - dice_coef: 0.9698 - iou: 0.9415 - recall_5: 0.9369 - precision_5: 0.9858 - val_loss: 0.2744 - val_dice_coef: 0.7398 - val_iou: 0.5969 - val_recall_5: 0.6953 - val_precision_5: 0.7917 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0425 - dice_coef: 0.9698 - iou: 0.9414 - recall_5: 0.9367 - precision_5: 0.9856\n",
            "Epoch 113: val_loss improved from 0.27294 to 0.26818, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0425 - dice_coef: 0.9698 - iou: 0.9414 - recall_5: 0.9367 - precision_5: 0.9856 - val_loss: 0.2682 - val_dice_coef: 0.7454 - val_iou: 0.6030 - val_recall_5: 0.7295 - val_precision_5: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0420 - dice_coef: 0.9701 - iou: 0.9421 - recall_5: 0.9370 - precision_5: 0.9861\n",
            "Epoch 114: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0420 - dice_coef: 0.9701 - iou: 0.9421 - recall_5: 0.9370 - precision_5: 0.9861 - val_loss: 0.2777 - val_dice_coef: 0.7363 - val_iou: 0.5929 - val_recall_5: 0.6751 - val_precision_5: 0.8093 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0420 - dice_coef: 0.9699 - iou: 0.9418 - recall_5: 0.9373 - precision_5: 0.9856\n",
            "Epoch 115: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0420 - dice_coef: 0.9699 - iou: 0.9418 - recall_5: 0.9373 - precision_5: 0.9856 - val_loss: 0.2759 - val_dice_coef: 0.7374 - val_iou: 0.5943 - val_recall_5: 0.6970 - val_precision_5: 0.7892 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0416 - dice_coef: 0.9702 - iou: 0.9422 - recall_5: 0.9371 - precision_5: 0.9861\n",
            "Epoch 116: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0416 - dice_coef: 0.9702 - iou: 0.9422 - recall_5: 0.9371 - precision_5: 0.9861 - val_loss: 0.2733 - val_dice_coef: 0.7404 - val_iou: 0.5977 - val_recall_5: 0.7122 - val_precision_5: 0.7791 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0413 - dice_coef: 0.9704 - iou: 0.9427 - recall_5: 0.9378 - precision_5: 0.9860\n",
            "Epoch 117: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0413 - dice_coef: 0.9704 - iou: 0.9427 - recall_5: 0.9378 - precision_5: 0.9860 - val_loss: 0.2697 - val_dice_coef: 0.7434 - val_iou: 0.6011 - val_recall_5: 0.7006 - val_precision_5: 0.7948 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0412 - dice_coef: 0.9704 - iou: 0.9426 - recall_5: 0.9370 - precision_5: 0.9864\n",
            "Epoch 118: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0412 - dice_coef: 0.9704 - iou: 0.9426 - recall_5: 0.9370 - precision_5: 0.9864 - val_loss: 0.2795 - val_dice_coef: 0.7339 - val_iou: 0.5909 - val_recall_5: 0.6556 - val_precision_5: 0.8319 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0405 - dice_coef: 0.9710 - iou: 0.9437 - recall_5: 0.9383 - precision_5: 0.9864\n",
            "Epoch 119: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0405 - dice_coef: 0.9710 - iou: 0.9437 - recall_5: 0.9383 - precision_5: 0.9864 - val_loss: 0.2771 - val_dice_coef: 0.7360 - val_iou: 0.5936 - val_recall_5: 0.6637 - val_precision_5: 0.8246 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0403 - dice_coef: 0.9710 - iou: 0.9438 - recall_5: 0.9386 - precision_5: 0.9864\n",
            "Epoch 120: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 102ms/step - loss: 0.0403 - dice_coef: 0.9710 - iou: 0.9438 - recall_5: 0.9386 - precision_5: 0.9864 - val_loss: 0.2691 - val_dice_coef: 0.7436 - val_iou: 0.6022 - val_recall_5: 0.7246 - val_precision_5: 0.7723 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0402 - dice_coef: 0.9710 - iou: 0.9437 - recall_5: 0.9380 - precision_5: 0.9866\n",
            "Epoch 121: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0402 - dice_coef: 0.9710 - iou: 0.9437 - recall_5: 0.9380 - precision_5: 0.9866 - val_loss: 0.2723 - val_dice_coef: 0.7404 - val_iou: 0.5980 - val_recall_5: 0.6935 - val_precision_5: 0.7978 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0399 - dice_coef: 0.9712 - iou: 0.9442 - recall_5: 0.9386 - precision_5: 0.9865\n",
            "Epoch 122: val_loss did not improve from 0.26818\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0399 - dice_coef: 0.9712 - iou: 0.9442 - recall_5: 0.9386 - precision_5: 0.9865 - val_loss: 0.2762 - val_dice_coef: 0.7362 - val_iou: 0.5930 - val_recall_5: 0.6751 - val_precision_5: 0.8085 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0399 - dice_coef: 0.9711 - iou: 0.9439 - recall_5: 0.9384 - precision_5: 0.9866\n",
            "Epoch 123: val_loss improved from 0.26818 to 0.26709, saving model to files/A/model.h5\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0399 - dice_coef: 0.9711 - iou: 0.9439 - recall_5: 0.9384 - precision_5: 0.9866 - val_loss: 0.2671 - val_dice_coef: 0.7448 - val_iou: 0.6029 - val_recall_5: 0.7242 - val_precision_5: 0.7736 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0396 - dice_coef: 0.9713 - iou: 0.9443 - recall_5: 0.9386 - precision_5: 0.9866\n",
            "Epoch 124: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.0396 - dice_coef: 0.9713 - iou: 0.9443 - recall_5: 0.9386 - precision_5: 0.9866 - val_loss: 0.2739 - val_dice_coef: 0.7383 - val_iou: 0.5938 - val_recall_5: 0.7136 - val_precision_5: 0.7678 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0394 - dice_coef: 0.9713 - iou: 0.9444 - recall_5: 0.9385 - precision_5: 0.9868\n",
            "Epoch 125: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0394 - dice_coef: 0.9713 - iou: 0.9444 - recall_5: 0.9385 - precision_5: 0.9868 - val_loss: 0.2767 - val_dice_coef: 0.7359 - val_iou: 0.5930 - val_recall_5: 0.6946 - val_precision_5: 0.7882 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0392 - dice_coef: 0.9714 - iou: 0.9445 - recall_5: 0.9388 - precision_5: 0.9867\n",
            "Epoch 126: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0392 - dice_coef: 0.9714 - iou: 0.9445 - recall_5: 0.9388 - precision_5: 0.9867 - val_loss: 0.2736 - val_dice_coef: 0.7378 - val_iou: 0.5943 - val_recall_5: 0.6742 - val_precision_5: 0.8132 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0392 - dice_coef: 0.9713 - iou: 0.9443 - recall_5: 0.9383 - precision_5: 0.9868\n",
            "Epoch 127: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 9s 101ms/step - loss: 0.0392 - dice_coef: 0.9713 - iou: 0.9443 - recall_5: 0.9383 - precision_5: 0.9868 - val_loss: 0.2674 - val_dice_coef: 0.7446 - val_iou: 0.6023 - val_recall_5: 0.7157 - val_precision_5: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0386 - dice_coef: 0.9719 - iou: 0.9454 - recall_5: 0.9391 - precision_5: 0.9872\n",
            "Epoch 128: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0386 - dice_coef: 0.9719 - iou: 0.9454 - recall_5: 0.9391 - precision_5: 0.9872 - val_loss: 0.2694 - val_dice_coef: 0.7424 - val_iou: 0.5999 - val_recall_5: 0.7476 - val_precision_5: 0.7428 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0386 - dice_coef: 0.9717 - iou: 0.9451 - recall_5: 0.9390 - precision_5: 0.9870\n",
            "Epoch 129: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0386 - dice_coef: 0.9717 - iou: 0.9451 - recall_5: 0.9390 - precision_5: 0.9870 - val_loss: 0.2780 - val_dice_coef: 0.7337 - val_iou: 0.5899 - val_recall_5: 0.7005 - val_precision_5: 0.7757 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0388 - dice_coef: 0.9715 - iou: 0.9447 - recall_5: 0.9385 - precision_5: 0.9867\n",
            "Epoch 130: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0388 - dice_coef: 0.9715 - iou: 0.9447 - recall_5: 0.9385 - precision_5: 0.9867 - val_loss: 0.2736 - val_dice_coef: 0.7383 - val_iou: 0.5939 - val_recall_5: 0.7401 - val_precision_5: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0384 - dice_coef: 0.9718 - iou: 0.9453 - recall_5: 0.9393 - precision_5: 0.9869\n",
            "Epoch 131: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0384 - dice_coef: 0.9718 - iou: 0.9453 - recall_5: 0.9393 - precision_5: 0.9869 - val_loss: 0.2681 - val_dice_coef: 0.7433 - val_iou: 0.6016 - val_recall_5: 0.6959 - val_precision_5: 0.8020 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0379 - dice_coef: 0.9722 - iou: 0.9460 - recall_5: 0.9396 - precision_5: 0.9872\n",
            "Epoch 132: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 106ms/step - loss: 0.0379 - dice_coef: 0.9722 - iou: 0.9460 - recall_5: 0.9396 - precision_5: 0.9872 - val_loss: 0.2746 - val_dice_coef: 0.7369 - val_iou: 0.5924 - val_recall_5: 0.6806 - val_precision_5: 0.8023 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0377 - dice_coef: 0.9723 - iou: 0.9462 - recall_5: 0.9394 - precision_5: 0.9875\n",
            "Epoch 133: val_loss did not improve from 0.26709\n",
            "\n",
            "Epoch 133: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "93/93 [==============================] - 10s 110ms/step - loss: 0.0377 - dice_coef: 0.9723 - iou: 0.9462 - recall_5: 0.9394 - precision_5: 0.9875 - val_loss: 0.2793 - val_dice_coef: 0.7319 - val_iou: 0.5884 - val_recall_5: 0.6935 - val_precision_5: 0.7813 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0377 - dice_coef: 0.9723 - iou: 0.9461 - recall_5: 0.9386 - precision_5: 0.9877\n",
            "Epoch 134: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 102ms/step - loss: 0.0377 - dice_coef: 0.9723 - iou: 0.9461 - recall_5: 0.9386 - precision_5: 0.9877 - val_loss: 0.2755 - val_dice_coef: 0.7359 - val_iou: 0.5936 - val_recall_5: 0.6738 - val_precision_5: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 135/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0373 - dice_coef: 0.9726 - iou: 0.9468 - recall_5: 0.9404 - precision_5: 0.9873\n",
            "Epoch 135: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0373 - dice_coef: 0.9726 - iou: 0.9468 - recall_5: 0.9404 - precision_5: 0.9873 - val_loss: 0.2734 - val_dice_coef: 0.7381 - val_iou: 0.5959 - val_recall_5: 0.6757 - val_precision_5: 0.8147 - lr: 1.0000e-06\n",
            "Epoch 136/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0372 - dice_coef: 0.9727 - iou: 0.9469 - recall_5: 0.9403 - precision_5: 0.9874\n",
            "Epoch 136: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0372 - dice_coef: 0.9727 - iou: 0.9469 - recall_5: 0.9403 - precision_5: 0.9874 - val_loss: 0.2738 - val_dice_coef: 0.7377 - val_iou: 0.5954 - val_recall_5: 0.6758 - val_precision_5: 0.8138 - lr: 1.0000e-06\n",
            "Epoch 137/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9471 - recall_5: 0.9404 - precision_5: 0.9875\n",
            "Epoch 137: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 105ms/step - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9471 - recall_5: 0.9404 - precision_5: 0.9875 - val_loss: 0.2714 - val_dice_coef: 0.7400 - val_iou: 0.5979 - val_recall_5: 0.6847 - val_precision_5: 0.8077 - lr: 1.0000e-06\n",
            "Epoch 138/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9472 - recall_5: 0.9404 - precision_5: 0.9876\n",
            "Epoch 138: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 104ms/step - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9472 - recall_5: 0.9404 - precision_5: 0.9876 - val_loss: 0.2712 - val_dice_coef: 0.7401 - val_iou: 0.5980 - val_recall_5: 0.6894 - val_precision_5: 0.8023 - lr: 1.0000e-06\n",
            "Epoch 139/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0370 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9402 - precision_5: 0.9877\n",
            "Epoch 139: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 109ms/step - loss: 0.0370 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9402 - precision_5: 0.9877 - val_loss: 0.2732 - val_dice_coef: 0.7383 - val_iou: 0.5960 - val_recall_5: 0.6824 - val_precision_5: 0.8073 - lr: 1.0000e-06\n",
            "Epoch 140/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9471 - recall_5: 0.9403 - precision_5: 0.9875\n",
            "Epoch 140: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 108ms/step - loss: 0.0371 - dice_coef: 0.9728 - iou: 0.9471 - recall_5: 0.9403 - precision_5: 0.9875 - val_loss: 0.2721 - val_dice_coef: 0.7393 - val_iou: 0.5970 - val_recall_5: 0.6854 - val_precision_5: 0.8055 - lr: 1.0000e-06\n",
            "Epoch 141/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0370 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9401 - precision_5: 0.9877\n",
            "Epoch 141: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 9s 102ms/step - loss: 0.0370 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9401 - precision_5: 0.9877 - val_loss: 0.2720 - val_dice_coef: 0.7394 - val_iou: 0.5971 - val_recall_5: 0.6832 - val_precision_5: 0.8077 - lr: 1.0000e-06\n",
            "Epoch 142/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0369 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9404 - precision_5: 0.9876\n",
            "Epoch 142: val_loss did not improve from 0.26709\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0369 - dice_coef: 0.9729 - iou: 0.9473 - recall_5: 0.9404 - precision_5: 0.9876 - val_loss: 0.2731 - val_dice_coef: 0.7383 - val_iou: 0.5959 - val_recall_5: 0.6786 - val_precision_5: 0.8114 - lr: 1.0000e-06\n",
            "Epoch 143/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0368 - dice_coef: 0.9730 - iou: 0.9476 - recall_5: 0.9405 - precision_5: 0.9878\n",
            "Epoch 143: val_loss did not improve from 0.26709\n",
            "\n",
            "Epoch 143: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "93/93 [==============================] - 10s 107ms/step - loss: 0.0368 - dice_coef: 0.9730 - iou: 0.9476 - recall_5: 0.9405 - precision_5: 0.9878 - val_loss: 0.2743 - val_dice_coef: 0.7372 - val_iou: 0.5947 - val_recall_5: 0.6756 - val_precision_5: 0.8130 - lr: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "G0qoXyNT9zJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import time\n",
        "from operator import add\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score, f1_score, recall_score, precision_score, accuracy_score, fbeta_score)\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.reshape(-1)\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "\n",
        "    ## Score\n",
        "    score_jaccard = jaccard_score(y_true, y_pred, average='binary')\n",
        "    score_f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    score_recall = recall_score(y_true, y_pred, average='binary')\n",
        "    score_precision = precision_score(y_true, y_pred, average='binary', zero_division=1)\n",
        "    score_acc = accuracy_score(y_true, y_pred)\n",
        "    score_fbeta = fbeta_score(y_true, y_pred, beta=2.0, average='binary', zero_division=1)\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_fbeta]\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    path = \"/content/drive/MyDrive/capstone/Kvasir-SEG\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_test_data(path)\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    size = (256, 256)\n",
        "    input_shape = (256, 256, 3)\n",
        "    model_name = \"A\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    create_dir(f\"results/{model_name}\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    model = load_model_file(model_path)\n",
        "\n",
        "    \"\"\" Sample prediction: To improve FPS \"\"\"\n",
        "    image = np.zeros((1, 256, 256, 3))\n",
        "    mask = model.predict(image)\n",
        "\n",
        "    \"\"\" Testing \"\"\"\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    time_taken = []\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
        "        name = y.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Image \"\"\"\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        image = cv2.resize(image, size)\n",
        "        ori_img = image\n",
        "        image = image/255.0\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image.astype(np.float32)\n",
        "\n",
        "        \"\"\" Mask \"\"\"\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, size)\n",
        "        ori_mask = mask\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        mask = mask/255.0\n",
        "        mask = mask.astype(np.float32)\n",
        "\n",
        "        \"\"\" Time taken \"\"\"\n",
        "        start_time = time.time()\n",
        "        pred_y = model.predict(image)\n",
        "        total_time = time.time() - start_time\n",
        "        time_taken.append(total_time)\n",
        "        print(f\"{name}: {total_time:1.5f}\")\n",
        "\n",
        "        \"\"\" Metrics calculation \"\"\"\n",
        "        score = calculate_metrics(mask, pred_y)\n",
        "        metrics_score = list(map(add, metrics_score, score))\n",
        "\n",
        "        \"\"\" Saving masks \"\"\"\n",
        "        pred_y = pred_y[0] > 0.5\n",
        "        pred_y = pred_y * 255\n",
        "        pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "        ori_img = ori_img\n",
        "        ori_mask = mask_parse(ori_mask)\n",
        "        pred_y = mask_parse(pred_y)\n",
        "        sep_line = np.ones((size[0], 10, 3)) * 255\n",
        "\n",
        "        tmp = [\n",
        "            ori_img, sep_line,\n",
        "            ori_mask, sep_line,\n",
        "            pred_y\n",
        "        ]\n",
        "\n",
        "        cat_images = np.concatenate(tmp, axis=1)\n",
        "        cv2.imwrite(f\"results/{model_name}/{name}.png\", cat_images)\n",
        "\n",
        "    jaccard = metrics_score[0]/len(test_x)\n",
        "    f1 = metrics_score[1]/len(test_x)\n",
        "    recall = metrics_score[2]/len(test_x)\n",
        "    precision = metrics_score[3]/len(test_x)\n",
        "    acc = metrics_score[4]/len(test_x)\n",
        "    f2 = metrics_score[5]/len(test_x)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - F2: {f2:1.4f}\")\n",
        "\n",
        "    mean_time_taken = np.mean(time_taken)\n",
        "    mean_fps = 1/mean_time_taken\n",
        "    print(\"Mean FPS: \", mean_fps)"
      ],
      "metadata": {
        "id": "fJg8vXU79w8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b7d5a0-d545-497e-8f6f-33de61bc3b61"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju87vqa0ndwg0850onjdz7ol: 0.07589\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju87xn2snfmv0987sc3d9xnq: 0.07947\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju87z6o6nh73085045bzsx6o: 0.07774\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju87zv8lni0o0850hbbecbq6: 0.07314\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8828oxnool0801qno9luhr: 0.07341\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju884985nlmx0817vzpax3y4: 0.07366\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7dp3dw2k4n0755zhe003ad: 0.07557\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7dqcwi2dz00850gcmr2ert: 0.07338\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7druhp2gp308715i6km7be: 0.07334\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dsrtb2f8i085064kwugfk: 0.11921\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju7dtb1e2j0t0818deq51ib3: 0.07315\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju7dubap2g0w0801fgl42mg9: 0.07421\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju7dvl5m2n4t0755hlnnjjet: 0.08477\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7dwe282dc309876rco45ts: 0.07307\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju7dxffn2eam0817qxosfwch: 0.06971\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju7dymur2od30755eg8yv2ht: 0.07238\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju7dz5yy2i7z0801ausi7rna: 0.07150\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju7ea4om2l910801bohqjccy: 0.08507\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7ebe962hr409872ovibahw: 0.11625\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7ecl9i2i060987xawjp4l0: 0.07542\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju7eea9b2m0z0801ynqv1fqu: 0.07459\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88oh0po9gq0801nge4tgr1: 0.07373\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju88q6h6obpd0871ckmiabbo: 0.07432\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju88rl5eo94l0850kf5wtrm1: 0.07173\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju88t4fvokxf07558ymyh281: 0.12406\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju88trl3ogi208716qvti51b: 0.07264\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju88v2f9oi8w0871hx9auh01: 0.07862\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju88vx2uoocy075531lc63n3: 0.07213\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju88y1mwoln50871emyfny1g: 0.13790\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju88z8bson4h0871nnd7fdxo: 0.14014\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju890guyoiti098753yg6cdu: 0.14059\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "cju8914beokbf0850isxpocrk: 0.09924\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju892fesoq2g0801n0e0jyia: 0.09964\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju893jmdompz0817xn3g1w4h: 0.11354\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "cju89y9h0puti0818i5yw29e6: 0.15746\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju89z6pqpqfx0817mfv8ixjc: 0.10152\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju8a1jtvpt9m081712iwkca7: 0.10936\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "cju8a2itsq4dv0755ntlovpxe: 0.14789\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju8a3nhbpwnb0850d37fo2na: 0.14002\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "cju8a56vxpy780850r45yu4wk: 0.12282\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "cju8a84g0q76m0818hwiggkod: 0.11995\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8abobpqbir08189u01huru: 0.07726\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8adb60qbiu080188mxpf8d: 0.12209\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8aeei7q8k308173n9y4klv: 0.11802\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8aj01yqeqm0850lhdz3xdw: 0.07761\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8alhigqn2h0801zksudldd: 0.08132\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8amfdtqi4x09871tygrgqe: 0.07606\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8ando2qqdo0818ck7i1be1: 0.07356\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8apjewqrk00801k5d71gky: 0.07395\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8aqq8uqmoq0987hphto9gg: 0.07017\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju8arof2qpf20850ifr1bnqj: 0.12198\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8ashhnquqr0801rwduzt7d: 0.07925\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8at3s1qqqx0850hcq8nmnq: 0.11696\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8auylgqx0z0871u4o4db7o: 0.07712\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8aw9n1qyg10801jkjlmors: 0.07024\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8axq24r4an0755yhv9d4ly: 0.07935\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8ayeq7r1fb0818z1junacy: 0.07570\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8azmhcr66e0755t61atz72: 0.07900\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8b0jr0r2oi0801jiquetd5: 0.08487\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8b1v3br45u087189kku66u: 0.07770\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8b2rmgr52s0801p54eyflx: 0.07765\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8b3ka8r64u0801fh18hk7l: 0.07499\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8b4ja9r2s808509d45ma86: 0.08426\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b542nr81x0871uxnkm9ih: 0.07424\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8b5p40r2c60987ofa0mu03: 0.11597\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8b6rp0r5st0850184f79xt: 0.07811\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8b7aqtr4a00987coba14b7: 0.12086\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8b8yair65w09878pyqtr96: 0.08888\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bafgqrf4x0818twisk3ea: 0.07240\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8bbznkrf5g0871jncffynk: 0.08016\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8bff9nrfi10850fmfzbf8v: 0.11887\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju8bgdmqrksy0801tozdmraa: 0.08213\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bh8surexp0987o5pzklk1: 0.07647\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju8bi8q7rlmn0871abc5ch8k: 0.11959\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bj2ssrmlm0871gc2ug2rs: 0.07540\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bk8oirjhw0817hgkua2w8: 0.08293\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8bljw9rqk20801kr54akrl: 0.07898\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bm24yrrdp081829mbo8ic: 0.07906\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bn7m2rmm70817hgxpb1uq: 0.08561\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bop5jrsid08716i24fqda: 0.07365\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bpctzrqkr0850zeldv9kt: 0.07886\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bqxxurs6i0850mu7mtef9: 0.11755\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8brv16rx7f0818uf5n89pv: 0.07371\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju8bssulrrcy0987h1vq5060: 0.09173\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "cju8buos5rz9b08715lfr0f4f: 0.11690\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bw697rwg308177tg8huas: 0.07800\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8bysfgrzkl081786jwac09: 0.08131\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bzzy2s66m08016z6mouqt: 0.07420\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8c1a0ws7o208181c6lbsom: 0.08081\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8c2rqzs5t80850d0zky5dy: 0.07348\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8c3xs7sauj0801ieyzezr5: 0.08644\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8c5223s8j80850b4kealt4: 0.08115\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8c5mxls96t0850wvkvsity: 0.08297\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8c5zcbsdfz0801o5t6jag1: 0.11329\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8c6hnxsdvr0801wn0vrsa6: 0.11527\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju8c82iosagu0817l74s4m5g: 0.13694\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju8c9akjsdjj0850s67uzlxq: 0.14518\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "cju8ca4geseia0850i2ru11hw: 0.11336\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju8cattbsivm0818p446wgel: 0.14672\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "cju8cbsyssiqj0871gr4jedjp: 0.10861\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "cju7efffp2ivf0817etg3jehl: 0.12306\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "cju6x35ervu2808015c7eoqe4: 0.14501\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju6x4t13vyw60755gtcf9ndu: 0.14124\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "cju6x97w4vwua0850x0997r0a: 0.11052\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6xa0qmvzun0818xjukgncj: 0.10486\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6xifswvwbo0987nibtdr50: 0.13954\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "cju6xlygpw7bs0818n691jsq4: 0.11400\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "cju6xmqd9w0250817l5kxfnsk: 0.15814\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju6ywm40wdbo0987pbftsvtg: 0.08603\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju6yxyt0wh080871sqpepu47: 0.07742\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju6yywx1whbb0871ksgfgf9f: 0.12083\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju6z1bzbwfq50817b2alatvr: 0.11237\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju6z2616wqbk07555bvnuyr1: 0.11910\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju6z600qwh4z081700qimgl9: 0.11810\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju6z7e4bwgdd0987ogkzq9kt: 0.07739\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju6z9a9kwsl007552s49rx6i: 0.08465\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju76erapykj30871x5eaxh4q: 0.07386\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju76l27oyrw907551ri2a7fl: 0.08458\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju76lsehyia10987u54vn8rb: 0.07514\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju76o55nymqd0871h31sph9w: 0.07545\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju77196iyshb0850ycbto50a: 0.12294\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju772304yw5t0818vbw8kkjf: 0.07834\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju773hsyyosz0817pk1e7sjq: 0.07564\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju774fmayxif0818u2g79usw: 0.08195\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7787c5yy3l080159mwqsnj: 0.07565\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju77afzlz3kp07550x5nafzs: 0.08513\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju77b3wyz4160755qis4ljsb: 0.07948\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju77bvg0yv4r0987yh60xmjo: 0.07437\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju77g99iyxc00817zqi2ppor: 0.07924\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju77idwfz36d0871tzfzz51i: 0.11907\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju77j66ez52p08019xygi0co: 0.08414\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju77k828z46w0871r0avuoo9: 0.07723\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju77q10sz9ug0801449wu1nu: 0.08752\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju77re6fz5bb0817vp9redjg: 0.08283\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju77t0razbvm080106o56289: 0.13498\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju77u1sjz77b0817ft44r3fk: 0.08078\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju77vvcwzcm50850lzoykuva: 0.07684\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju783tmkzkqu081803g7q5vk: 0.11961\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju784jpdzeae0987q5ypq883: 0.08796\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju785htizjzo08017tvlhtg4: 0.07731\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju787jnjzjuj0871p94nck9g: 0.11437\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7adqyj1jcx08712r1ro5gx: 0.11915\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju7ae7bq1f820987toc8si1d: 0.09096\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju7aez2x1jtj0871ztezs3oi: 0.07837\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7afqon1ip40850ue2308b6: 0.07859\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7agj961l2r0818z29iq8yn: 0.07615\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7ahtkb1jr90801jck4kbds: 0.11705\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju7aifyo1p3n07552nxjx51f: 0.08162\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju7ajnbo1gvm098749rdouk0: 0.08205\n",
            "\n",
            "Jaccard: 0.5989 - F1: 0.7056 - Recall: 0.6971 - Precision: 0.8426 - Acc: 0.9348 - F2: 0.6934\n",
            "Mean FPS:  10.649164823152578\n"
          ]
        }
      ]
    }
  ]
}