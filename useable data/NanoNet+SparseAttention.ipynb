{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK2gE4IYpFqu",
        "outputId": "238d9f09-1b68-49db-920a-caca93754f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKetU_kznosl"
      },
      "source": [
        "#metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OOyT5H-zmq5-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + 1e-15) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-15)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return dice_loss(y_true, y_pred) + tf.keras.losses.binary_crossentropy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKC0lspJn3Pd"
      },
      "source": [
        "#sparse attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ArI0wua-n2ch"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Dropout\n",
        "\n",
        "class SparseAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, key_dim, num_heads=1,regularization_coeff=0.01):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.layer_norm1 = None\n",
        "        self.layer_norm2 = None\n",
        "        self.output_dense = None\n",
        "        self.v_dense = None\n",
        "        self.k_dense = None\n",
        "        self.q_dense = None\n",
        "        self.key_dim = key_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.regularization_coeff = regularization_coeff\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.q_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.k_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "        self.v_dense = tf.keras.layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                         kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "\n",
        "        self.output_dense = tf.keras.layers.Dense(input_shape[-1], kernel_regularizer=tf.keras.regularizers.l2(self.regularization_coeff))\n",
        "\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        # Apply Linear Transformation for Query, Key and Value\n",
        "        q = Dropout(0.1)(self.q_dense(x))\n",
        "        k = Dropout(0.1)(self.q_dense(x))\n",
        "        v = Dropout(0.1)(self.q_dense(x))\n",
        "\n",
        "\n",
        "        # Split the heads\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "        # Calculate Attention Score\n",
        "        attn_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_score = attn_score / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
        "\n",
        "        # Apply Sparsemax\n",
        "        attn_score = self.sparsemax(attn_score)\n",
        "\n",
        "        # Apply Dropout after the Dense layer\n",
        "        attn_score = Dropout(0.1)(attn_score)\n",
        "        # Calculate the output value using attention score\n",
        "        attn_values = tf.matmul(attn_score, v)\n",
        "\n",
        "        # Combine the heads back\n",
        "        attn_values = self.combine_heads(attn_values)\n",
        "\n",
        "        # Apply Layer Normalization (First LayerNorm)\n",
        "        attn_values = self.layer_norm1(attn_values)\n",
        "\n",
        "        # Final Linear Transformation\n",
        "        output = self.output_dense(attn_values)\n",
        "\n",
        "        # Apply Layer Normalization (Second LayerNorm, if needed)\n",
        "        output = self.layer_norm2(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def sparsemax(self, logits, axis=-1):\n",
        "        logits = tf.convert_to_tensor(logits)\n",
        "        ob_dim = tf.shape(logits)[axis]\n",
        "        z = tf.sort(logits, axis=axis, direction='DESCENDING')\n",
        "        z_cumsum = tf.cumsum(z, axis=axis)\n",
        "        k = tf.range(1, ob_dim + 1, dtype=tf.float32)\n",
        "        z_check = 1 + k * z >= z_cumsum\n",
        "        k_max = tf.reduce_sum(tf.cast(z_check, tf.float32), axis=axis, keepdims=True)\n",
        "        z_max = tf.gather(z, tf.cast(k_max - 1, tf.int32), batch_dims=len(logits.shape) - 1)\n",
        "        out = tf.nn.relu(logits - z_max)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.key_dim))\n",
        "        return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.transpose(x, [0, 2, 1, 3])\n",
        "        return tf.reshape(x, (batch_size, -1, self.key_dim * self.num_heads))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9XDaRgoCQs"
      },
      "source": [
        "#Se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "csdWOzRcoB5d"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, Add, Permute, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def squeeze_excite_block(input, ratio=16):\n",
        "    ''' Create a channel-wise squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def spatial_squeeze_excite_block(input):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    se = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False,\n",
        "                kernel_initializer='he_normal')(input)\n",
        "\n",
        "    x = Multiply([input, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def channel_spatial_squeeze_excite(input, ratio=16):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    cse = squeeze_excite_block(input, ratio)\n",
        "    sse = spatial_squeeze_excite_block(input)\n",
        "\n",
        "    x = Add([cse, sse])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ZIULAUm6TD"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyi9W__ngsl",
        "outputId": "d2ab3848-97ce-4fad-d4f2-1db01902c4ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 32)         864       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)         (None, 32, 32, 48)           9264      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_190 (B  (None, 32, 32, 48)           192       ['conv2d_200[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_160 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_190[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)         (None, 32, 32, 48)           20784     ['activation_160[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_191 (B  (None, 32, 32, 48)           192       ['conv2d_201[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_161 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_191[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)         (None, 32, 32, 192)          83136     ['activation_161[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_203 (Conv2D)         (None, 32, 32, 192)          37056     ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_192 (B  (None, 32, 32, 192)          768       ['conv2d_202[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_193 (B  (None, 32, 32, 192)          768       ['conv2d_203[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_40 (Add)                (None, 32, 32, 192)          0         ['batch_normalization_192[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_193[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_162 (Activation  (None, 32, 32, 192)          0         ['add_40[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 192)                  0         ['activation_162[0][0]']      \n",
            " 0 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_56 (Reshape)        (None, 1, 1, 192)            0         ['global_average_pooling2d_40[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_80 (Dense)            (None, 1, 1, 12)             2304      ['reshape_56[0][0]']          \n",
            "                                                                                                  \n",
            " dense_81 (Dense)            (None, 1, 1, 192)            2304      ['dense_80[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_40 (Multiply)      (None, 32, 32, 192)          0         ['activation_162[0][0]',      \n",
            "                                                                     'dense_81[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_57 (Reshape)        (None, 1024, 192)            0         ['multiply_40[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_12 (Spars  (None, None, 192)            148800    ['reshape_57[0][0]']          \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_58 (Reshape)        (None, 32, 32, 192)          0         ['sparse_attention_12[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_194 (B  (None, 64, 64, 128)          512       ['conv2d_204[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_30 (UpSampli  (None, 64, 64, 192)          0         ['reshape_58[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_163 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_194[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenat  (None, 64, 64, 320)          0         ['up_sampling2d_30[0][0]',    \n",
            " e)                                                                  'activation_163[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)         (None, 64, 64, 32)           10272     ['concatenate_30[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_195 (B  (None, 64, 64, 32)           128       ['conv2d_205[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_164 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_195[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_164[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_196 (B  (None, 64, 64, 32)           128       ['conv2d_206[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_165 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_196[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_165[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)         (None, 64, 64, 128)          41088     ['concatenate_30[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_197 (B  (None, 64, 64, 128)          512       ['conv2d_207[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_198 (B  (None, 64, 64, 128)          512       ['conv2d_208[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_41 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_197[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_198[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_166 (Activation  (None, 64, 64, 128)          0         ['add_41[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 128)                  0         ['activation_166[0][0]']      \n",
            " 1 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_59 (Reshape)        (None, 1, 1, 128)            0         ['global_average_pooling2d_41[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_82 (Dense)            (None, 1, 1, 8)              1024      ['reshape_59[0][0]']          \n",
            "                                                                                                  \n",
            " dense_83 (Dense)            (None, 1, 1, 128)            1024      ['dense_82[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_41 (Multiply)      (None, 64, 64, 128)          0         ['activation_166[0][0]',      \n",
            "                                                                     'dense_83[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_199 (B  (None, 128, 128, 64)         256       ['conv2d_209[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_31 (UpSampli  (None, 128, 128, 128)        0         ['multiply_41[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_167 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_199[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_31[0][0]',    \n",
            " e)                                                                  'activation_167[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_31[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_200 (B  (None, 128, 128, 16)         64        ['conv2d_210[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_168 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_200[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_168[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_201 (B  (None, 128, 128, 16)         64        ['conv2d_211[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_169 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_201[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_169[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)         (None, 128, 128, 64)         12352     ['concatenate_31[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_202 (B  (None, 128, 128, 64)         256       ['conv2d_212[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_203 (B  (None, 128, 128, 64)         256       ['conv2d_213[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_42 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_202[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_203[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_170 (Activation  (None, 128, 128, 64)         0         ['add_42[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 64)                   0         ['activation_170[0][0]']      \n",
            " 2 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_60 (Reshape)        (None, 1, 1, 64)             0         ['global_average_pooling2d_42[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_84 (Dense)            (None, 1, 1, 4)              256       ['reshape_60[0][0]']          \n",
            "                                                                                                  \n",
            " dense_85 (Dense)            (None, 1, 1, 64)             256       ['dense_84[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_42 (Multiply)      (None, 128, 128, 64)         0         ['activation_170[0][0]',      \n",
            "                                                                     'dense_85[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_204 (B  (None, 256, 256, 32)         128       ['conv2d_214[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_32 (UpSampli  (None, 256, 256, 64)         0         ['multiply_42[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_171 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_204[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_32[0][0]',    \n",
            " e)                                                                  'activation_171[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_32[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_205 (B  (None, 256, 256, 8)          32        ['conv2d_215[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_172 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_205[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)         (None, 256, 256, 8)          584       ['activation_172[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_206 (B  (None, 256, 256, 8)          32        ['conv2d_216[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_173 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_206[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_173[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_218 (Conv2D)         (None, 256, 256, 32)         3104      ['concatenate_32[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_207 (B  (None, 256, 256, 32)         128       ['conv2d_217[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_208 (B  (None, 256, 256, 32)         128       ['conv2d_218[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_43 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_207[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_208[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_174 (Activation  (None, 256, 256, 32)         0         ['add_43[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 32)                   0         ['activation_174[0][0]']      \n",
            " 3 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_61 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_43[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_86 (Dense)            (None, 1, 1, 2)              64        ['reshape_61[0][0]']          \n",
            "                                                                                                  \n",
            " dense_87 (Dense)            (None, 1, 1, 32)             64        ['dense_86[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_43 (Multiply)      (None, 256, 256, 32)         0         ['activation_174[0][0]',      \n",
            "                                                                     'dense_87[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_43[0][0]']         \n",
            "                                                                                                  \n",
            " activation_175 (Activation  (None, 256, 256, 1)          0         ['conv2d_219[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 533681 (2.04 MB)\n",
            "Trainable params: 527249 (2.01 MB)\n",
            "Non-trainable params: 6432 (25.12 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape\n",
        "os.environ['SSL_CERT_DIR'] = '/etc/ssl/certs'\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Multiply, Add, BatchNormalization, Activation\n",
        "from keras.layers import Cropping2D,UpSampling2D, Input, Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def residual_block(x, num_filters):\n",
        "    x_init = x\n",
        "    x = Conv2D(num_filters//4, (1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters//4, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\", kernel_regularizer=l2(0.02))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    s = Conv2D(num_filters, (1, 1), padding=\"same\")(x_init)\n",
        "    s = BatchNormalization()(s)\n",
        "\n",
        "    x = Add()([x, s])\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_excite_block(x)\n",
        "    return x\n",
        "\n",
        "def NanoNet_A_Sparse_Attention(input_shape):\n",
        "    f = [32, 64, 128]\n",
        "    inputs = Input(shape=input_shape, name=\"input_image\")\n",
        "\n",
        "    # Encoder: MobileNetV2\n",
        "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=1)\n",
        "    encoder_output = encoder.get_layer(name=\"block_6_expand_relu\").output\n",
        "    skip_connections_name = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\"]\n",
        "\n",
        "    x = residual_block(encoder_output, 192)  # Residual Block\n",
        "\n",
        "\n",
        "    # SparseAttention Layer\n",
        "    transformer_shape = (x.shape[1], x.shape[2], x.shape[3])\n",
        "    x = Reshape((transformer_shape[0] * transformer_shape[1], transformer_shape[2]))(x)\n",
        "    x = SparseAttention(key_dim=transformer_shape[2], num_heads=2)(x)\n",
        "    x = Reshape((transformer_shape[0], transformer_shape[1], transformer_shape[2]))(x)\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(1, len(skip_connections_name) + 1, 1):\n",
        "        x_skip = encoder.get_layer(skip_connections_name[-i]).output\n",
        "        x_skip = Conv2D(f[-i], (1, 1), padding=\"same\")(x_skip)\n",
        "        x_skip = BatchNormalization()(x_skip)\n",
        "        x_skip = Activation(\"relu\")(x_skip)\n",
        "\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "\n",
        "        try:\n",
        "            x = Concatenate()([x, x_skip])\n",
        "        except Exception as e:\n",
        "            x = Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "            x = Concatenate()([x, x_skip])\n",
        "\n",
        "        x = residual_block(x, f[-i])\n",
        "    # Output layer\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    params = {\"img_height\": 256, \"img_width\": 256, \"img_channels\": 3, \"mask_channels\": 1}\n",
        "    input_shape = (params[\"img_height\"], params[\"img_width\"], params[\"img_channels\"])\n",
        "    model = NanoNet_A_Sparse_Attention(input_shape)\n",
        "    model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaVq7EoOvQFH"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IYnZY6mivWl3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from dimensionality_reduction import apply_pca_to_image,reduce_mask_dimension,save_image\n",
        "\n",
        "H = 256\n",
        "W = 256\n",
        "\n",
        "def load_names(path, file_path):\n",
        "    f = open(file_path, \"r\")\n",
        "    data = f.read().split(\"\\n\")[:-1]\n",
        "    images = [os.path.join(path, \"images\", name) + \".jpg\" for name in data]\n",
        "    masks = [os.path.join(path, \"masks\", name) + \".jpg\" for name in data]\n",
        "    return images, masks\n",
        "\n",
        "def load_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    valid_names_path = f\"{path}/val.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    valid_x, valid_y = load_names(path, valid_names_path)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y)\n",
        "\n",
        "def load_test_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    test_names_path = f\"{path}/test.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    test_x, test_y = load_names(path, test_names_path)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "def read_image(path):\n",
        "\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_image2(img_path):\n",
        "    return np.load(img_path)\n",
        "\n",
        "def read_mask2(mask_path):\n",
        "    return np.load(mask_path)\n",
        "\n",
        "\n",
        "\n",
        "# def augment_data(image, mask):\n",
        "#     # Random horizontal flip\n",
        "#     if tf.random.uniform(()) > 0.5:\n",
        "#         image = tf.image.flip_left_right(image)\n",
        "#         mask = tf.image.flip_left_right(mask)\n",
        "\n",
        "#     # Random vertical flip\n",
        "#     if tf.random.uniform(()) > 0.5:\n",
        "#         image = tf.image.flip_up_down(image)\n",
        "#         mask = tf.image.flip_up_down(mask)\n",
        "\n",
        "#     # Random rotation (in 90-degree increments)\n",
        "#     num_rotations = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)\n",
        "#     image = tf.image.rot90(image, k=num_rotations)\n",
        "#     mask = tf.image.rot90(mask, k=num_rotations)\n",
        "\n",
        "#     # Intensity-based augmentation\n",
        "\n",
        "#     # RGB to HSV\n",
        "#     image_hsv = tf.image.rgb_to_hsv(image)\n",
        "\n",
        "#     # Do some operations in HSV space, adjust saturation\n",
        "\n",
        "#     delta = 0.2\n",
        "#     image_hsv = tf.stack([\n",
        "#         image_hsv[:, :, 0],  # Hue\n",
        "#         tf.clip_by_value(image_hsv[:, :, 1] + delta, 0, 1),  # Saturation\n",
        "#         image_hsv[:, :, 2],  # Value\n",
        "#     ], axis=-1)\n",
        "\n",
        "#     # Convert back to RGB\n",
        "#     image_rgb = tf.image.hsv_to_rgb(image_hsv)\n",
        "\n",
        "#     return image_rgb, mask\n",
        "\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    # x, y = augment_data(x, y)\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch_size=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1000)  # Scramble data\n",
        "    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.cache()  # Cache data\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def process_dataset(image_paths, mask_paths, save_image_dir, save_mask_dir):\n",
        "    processed_image_paths = []\n",
        "    processed_mask_paths = []\n",
        "\n",
        "    for i, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
        "        img = read_image2(img_path)\n",
        "        mask = read_mask2(mask_path)\n",
        "\n",
        "        processed_img = apply_pca_to_image(img)\n",
        "        processed_mask = reduce_mask_dimension(mask)\n",
        "\n",
        "        processed_img_path = os.path.join(save_image_dir, f\"processed_image_{i}.jpg\")\n",
        "        processed_mask_path = os.path.join(save_mask_dir, f\"processed_mask_{i}.jpg\")\n",
        "\n",
        "        save_image(processed_img, processed_img_path)\n",
        "        save_image(processed_mask, processed_mask_path)\n",
        "\n",
        "        processed_image_paths.append(processed_img_path)\n",
        "        processed_mask_paths.append(processed_mask_path)\n",
        "\n",
        "    return processed_image_paths, processed_mask_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHOMge78ve6a"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "FkXytfLvvkPW"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import CustomObjectScope\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import load_model\n",
        "from keras.utils import custom_object_scope\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "    except OSError:\n",
        "        print(f\"Error: creating directory with name {path}\")\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def load_model_file(path):\n",
        "    with CustomObjectScope({\n",
        "            'iou': iou,\n",
        "            'dice_coef': dice_coef,\n",
        "            'dice_loss': dice_loss,\n",
        "            'bce_dice_loss': bce_dice_loss,\n",
        "            'SparseAttention': SparseAttention  #  SparseAttention\n",
        "        }):\n",
        "        model = tf.keras.models.load_model(path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URswsGAAv4pO"
      },
      "source": [
        "# sgdr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DFBfW-Gev7Lo"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "class SGDRScheduler(Callback):\n",
        "\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DLQuj1MvtK4"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejrVNs_vzm6",
        "outputId": "7d13b537-ff02-4610-fa89-5bb565f66573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 32)         864       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)         (None, 32, 32, 48)           9264      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_209 (B  (None, 32, 32, 48)           192       ['conv2d_220[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_176 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_209[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)         (None, 32, 32, 48)           20784     ['activation_176[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_210 (B  (None, 32, 32, 48)           192       ['conv2d_221[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_177 (Activation  (None, 32, 32, 48)           0         ['batch_normalization_210[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)         (None, 32, 32, 192)          83136     ['activation_177[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_223 (Conv2D)         (None, 32, 32, 192)          37056     ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_211 (B  (None, 32, 32, 192)          768       ['conv2d_222[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_212 (B  (None, 32, 32, 192)          768       ['conv2d_223[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_44 (Add)                (None, 32, 32, 192)          0         ['batch_normalization_211[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_212[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_178 (Activation  (None, 32, 32, 192)          0         ['add_44[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 192)                  0         ['activation_178[0][0]']      \n",
            " 4 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_62 (Reshape)        (None, 1, 1, 192)            0         ['global_average_pooling2d_44[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_88 (Dense)            (None, 1, 1, 12)             2304      ['reshape_62[0][0]']          \n",
            "                                                                                                  \n",
            " dense_89 (Dense)            (None, 1, 1, 192)            2304      ['dense_88[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_44 (Multiply)      (None, 32, 32, 192)          0         ['activation_178[0][0]',      \n",
            "                                                                     'dense_89[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_63 (Reshape)        (None, 1024, 192)            0         ['multiply_44[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_13 (Spars  (None, None, 192)            148800    ['reshape_63[0][0]']          \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_64 (Reshape)        (None, 32, 32, 192)          0         ['sparse_attention_13[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_213 (B  (None, 64, 64, 128)          512       ['conv2d_224[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_33 (UpSampli  (None, 64, 64, 192)          0         ['reshape_64[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_179 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_213[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenat  (None, 64, 64, 320)          0         ['up_sampling2d_33[0][0]',    \n",
            " e)                                                                  'activation_179[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)         (None, 64, 64, 32)           10272     ['concatenate_33[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_214 (B  (None, 64, 64, 32)           128       ['conv2d_225[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_180 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_214[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_180[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_215 (B  (None, 64, 64, 32)           128       ['conv2d_226[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_181 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_215[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_181[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_228 (Conv2D)         (None, 64, 64, 128)          41088     ['concatenate_33[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_216 (B  (None, 64, 64, 128)          512       ['conv2d_227[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_217 (B  (None, 64, 64, 128)          512       ['conv2d_228[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_45 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_216[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_217[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_182 (Activation  (None, 64, 64, 128)          0         ['add_45[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 128)                  0         ['activation_182[0][0]']      \n",
            " 5 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_65 (Reshape)        (None, 1, 1, 128)            0         ['global_average_pooling2d_45[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_90 (Dense)            (None, 1, 1, 8)              1024      ['reshape_65[0][0]']          \n",
            "                                                                                                  \n",
            " dense_91 (Dense)            (None, 1, 1, 128)            1024      ['dense_90[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_229 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_45 (Multiply)      (None, 64, 64, 128)          0         ['activation_182[0][0]',      \n",
            "                                                                     'dense_91[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_218 (B  (None, 128, 128, 64)         256       ['conv2d_229[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_34 (UpSampli  (None, 128, 128, 128)        0         ['multiply_45[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_183 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_218[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_34[0][0]',    \n",
            " e)                                                                  'activation_183[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_230 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_34[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_219 (B  (None, 128, 128, 16)         64        ['conv2d_230[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_184 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_219[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_231 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_184[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_220 (B  (None, 128, 128, 16)         64        ['conv2d_231[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_185 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_220[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_232 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_185[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_233 (Conv2D)         (None, 128, 128, 64)         12352     ['concatenate_34[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_221 (B  (None, 128, 128, 64)         256       ['conv2d_232[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_222 (B  (None, 128, 128, 64)         256       ['conv2d_233[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_46 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_221[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_222[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_186 (Activation  (None, 128, 128, 64)         0         ['add_46[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 64)                   0         ['activation_186[0][0]']      \n",
            " 6 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_66 (Reshape)        (None, 1, 1, 64)             0         ['global_average_pooling2d_46[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_92 (Dense)            (None, 1, 1, 4)              256       ['reshape_66[0][0]']          \n",
            "                                                                                                  \n",
            " dense_93 (Dense)            (None, 1, 1, 64)             256       ['dense_92[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_234 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_46 (Multiply)      (None, 128, 128, 64)         0         ['activation_186[0][0]',      \n",
            "                                                                     'dense_93[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_223 (B  (None, 256, 256, 32)         128       ['conv2d_234[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_35 (UpSampli  (None, 256, 256, 64)         0         ['multiply_46[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_187 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_223[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_35[0][0]',    \n",
            " e)                                                                  'activation_187[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_235 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_35[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_224 (B  (None, 256, 256, 8)          32        ['conv2d_235[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_188 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_224[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_236 (Conv2D)         (None, 256, 256, 8)          584       ['activation_188[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_225 (B  (None, 256, 256, 8)          32        ['conv2d_236[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_189 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_225[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_237 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_189[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_238 (Conv2D)         (None, 256, 256, 32)         3104      ['concatenate_35[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_226 (B  (None, 256, 256, 32)         128       ['conv2d_237[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_227 (B  (None, 256, 256, 32)         128       ['conv2d_238[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_47 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_226[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_227[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_190 (Activation  (None, 256, 256, 32)         0         ['add_47[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 32)                   0         ['activation_190[0][0]']      \n",
            " 7 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_67 (Reshape)        (None, 1, 1, 32)             0         ['global_average_pooling2d_47[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_94 (Dense)            (None, 1, 1, 2)              64        ['reshape_67[0][0]']          \n",
            "                                                                                                  \n",
            " dense_95 (Dense)            (None, 1, 1, 32)             64        ['dense_94[0][0]']            \n",
            "                                                                                                  \n",
            " multiply_47 (Multiply)      (None, 256, 256, 32)         0         ['activation_190[0][0]',      \n",
            "                                                                     'dense_95[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_239 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_47[0][0]']         \n",
            "                                                                                                  \n",
            " activation_191 (Activation  (None, 256, 256, 1)          0         ['conv2d_239[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 533681 (2.04 MB)\n",
            "Trainable params: 527249 (2.01 MB)\n",
            "Non-trainable params: 6432 (25.12 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 8.6434 - dice_coef: 0.2711 - iou: 0.1590 - recall_5: 0.7791 - precision_5: 0.2208\n",
            "Epoch 1: val_loss improved from inf to 7.53706, saving model to files/NanoNetA_SparseAttention/model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r98/98 [==============================] - 48s 173ms/step - loss: 8.6434 - dice_coef: 0.2711 - iou: 0.1590 - recall_5: 0.7791 - precision_5: 0.2208 - val_loss: 7.5371 - val_dice_coef: 0.2295 - val_iou: 0.1318 - val_recall_5: 0.8180 - val_precision_5: 0.2227 - lr: 1.0000e-04\n",
            "Epoch 2/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 6.3458 - dice_coef: 0.3656 - iou: 0.2264 - recall_5: 0.7546 - precision_5: 0.4833\n",
            "Epoch 2: val_loss improved from 7.53706 to 5.70456, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 6.3458 - dice_coef: 0.3656 - iou: 0.2264 - recall_5: 0.7546 - precision_5: 0.4833 - val_loss: 5.7046 - val_dice_coef: 0.2577 - val_iou: 0.1508 - val_recall_5: 0.4545 - val_precision_5: 0.6036 - lr: 1.0000e-04\n",
            "Epoch 3/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.7981 - dice_coef: 0.4257 - iou: 0.2734 - recall_5: 0.7785 - precision_5: 0.5980\n",
            "Epoch 3: val_loss improved from 5.70456 to 4.46629, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 4.7981 - dice_coef: 0.4257 - iou: 0.2734 - recall_5: 0.7785 - precision_5: 0.5980 - val_loss: 4.4663 - val_dice_coef: 0.2793 - val_iou: 0.1655 - val_recall_5: 0.4982 - val_precision_5: 0.6393 - lr: 1.0000e-04\n",
            "Epoch 4/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 3.7219 - dice_coef: 0.4697 - iou: 0.3103 - recall_5: 0.7981 - precision_5: 0.6571\n",
            "Epoch 4: val_loss improved from 4.46629 to 3.63174, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 127ms/step - loss: 3.7219 - dice_coef: 0.4697 - iou: 0.3103 - recall_5: 0.7981 - precision_5: 0.6571 - val_loss: 3.6317 - val_dice_coef: 0.2912 - val_iou: 0.1737 - val_recall_5: 0.5508 - val_precision_5: 0.6317 - lr: 1.0000e-04\n",
            "Epoch 5/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 2.9490 - dice_coef: 0.5085 - iou: 0.3445 - recall_5: 0.8067 - precision_5: 0.7129\n",
            "Epoch 5: val_loss improved from 3.63174 to 3.01015, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 13s 129ms/step - loss: 2.9490 - dice_coef: 0.5085 - iou: 0.3445 - recall_5: 0.8067 - precision_5: 0.7129 - val_loss: 3.0101 - val_dice_coef: 0.3248 - val_iou: 0.1981 - val_recall_5: 0.6658 - val_precision_5: 0.5766 - lr: 1.0000e-04\n",
            "Epoch 6/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 2.3772 - dice_coef: 0.5472 - iou: 0.3806 - recall_5: 0.8171 - precision_5: 0.7614\n",
            "Epoch 6: val_loss improved from 3.01015 to 2.62923, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 2.3772 - dice_coef: 0.5472 - iou: 0.3806 - recall_5: 0.8171 - precision_5: 0.7614 - val_loss: 2.6292 - val_dice_coef: 0.3545 - val_iou: 0.2203 - val_recall_5: 0.8326 - val_precision_5: 0.4290 - lr: 1.0000e-04\n",
            "Epoch 7/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.9483 - dice_coef: 0.5845 - iou: 0.4172 - recall_5: 0.8283 - precision_5: 0.8036\n",
            "Epoch 7: val_loss improved from 2.62923 to 2.19438, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 1.9483 - dice_coef: 0.5845 - iou: 0.4172 - recall_5: 0.8283 - precision_5: 0.8036 - val_loss: 2.1944 - val_dice_coef: 0.3640 - val_iou: 0.2275 - val_recall_5: 0.6110 - val_precision_5: 0.6367 - lr: 1.0000e-04\n",
            "Epoch 8/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.6322 - dice_coef: 0.6163 - iou: 0.4497 - recall_5: 0.8297 - precision_5: 0.8295\n",
            "Epoch 8: val_loss improved from 2.19438 to 1.89914, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 1.6322 - dice_coef: 0.6163 - iou: 0.4497 - recall_5: 0.8297 - precision_5: 0.8295 - val_loss: 1.8991 - val_dice_coef: 0.4147 - val_iou: 0.2665 - val_recall_5: 0.7311 - val_precision_5: 0.5697 - lr: 1.0000e-04\n",
            "Epoch 9/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.3796 - dice_coef: 0.6554 - iou: 0.4917 - recall_5: 0.8333 - precision_5: 0.8511\n",
            "Epoch 9: val_loss improved from 1.89914 to 1.59489, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 1.3796 - dice_coef: 0.6554 - iou: 0.4917 - recall_5: 0.8333 - precision_5: 0.8511 - val_loss: 1.5949 - val_dice_coef: 0.4980 - val_iou: 0.3356 - val_recall_5: 0.6779 - val_precision_5: 0.5820 - lr: 1.0000e-04\n",
            "Epoch 10/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.1858 - dice_coef: 0.6918 - iou: 0.5330 - recall_5: 0.8345 - precision_5: 0.8660\n",
            "Epoch 10: val_loss improved from 1.59489 to 1.57381, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 1.1858 - dice_coef: 0.6918 - iou: 0.5330 - recall_5: 0.8345 - precision_5: 0.8660 - val_loss: 1.5738 - val_dice_coef: 0.4954 - val_iou: 0.3334 - val_recall_5: 0.8345 - val_precision_5: 0.4800 - lr: 1.0000e-04\n",
            "Epoch 11/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0284 - dice_coef: 0.7271 - iou: 0.5750 - recall_5: 0.8399 - precision_5: 0.8878\n",
            "Epoch 11: val_loss did not improve from 1.57381\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 1.0284 - dice_coef: 0.7271 - iou: 0.5750 - recall_5: 0.8399 - precision_5: 0.8878 - val_loss: 1.6051 - val_dice_coef: 0.4993 - val_iou: 0.3374 - val_recall_5: 0.8319 - val_precision_5: 0.4281 - lr: 1.0000e-04\n",
            "Epoch 12/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.9078 - dice_coef: 0.7550 - iou: 0.6102 - recall_5: 0.8452 - precision_5: 0.8992\n",
            "Epoch 12: val_loss improved from 1.57381 to 1.24842, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.9078 - dice_coef: 0.7550 - iou: 0.6102 - recall_5: 0.8452 - precision_5: 0.8992 - val_loss: 1.2484 - val_dice_coef: 0.5723 - val_iou: 0.4056 - val_recall_5: 0.7534 - val_precision_5: 0.5852 - lr: 1.0000e-04\n",
            "Epoch 13/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8285 - dice_coef: 0.7692 - iou: 0.6286 - recall_5: 0.8468 - precision_5: 0.8995\n",
            "Epoch 13: val_loss improved from 1.24842 to 1.13621, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.8285 - dice_coef: 0.7692 - iou: 0.6286 - recall_5: 0.8468 - precision_5: 0.8995 - val_loss: 1.1362 - val_dice_coef: 0.5700 - val_iou: 0.4030 - val_recall_5: 0.6548 - val_precision_5: 0.6656 - lr: 1.0000e-04\n",
            "Epoch 14/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7582 - dice_coef: 0.7829 - iou: 0.6470 - recall_5: 0.8488 - precision_5: 0.9015\n",
            "Epoch 14: val_loss improved from 1.13621 to 1.09435, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 123ms/step - loss: 0.7582 - dice_coef: 0.7829 - iou: 0.6470 - recall_5: 0.8488 - precision_5: 0.9015 - val_loss: 1.0944 - val_dice_coef: 0.5922 - val_iou: 0.4255 - val_recall_5: 0.7551 - val_precision_5: 0.6154 - lr: 1.0000e-04\n",
            "Epoch 15/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6670 - dice_coef: 0.8128 - iou: 0.6874 - recall_5: 0.8648 - precision_5: 0.9275\n",
            "Epoch 15: val_loss improved from 1.09435 to 1.01404, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 14s 142ms/step - loss: 0.6670 - dice_coef: 0.8128 - iou: 0.6874 - recall_5: 0.8648 - precision_5: 0.9275 - val_loss: 1.0140 - val_dice_coef: 0.5700 - val_iou: 0.4081 - val_recall_5: 0.5308 - val_precision_5: 0.8630 - lr: 1.0000e-04\n",
            "Epoch 16/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6162 - dice_coef: 0.8246 - iou: 0.7044 - recall_5: 0.8695 - precision_5: 0.9271\n",
            "Epoch 16: val_loss improved from 1.01404 to 0.91251, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.6162 - dice_coef: 0.8246 - iou: 0.7044 - recall_5: 0.8695 - precision_5: 0.9271 - val_loss: 0.9125 - val_dice_coef: 0.6430 - val_iou: 0.4781 - val_recall_5: 0.6225 - val_precision_5: 0.8130 - lr: 1.0000e-04\n",
            "Epoch 17/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5686 - dice_coef: 0.8360 - iou: 0.7205 - recall_5: 0.8702 - precision_5: 0.9348\n",
            "Epoch 17: val_loss improved from 0.91251 to 0.85291, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 125ms/step - loss: 0.5686 - dice_coef: 0.8360 - iou: 0.7205 - recall_5: 0.8702 - precision_5: 0.9348 - val_loss: 0.8529 - val_dice_coef: 0.6808 - val_iou: 0.5218 - val_recall_5: 0.6791 - val_precision_5: 0.8053 - lr: 1.0000e-04\n",
            "Epoch 18/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5274 - dice_coef: 0.8472 - iou: 0.7369 - recall_5: 0.8779 - precision_5: 0.9360\n",
            "Epoch 18: val_loss did not improve from 0.85291\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.5274 - dice_coef: 0.8472 - iou: 0.7369 - recall_5: 0.8779 - precision_5: 0.9360 - val_loss: 0.8916 - val_dice_coef: 0.6747 - val_iou: 0.5136 - val_recall_5: 0.8066 - val_precision_5: 0.6562 - lr: 1.0000e-04\n",
            "Epoch 19/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4888 - dice_coef: 0.8600 - iou: 0.7559 - recall_5: 0.8798 - precision_5: 0.9393\n",
            "Epoch 19: val_loss improved from 0.85291 to 0.80342, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.4888 - dice_coef: 0.8600 - iou: 0.7559 - recall_5: 0.8798 - precision_5: 0.9393 - val_loss: 0.8034 - val_dice_coef: 0.7010 - val_iou: 0.5437 - val_recall_5: 0.7112 - val_precision_5: 0.7632 - lr: 1.0000e-04\n",
            "Epoch 20/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4531 - dice_coef: 0.8712 - iou: 0.7738 - recall_5: 0.8852 - precision_5: 0.9436\n",
            "Epoch 20: val_loss did not improve from 0.80342\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.4531 - dice_coef: 0.8712 - iou: 0.7738 - recall_5: 0.8852 - precision_5: 0.9436 - val_loss: 0.8392 - val_dice_coef: 0.6484 - val_iou: 0.4886 - val_recall_5: 0.5731 - val_precision_5: 0.8663 - lr: 1.0000e-04\n",
            "Epoch 21/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4350 - dice_coef: 0.8735 - iou: 0.7775 - recall_5: 0.8825 - precision_5: 0.9436\n",
            "Epoch 21: val_loss improved from 0.80342 to 0.72144, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.4350 - dice_coef: 0.8735 - iou: 0.7775 - recall_5: 0.8825 - precision_5: 0.9436 - val_loss: 0.7214 - val_dice_coef: 0.7202 - val_iou: 0.5665 - val_recall_5: 0.7406 - val_precision_5: 0.7838 - lr: 1.0000e-04\n",
            "Epoch 22/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4035 - dice_coef: 0.8839 - iou: 0.7937 - recall_5: 0.8899 - precision_5: 0.9504\n",
            "Epoch 22: val_loss improved from 0.72144 to 0.70800, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 124ms/step - loss: 0.4035 - dice_coef: 0.8839 - iou: 0.7937 - recall_5: 0.8899 - precision_5: 0.9504 - val_loss: 0.7080 - val_dice_coef: 0.7115 - val_iou: 0.5550 - val_recall_5: 0.7194 - val_precision_5: 0.7769 - lr: 1.0000e-04\n",
            "Epoch 23/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3678 - dice_coef: 0.8967 - iou: 0.8138 - recall_5: 0.8992 - precision_5: 0.9585\n",
            "Epoch 23: val_loss did not improve from 0.70800\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.3678 - dice_coef: 0.8967 - iou: 0.8138 - recall_5: 0.8992 - precision_5: 0.9585 - val_loss: 0.7108 - val_dice_coef: 0.7277 - val_iou: 0.5748 - val_recall_5: 0.7222 - val_precision_5: 0.7698 - lr: 1.0000e-04\n",
            "Epoch 24/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3466 - dice_coef: 0.9032 - iou: 0.8245 - recall_5: 0.9034 - precision_5: 0.9597\n",
            "Epoch 24: val_loss did not improve from 0.70800\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.3466 - dice_coef: 0.9032 - iou: 0.8245 - recall_5: 0.9034 - precision_5: 0.9597 - val_loss: 0.7304 - val_dice_coef: 0.6968 - val_iou: 0.5409 - val_recall_5: 0.6615 - val_precision_5: 0.8034 - lr: 1.0000e-04\n",
            "Epoch 25/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3412 - dice_coef: 0.9010 - iou: 0.8206 - recall_5: 0.9003 - precision_5: 0.9545\n",
            "Epoch 25: val_loss did not improve from 0.70800\n",
            "98/98 [==============================] - 11s 108ms/step - loss: 0.3412 - dice_coef: 0.9010 - iou: 0.8206 - recall_5: 0.9003 - precision_5: 0.9545 - val_loss: 0.7328 - val_dice_coef: 0.6983 - val_iou: 0.5416 - val_recall_5: 0.6269 - val_precision_5: 0.8346 - lr: 1.0000e-04\n",
            "Epoch 26/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3167 - dice_coef: 0.9094 - iou: 0.8350 - recall_5: 0.9073 - precision_5: 0.9602\n",
            "Epoch 26: val_loss did not improve from 0.70800\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.3167 - dice_coef: 0.9094 - iou: 0.8350 - recall_5: 0.9073 - precision_5: 0.9602 - val_loss: 0.7241 - val_dice_coef: 0.7104 - val_iou: 0.5548 - val_recall_5: 0.6548 - val_precision_5: 0.8139 - lr: 1.0000e-04\n",
            "Epoch 27/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3092 - dice_coef: 0.9096 - iou: 0.8352 - recall_5: 0.9027 - precision_5: 0.9603\n",
            "Epoch 27: val_loss did not improve from 0.70800\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.3092 - dice_coef: 0.9096 - iou: 0.8352 - recall_5: 0.9027 - precision_5: 0.9603 - val_loss: 0.7650 - val_dice_coef: 0.6846 - val_iou: 0.5259 - val_recall_5: 0.6084 - val_precision_5: 0.8367 - lr: 1.0000e-04\n",
            "Epoch 28/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2984 - dice_coef: 0.9114 - iou: 0.8381 - recall_5: 0.9068 - precision_5: 0.9586\n",
            "Epoch 28: val_loss improved from 0.70800 to 0.68283, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.2984 - dice_coef: 0.9114 - iou: 0.8381 - recall_5: 0.9068 - precision_5: 0.9586 - val_loss: 0.6828 - val_dice_coef: 0.7162 - val_iou: 0.5629 - val_recall_5: 0.6508 - val_precision_5: 0.8406 - lr: 1.0000e-04\n",
            "Epoch 29/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2796 - dice_coef: 0.9186 - iou: 0.8501 - recall_5: 0.9115 - precision_5: 0.9622\n",
            "Epoch 29: val_loss did not improve from 0.68283\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.2796 - dice_coef: 0.9186 - iou: 0.8501 - recall_5: 0.9115 - precision_5: 0.9622 - val_loss: 0.6921 - val_dice_coef: 0.7246 - val_iou: 0.5720 - val_recall_5: 0.6644 - val_precision_5: 0.8251 - lr: 1.0000e-04\n",
            "Epoch 30/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2566 - dice_coef: 0.9275 - iou: 0.8654 - recall_5: 0.9181 - precision_5: 0.9700\n",
            "Epoch 30: val_loss did not improve from 0.68283\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.2566 - dice_coef: 0.9275 - iou: 0.8654 - recall_5: 0.9181 - precision_5: 0.9700 - val_loss: 0.6844 - val_dice_coef: 0.7277 - val_iou: 0.5774 - val_recall_5: 0.6543 - val_precision_5: 0.8474 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2452 - dice_coef: 0.9304 - iou: 0.8704 - recall_5: 0.9192 - precision_5: 0.9711\n",
            "Epoch 31: val_loss improved from 0.68283 to 0.67932, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 0.2452 - dice_coef: 0.9304 - iou: 0.8704 - recall_5: 0.9192 - precision_5: 0.9711 - val_loss: 0.6793 - val_dice_coef: 0.7342 - val_iou: 0.5865 - val_recall_5: 0.7643 - val_precision_5: 0.7475 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2445 - dice_coef: 0.9273 - iou: 0.8655 - recall_5: 0.9186 - precision_5: 0.9670\n",
            "Epoch 32: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.2445 - dice_coef: 0.9273 - iou: 0.8655 - recall_5: 0.9186 - precision_5: 0.9670 - val_loss: 0.7115 - val_dice_coef: 0.7370 - val_iou: 0.5879 - val_recall_5: 0.6907 - val_precision_5: 0.7941 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2971 - dice_coef: 0.8984 - iou: 0.8183 - recall_5: 0.8850 - precision_5: 0.9474\n",
            "Epoch 33: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.2971 - dice_coef: 0.8984 - iou: 0.8183 - recall_5: 0.8850 - precision_5: 0.9474 - val_loss: 0.7314 - val_dice_coef: 0.7085 - val_iou: 0.5524 - val_recall_5: 0.7542 - val_precision_5: 0.6990 - lr: 1.0000e-04\n",
            "Epoch 34/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2581 - dice_coef: 0.9175 - iou: 0.8490 - recall_5: 0.9063 - precision_5: 0.9600\n",
            "Epoch 34: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.2581 - dice_coef: 0.9175 - iou: 0.8490 - recall_5: 0.9063 - precision_5: 0.9600 - val_loss: 0.6796 - val_dice_coef: 0.7361 - val_iou: 0.5887 - val_recall_5: 0.7268 - val_precision_5: 0.7566 - lr: 1.0000e-04\n",
            "Epoch 35/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2313 - dice_coef: 0.9314 - iou: 0.8721 - recall_5: 0.9168 - precision_5: 0.9687\n",
            "Epoch 35: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.2313 - dice_coef: 0.9314 - iou: 0.8721 - recall_5: 0.9168 - precision_5: 0.9687 - val_loss: 0.6871 - val_dice_coef: 0.7252 - val_iou: 0.5738 - val_recall_5: 0.7737 - val_precision_5: 0.7111 - lr: 1.0000e-04\n",
            "Epoch 36/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2175 - dice_coef: 0.9360 - iou: 0.8801 - recall_5: 0.9219 - precision_5: 0.9711\n",
            "Epoch 36: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.2175 - dice_coef: 0.9360 - iou: 0.8801 - recall_5: 0.9219 - precision_5: 0.9711 - val_loss: 0.7669 - val_dice_coef: 0.7208 - val_iou: 0.5673 - val_recall_5: 0.7930 - val_precision_5: 0.6784 - lr: 1.0000e-04\n",
            "Epoch 37/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2144 - dice_coef: 0.9356 - iou: 0.8796 - recall_5: 0.9202 - precision_5: 0.9693\n",
            "Epoch 37: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.2144 - dice_coef: 0.9356 - iou: 0.8796 - recall_5: 0.9202 - precision_5: 0.9693 - val_loss: 0.7749 - val_dice_coef: 0.7011 - val_iou: 0.5462 - val_recall_5: 0.8081 - val_precision_5: 0.6523 - lr: 1.0000e-04\n",
            "Epoch 38/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2053 - dice_coef: 0.9383 - iou: 0.8843 - recall_5: 0.9225 - precision_5: 0.9723\n",
            "Epoch 38: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.2053 - dice_coef: 0.9383 - iou: 0.8843 - recall_5: 0.9225 - precision_5: 0.9723 - val_loss: 0.8644 - val_dice_coef: 0.6875 - val_iou: 0.5284 - val_recall_5: 0.8289 - val_precision_5: 0.6069 - lr: 1.0000e-04\n",
            "Epoch 39/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2001 - dice_coef: 0.9393 - iou: 0.8859 - recall_5: 0.9241 - precision_5: 0.9720\n",
            "Epoch 39: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.2001 - dice_coef: 0.9393 - iou: 0.8859 - recall_5: 0.9241 - precision_5: 0.9720 - val_loss: 0.7816 - val_dice_coef: 0.7199 - val_iou: 0.5660 - val_recall_5: 0.8093 - val_precision_5: 0.6716 - lr: 1.0000e-04\n",
            "Epoch 40/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1962 - dice_coef: 0.9400 - iou: 0.8873 - recall_5: 0.9231 - precision_5: 0.9729\n",
            "Epoch 40: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1962 - dice_coef: 0.9400 - iou: 0.8873 - recall_5: 0.9231 - precision_5: 0.9729 - val_loss: 0.9027 - val_dice_coef: 0.6803 - val_iou: 0.5214 - val_recall_5: 0.8242 - val_precision_5: 0.5961 - lr: 1.0000e-04\n",
            "Epoch 41/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1899 - dice_coef: 0.9418 - iou: 0.8904 - recall_5: 0.9258 - precision_5: 0.9727\n",
            "Epoch 41: val_loss did not improve from 0.67932\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1899 - dice_coef: 0.9418 - iou: 0.8904 - recall_5: 0.9258 - precision_5: 0.9727 - val_loss: 0.6820 - val_dice_coef: 0.7202 - val_iou: 0.5694 - val_recall_5: 0.6427 - val_precision_5: 0.8424 - lr: 1.0000e-04\n",
            "Epoch 42/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1878 - dice_coef: 0.9419 - iou: 0.8908 - recall_5: 0.9337 - precision_5: 0.9656\n",
            "Epoch 42: val_loss did not improve from 0.67932\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1878 - dice_coef: 0.9419 - iou: 0.8908 - recall_5: 0.9337 - precision_5: 0.9656 - val_loss: 0.7227 - val_dice_coef: 0.7036 - val_iou: 0.5487 - val_recall_5: 0.5732 - val_precision_5: 0.9090 - lr: 1.0000e-05\n",
            "Epoch 43/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1763 - dice_coef: 0.9476 - iou: 0.9008 - recall_5: 0.9313 - precision_5: 0.9763\n",
            "Epoch 43: val_loss improved from 0.67932 to 0.66737, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 120ms/step - loss: 0.1763 - dice_coef: 0.9476 - iou: 0.9008 - recall_5: 0.9313 - precision_5: 0.9763 - val_loss: 0.6674 - val_dice_coef: 0.7344 - val_iou: 0.5841 - val_recall_5: 0.6296 - val_precision_5: 0.8787 - lr: 1.0000e-05\n",
            "Epoch 44/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1698 - dice_coef: 0.9509 - iou: 0.9066 - recall_5: 0.9350 - precision_5: 0.9785\n",
            "Epoch 44: val_loss improved from 0.66737 to 0.65880, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1698 - dice_coef: 0.9509 - iou: 0.9066 - recall_5: 0.9350 - precision_5: 0.9785 - val_loss: 0.6588 - val_dice_coef: 0.7398 - val_iou: 0.5910 - val_recall_5: 0.6442 - val_precision_5: 0.8659 - lr: 1.0000e-05\n",
            "Epoch 45/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1660 - dice_coef: 0.9526 - iou: 0.9096 - recall_5: 0.9364 - precision_5: 0.9801\n",
            "Epoch 45: val_loss improved from 0.65880 to 0.65351, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.1660 - dice_coef: 0.9526 - iou: 0.9096 - recall_5: 0.9364 - precision_5: 0.9801 - val_loss: 0.6535 - val_dice_coef: 0.7432 - val_iou: 0.5952 - val_recall_5: 0.6576 - val_precision_5: 0.8524 - lr: 1.0000e-05\n",
            "Epoch 46/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1635 - dice_coef: 0.9536 - iou: 0.9115 - recall_5: 0.9381 - precision_5: 0.9802\n",
            "Epoch 46: val_loss improved from 0.65351 to 0.65264, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.1635 - dice_coef: 0.9536 - iou: 0.9115 - recall_5: 0.9381 - precision_5: 0.9802 - val_loss: 0.6526 - val_dice_coef: 0.7456 - val_iou: 0.5983 - val_recall_5: 0.6656 - val_precision_5: 0.8455 - lr: 1.0000e-05\n",
            "Epoch 47/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1611 - dice_coef: 0.9546 - iou: 0.9133 - recall_5: 0.9383 - precision_5: 0.9813\n",
            "Epoch 47: val_loss did not improve from 0.65264\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1611 - dice_coef: 0.9546 - iou: 0.9133 - recall_5: 0.9383 - precision_5: 0.9813 - val_loss: 0.6536 - val_dice_coef: 0.7451 - val_iou: 0.5976 - val_recall_5: 0.6630 - val_precision_5: 0.8478 - lr: 1.0000e-05\n",
            "Epoch 48/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1588 - dice_coef: 0.9555 - iou: 0.9149 - recall_5: 0.9395 - precision_5: 0.9816\n",
            "Epoch 48: val_loss improved from 0.65264 to 0.65120, saving model to files/NanoNetA_SparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1588 - dice_coef: 0.9555 - iou: 0.9149 - recall_5: 0.9395 - precision_5: 0.9816 - val_loss: 0.6512 - val_dice_coef: 0.7474 - val_iou: 0.6005 - val_recall_5: 0.6753 - val_precision_5: 0.8355 - lr: 1.0000e-05\n",
            "Epoch 49/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1569 - dice_coef: 0.9561 - iou: 0.9161 - recall_5: 0.9400 - precision_5: 0.9822\n",
            "Epoch 49: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1569 - dice_coef: 0.9561 - iou: 0.9161 - recall_5: 0.9400 - precision_5: 0.9822 - val_loss: 0.6546 - val_dice_coef: 0.7461 - val_iou: 0.5991 - val_recall_5: 0.6691 - val_precision_5: 0.8410 - lr: 1.0000e-05\n",
            "Epoch 50/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1546 - dice_coef: 0.9571 - iou: 0.9179 - recall_5: 0.9407 - precision_5: 0.9828\n",
            "Epoch 50: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1546 - dice_coef: 0.9571 - iou: 0.9179 - recall_5: 0.9407 - precision_5: 0.9828 - val_loss: 0.6532 - val_dice_coef: 0.7471 - val_iou: 0.6002 - val_recall_5: 0.6796 - val_precision_5: 0.8293 - lr: 1.0000e-05\n",
            "Epoch 51/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1532 - dice_coef: 0.9574 - iou: 0.9185 - recall_5: 0.9409 - precision_5: 0.9831\n",
            "Epoch 51: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1532 - dice_coef: 0.9574 - iou: 0.9185 - recall_5: 0.9409 - precision_5: 0.9831 - val_loss: 0.6543 - val_dice_coef: 0.7475 - val_iou: 0.6009 - val_recall_5: 0.6722 - val_precision_5: 0.8385 - lr: 1.0000e-05\n",
            "Epoch 52/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1514 - dice_coef: 0.9580 - iou: 0.9195 - recall_5: 0.9417 - precision_5: 0.9832\n",
            "Epoch 52: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1514 - dice_coef: 0.9580 - iou: 0.9195 - recall_5: 0.9417 - precision_5: 0.9832 - val_loss: 0.6543 - val_dice_coef: 0.7486 - val_iou: 0.6022 - val_recall_5: 0.6776 - val_precision_5: 0.8333 - lr: 1.0000e-05\n",
            "Epoch 53/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1497 - dice_coef: 0.9585 - iou: 0.9205 - recall_5: 0.9420 - precision_5: 0.9835\n",
            "Epoch 53: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1497 - dice_coef: 0.9585 - iou: 0.9205 - recall_5: 0.9420 - precision_5: 0.9835 - val_loss: 0.6555 - val_dice_coef: 0.7482 - val_iou: 0.6017 - val_recall_5: 0.6838 - val_precision_5: 0.8236 - lr: 1.0000e-05\n",
            "Epoch 54/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1484 - dice_coef: 0.9588 - iou: 0.9211 - recall_5: 0.9422 - precision_5: 0.9838\n",
            "Epoch 54: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1484 - dice_coef: 0.9588 - iou: 0.9211 - recall_5: 0.9422 - precision_5: 0.9838 - val_loss: 0.6576 - val_dice_coef: 0.7478 - val_iou: 0.6012 - val_recall_5: 0.6701 - val_precision_5: 0.8403 - lr: 1.0000e-05\n",
            "Epoch 55/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1466 - dice_coef: 0.9594 - iou: 0.9221 - recall_5: 0.9428 - precision_5: 0.9840\n",
            "Epoch 55: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.1466 - dice_coef: 0.9594 - iou: 0.9221 - recall_5: 0.9428 - precision_5: 0.9840 - val_loss: 0.6568 - val_dice_coef: 0.7491 - val_iou: 0.6028 - val_recall_5: 0.6911 - val_precision_5: 0.8160 - lr: 1.0000e-05\n",
            "Epoch 56/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1447 - dice_coef: 0.9600 - iou: 0.9233 - recall_5: 0.9432 - precision_5: 0.9845\n",
            "Epoch 56: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 0.1447 - dice_coef: 0.9600 - iou: 0.9233 - recall_5: 0.9432 - precision_5: 0.9845 - val_loss: 0.6587 - val_dice_coef: 0.7492 - val_iou: 0.6030 - val_recall_5: 0.6870 - val_precision_5: 0.8201 - lr: 1.0000e-05\n",
            "Epoch 57/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1433 - dice_coef: 0.9603 - iou: 0.9238 - recall_5: 0.9436 - precision_5: 0.9845\n",
            "Epoch 57: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1433 - dice_coef: 0.9603 - iou: 0.9238 - recall_5: 0.9436 - precision_5: 0.9845 - val_loss: 0.6572 - val_dice_coef: 0.7491 - val_iou: 0.6030 - val_recall_5: 0.6777 - val_precision_5: 0.8316 - lr: 1.0000e-05\n",
            "Epoch 58/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1418 - dice_coef: 0.9608 - iou: 0.9246 - recall_5: 0.9437 - precision_5: 0.9850\n",
            "Epoch 58: val_loss did not improve from 0.65120\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1418 - dice_coef: 0.9608 - iou: 0.9246 - recall_5: 0.9437 - precision_5: 0.9850 - val_loss: 0.6591 - val_dice_coef: 0.7487 - val_iou: 0.6025 - val_recall_5: 0.6775 - val_precision_5: 0.8311 - lr: 1.0000e-05\n",
            "Epoch 59/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1402 - dice_coef: 0.9613 - iou: 0.9257 - recall_5: 0.9427 - precision_5: 0.9861\n",
            "Epoch 59: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.1402 - dice_coef: 0.9613 - iou: 0.9257 - recall_5: 0.9427 - precision_5: 0.9861 - val_loss: 0.6569 - val_dice_coef: 0.7501 - val_iou: 0.6042 - val_recall_5: 0.6884 - val_precision_5: 0.8206 - lr: 1.0000e-06\n",
            "Epoch 60/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1397 - dice_coef: 0.9615 - iou: 0.9260 - recall_5: 0.9445 - precision_5: 0.9852\n",
            "Epoch 60: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1397 - dice_coef: 0.9615 - iou: 0.9260 - recall_5: 0.9445 - precision_5: 0.9852 - val_loss: 0.6570 - val_dice_coef: 0.7501 - val_iou: 0.6042 - val_recall_5: 0.6910 - val_precision_5: 0.8174 - lr: 1.0000e-06\n",
            "Epoch 61/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1395 - dice_coef: 0.9616 - iou: 0.9262 - recall_5: 0.9445 - precision_5: 0.9854\n",
            "Epoch 61: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1395 - dice_coef: 0.9616 - iou: 0.9262 - recall_5: 0.9445 - precision_5: 0.9854 - val_loss: 0.6575 - val_dice_coef: 0.7501 - val_iou: 0.6042 - val_recall_5: 0.6918 - val_precision_5: 0.8161 - lr: 1.0000e-06\n",
            "Epoch 62/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1394 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9450 - precision_5: 0.9852\n",
            "Epoch 62: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1394 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9450 - precision_5: 0.9852 - val_loss: 0.6580 - val_dice_coef: 0.7500 - val_iou: 0.6041 - val_recall_5: 0.6922 - val_precision_5: 0.8155 - lr: 1.0000e-06\n",
            "Epoch 63/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1392 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9450 - precision_5: 0.9852\n",
            "Epoch 63: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1392 - dice_coef: 0.9617 - iou: 0.9263 - recall_5: 0.9450 - precision_5: 0.9852 - val_loss: 0.6594 - val_dice_coef: 0.7498 - val_iou: 0.6039 - val_recall_5: 0.6958 - val_precision_5: 0.8106 - lr: 1.0000e-06\n",
            "Epoch 64/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1388 - dice_coef: 0.9618 - iou: 0.9266 - recall_5: 0.9448 - precision_5: 0.9856\n",
            "Epoch 64: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1388 - dice_coef: 0.9618 - iou: 0.9266 - recall_5: 0.9448 - precision_5: 0.9856 - val_loss: 0.6593 - val_dice_coef: 0.7497 - val_iou: 0.6038 - val_recall_5: 0.6931 - val_precision_5: 0.8136 - lr: 1.0000e-06\n",
            "Epoch 65/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1387 - dice_coef: 0.9619 - iou: 0.9267 - recall_5: 0.9453 - precision_5: 0.9852\n",
            "Epoch 65: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1387 - dice_coef: 0.9619 - iou: 0.9267 - recall_5: 0.9453 - precision_5: 0.9852 - val_loss: 0.6608 - val_dice_coef: 0.7495 - val_iou: 0.6036 - val_recall_5: 0.6950 - val_precision_5: 0.8105 - lr: 1.0000e-06\n",
            "Epoch 66/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1384 - dice_coef: 0.9619 - iou: 0.9268 - recall_5: 0.9449 - precision_5: 0.9856\n",
            "Epoch 66: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1384 - dice_coef: 0.9619 - iou: 0.9268 - recall_5: 0.9449 - precision_5: 0.9856 - val_loss: 0.6604 - val_dice_coef: 0.7496 - val_iou: 0.6037 - val_recall_5: 0.6931 - val_precision_5: 0.8128 - lr: 1.0000e-06\n",
            "Epoch 67/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1382 - dice_coef: 0.9620 - iou: 0.9269 - recall_5: 0.9455 - precision_5: 0.9852\n",
            "Epoch 67: val_loss did not improve from 0.65120\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1382 - dice_coef: 0.9620 - iou: 0.9269 - recall_5: 0.9455 - precision_5: 0.9852 - val_loss: 0.6612 - val_dice_coef: 0.7496 - val_iou: 0.6036 - val_recall_5: 0.6954 - val_precision_5: 0.8100 - lr: 1.0000e-06\n",
            "Epoch 68/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1381 - dice_coef: 0.9620 - iou: 0.9270 - recall_5: 0.9449 - precision_5: 0.9856\n",
            "Epoch 68: val_loss did not improve from 0.65120\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1381 - dice_coef: 0.9620 - iou: 0.9270 - recall_5: 0.9449 - precision_5: 0.9856 - val_loss: 0.6600 - val_dice_coef: 0.7497 - val_iou: 0.6038 - val_recall_5: 0.6908 - val_precision_5: 0.8160 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from keras.metrics import Recall, Precision, MeanIoU\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    \"\"\" Remove folders and files \"\"\"\n",
        "    # os.system(\"rm files/files.csv\")\n",
        "    # os.system(\"rm -r logs\")\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    input_shape = (256, 256, 3)\n",
        "    batch_size = 8\n",
        "    lr = 1e-4\n",
        "    epochs = 200\n",
        "    model_name = \"NanoNetA_SparseAttention\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "    csv_path = f\"files/{model_name}/model.csv\"\n",
        "    log_path = f\"logs/{model_name}/\"\n",
        "    \"\"\" Creating folders \"\"\"\n",
        "    create_dir(f\"files/{model_name}\")\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    path = '/content/drive/MyDrive/capstone/Kvasir-SEG'\n",
        "\n",
        "    (train_x, train_y), (valid_x, valid_y) = load_data(path)\n",
        "\n",
        "    # processed_train_x, processed_train_y = process_dataset(\n",
        "    #     train_x, train_y,\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/train/images\",\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/train/masks\"\n",
        "    # )\n",
        "    # processed_valid_x, processed_valid_y = process_dataset(\n",
        "    #     valid_x, valid_y,\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/valid/images\",\n",
        "    #     \"/Users/xuzhenke/Documents/USYD/CapStone/Capstone-Project/Kvasir-SEG/processed/valid/masks\"\n",
        "    # )\n",
        "\n",
        "    # train_dataset = tf_dataset(processed_train_x, processed_train_y, batch)\n",
        "    # valid_dataset = tf_dataset(processed_valid_x, processed_valid_y, batch)\n",
        "    #\n",
        "    # extractor = feature_extractor(input_shape)\n",
        "    # train_dataset = feature_extracted_tf_dataset(train_x, train_y, extractor, batch)\n",
        "    # valid_dataset = feature_extracted_tf_dataset(valid_x, valid_y, extractor, batch)\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
        "\n",
        "    # \"\"\" Model \"\"\"\n",
        "    model = NanoNet_A_Sparse_Attention(input_shape)\n",
        "\n",
        "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "    model.compile(loss=bce_dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
        "    model.summary()\n",
        "\n",
        "    #\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(log_dir=log_path),\n",
        "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
        "    ]\n",
        "\n",
        "    train_steps = (len(train_x)//batch_size)\n",
        "    valid_steps = (len(valid_x)//batch_size)\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_steps += 1\n",
        "\n",
        "    model.fit(train_dataset,\n",
        "            epochs=epochs,\n",
        "            validation_data=valid_dataset,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_steps=valid_steps,\n",
        "            callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0qoXyNT9zJX"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJg8vXU79w8y",
        "outputId": "6c86e044-a6c7-46c9-df30-e3bfc23069fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju87vqa0ndwg0850onjdz7ol: 0.07314\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju87xn2snfmv0987sc3d9xnq: 0.08511\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju87z6o6nh73085045bzsx6o: 0.07433\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju87zv8lni0o0850hbbecbq6: 0.11769\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju8828oxnool0801qno9luhr: 0.07749\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju884985nlmx0817vzpax3y4: 0.07247\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dp3dw2k4n0755zhe003ad: 0.07368\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dqcwi2dz00850gcmr2ert: 0.07080\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju7druhp2gp308715i6km7be: 0.07338\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dsrtb2f8i085064kwugfk: 0.07005\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7dtb1e2j0t0818deq51ib3: 0.07136\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7dubap2g0w0801fgl42mg9: 0.08271\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dvl5m2n4t0755hlnnjjet: 0.07021\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dwe282dc309876rco45ts: 0.07128\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju7dxffn2eam0817qxosfwch: 0.07124\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju7dymur2od30755eg8yv2ht: 0.07153\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju7dz5yy2i7z0801ausi7rna: 0.07612\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju7ea4om2l910801bohqjccy: 0.07269\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju7ebe962hr409872ovibahw: 0.07295\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7ecl9i2i060987xawjp4l0: 0.08631\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju7eea9b2m0z0801ynqv1fqu: 0.06843\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88oh0po9gq0801nge4tgr1: 0.07300\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju88q6h6obpd0871ckmiabbo: 0.07580\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88rl5eo94l0850kf5wtrm1: 0.07292\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju88t4fvokxf07558ymyh281: 0.06878\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88trl3ogi208716qvti51b: 0.07120\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju88v2f9oi8w0871hx9auh01: 0.07589\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88vx2uoocy075531lc63n3: 0.07552\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju88y1mwoln50871emyfny1g: 0.07331\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju88z8bson4h0871nnd7fdxo: 0.07430\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju890guyoiti098753yg6cdu: 0.10657\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "cju8914beokbf0850isxpocrk: 0.10134\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju892fesoq2g0801n0e0jyia: 0.10682\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju893jmdompz0817xn3g1w4h: 0.10630\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju89y9h0puti0818i5yw29e6: 0.10619\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju89z6pqpqfx0817mfv8ixjc: 0.11168\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "cju8a1jtvpt9m081712iwkca7: 0.12131\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "cju8a2itsq4dv0755ntlovpxe: 0.15031\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "cju8a3nhbpwnb0850d37fo2na: 0.12155\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "cju8a56vxpy780850r45yu4wk: 0.13628\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "cju8a84g0q76m0818hwiggkod: 0.10650\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju8abobpqbir08189u01huru: 0.09396\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "cju8adb60qbiu080188mxpf8d: 0.09820\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju8aeei7q8k308173n9y4klv: 0.10663\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju8aj01yqeqm0850lhdz3xdw: 0.10721\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8alhigqn2h0801zksudldd: 0.07430\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8amfdtqi4x09871tygrgqe: 0.07590\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8ando2qqdo0818ck7i1be1: 0.07307\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju8apjewqrk00801k5d71gky: 0.07860\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8aqq8uqmoq0987hphto9gg: 0.07087\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8arof2qpf20850ifr1bnqj: 0.07444\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8ashhnquqr0801rwduzt7d: 0.07282\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "cju8at3s1qqqx0850hcq8nmnq: 0.12988\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8auylgqx0z0871u4o4db7o: 0.11451\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju8aw9n1qyg10801jkjlmors: 0.08045\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8axq24r4an0755yhv9d4ly: 0.07368\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8ayeq7r1fb0818z1junacy: 0.07518\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8azmhcr66e0755t61atz72: 0.06970\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b0jr0r2oi0801jiquetd5: 0.07080\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8b1v3br45u087189kku66u: 0.07875\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8b2rmgr52s0801p54eyflx: 0.07779\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8b3ka8r64u0801fh18hk7l: 0.07807\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b4ja9r2s808509d45ma86: 0.07095\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8b542nr81x0871uxnkm9ih: 0.07255\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b5p40r2c60987ofa0mu03: 0.06918\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b6rp0r5st0850184f79xt: 0.07234\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b7aqtr4a00987coba14b7: 0.07526\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b8yair65w09878pyqtr96: 0.07248\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bafgqrf4x0818twisk3ea: 0.08043\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bbznkrf5g0871jncffynk: 0.07225\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8bff9nrfi10850fmfzbf8v: 0.07084\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bgdmqrksy0801tozdmraa: 0.08238\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bh8surexp0987o5pzklk1: 0.07419\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8bi8q7rlmn0871abc5ch8k: 0.07212\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8bj2ssrmlm0871gc2ug2rs: 0.07454\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bk8oirjhw0817hgkua2w8: 0.07494\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8bljw9rqk20801kr54akrl: 0.12759\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bm24yrrdp081829mbo8ic: 0.07577\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8bn7m2rmm70817hgxpb1uq: 0.07738\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8bop5jrsid08716i24fqda: 0.07781\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bpctzrqkr0850zeldv9kt: 0.07456\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "cju8bqxxurs6i0850mu7mtef9: 0.08704\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8brv16rx7f0818uf5n89pv: 0.07673\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bssulrrcy0987h1vq5060: 0.07218\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8buos5rz9b08715lfr0f4f: 0.08445\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bw697rwg308177tg8huas: 0.07438\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bysfgrzkl081786jwac09: 0.07244\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bzzy2s66m08016z6mouqt: 0.07657\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8c1a0ws7o208181c6lbsom: 0.07285\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8c2rqzs5t80850d0zky5dy: 0.07500\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8c3xs7sauj0801ieyzezr5: 0.07515\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8c5223s8j80850b4kealt4: 0.07576\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8c5mxls96t0850wvkvsity: 0.07401\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8c5zcbsdfz0801o5t6jag1: 0.09319\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8c6hnxsdvr0801wn0vrsa6: 0.07413\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju8c82iosagu0817l74s4m5g: 0.08016\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8c9akjsdjj0850s67uzlxq: 0.07329\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8ca4geseia0850i2ru11hw: 0.07555\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8cattbsivm0818p446wgel: 0.07047\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8cbsyssiqj0871gr4jedjp: 0.07303\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju7efffp2ivf0817etg3jehl: 0.07899\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju6x35ervu2808015c7eoqe4: 0.07951\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju6x4t13vyw60755gtcf9ndu: 0.10312\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "cju6x97w4vwua0850x0997r0a: 0.10050\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju6xa0qmvzun0818xjukgncj: 0.10704\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6xifswvwbo0987nibtdr50: 0.10437\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju6xlygpw7bs0818n691jsq4: 0.10421\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "cju6xmqd9w0250817l5kxfnsk: 0.10916\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "cju6ywm40wdbo0987pbftsvtg: 0.11159\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju6yxyt0wh080871sqpepu47: 0.10830\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "cju6yywx1whbb0871ksgfgf9f: 0.10192\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju6z1bzbwfq50817b2alatvr: 0.10396\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6z2616wqbk07555bvnuyr1: 0.14506\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju6z600qwh4z081700qimgl9: 0.10338\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju6z7e4bwgdd0987ogkzq9kt: 0.10720\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "cju6z9a9kwsl007552s49rx6i: 0.13249\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "cju76erapykj30871x5eaxh4q: 0.11661\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju76l27oyrw907551ri2a7fl: 0.08283\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju76lsehyia10987u54vn8rb: 0.07228\n",
            "\n",
            "Jaccard: 0.6751 - F1: 0.7744 - Recall: 0.7694 - Precision: 0.8542 - Acc: 0.9468 - F2: 0.7652\n",
            "Mean FPS:  11.655447312269583\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import time\n",
        "from operator import add\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score, f1_score, recall_score, precision_score, accuracy_score, fbeta_score)\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.reshape(-1)\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "\n",
        "    ## Score\n",
        "    score_jaccard = jaccard_score(y_true, y_pred, average='binary')\n",
        "    score_f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    score_recall = recall_score(y_true, y_pred, average='binary')\n",
        "    score_precision = precision_score(y_true, y_pred, average='binary', zero_division=1)\n",
        "    score_acc = accuracy_score(y_true, y_pred)\n",
        "    score_fbeta = fbeta_score(y_true, y_pred, beta=2.0, average='binary', zero_division=1)\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_fbeta]\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    path = \"/content/drive/MyDrive/capstone/Kvasir-SEG\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_test_data(path)\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    size = (256, 256)\n",
        "    input_shape = (256, 256, 3)\n",
        "    model_name = \"NanoNetA_SparseAttention\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    create_dir(f\"results/{model_name}\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    model = load_model_file(model_path)\n",
        "\n",
        "    \"\"\" Sample prediction: To improve FPS \"\"\"\n",
        "    image = np.zeros((1, 256, 256, 3))\n",
        "    mask = model.predict(image)\n",
        "\n",
        "    \"\"\" Testing \"\"\"\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    time_taken = []\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
        "        name = y.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Image \"\"\"\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        image = cv2.resize(image, size)\n",
        "        ori_img = image\n",
        "        image = image/255.0\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image.astype(np.float32)\n",
        "\n",
        "        \"\"\" Mask \"\"\"\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, size)\n",
        "        ori_mask = mask\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        mask = mask/255.0\n",
        "        mask = mask.astype(np.float32)\n",
        "\n",
        "        \"\"\" Time taken \"\"\"\n",
        "        start_time = time.time()\n",
        "        pred_y = model.predict(image)\n",
        "        total_time = time.time() - start_time\n",
        "        time_taken.append(total_time)\n",
        "        print(f\"{name}: {total_time:1.5f}\")\n",
        "\n",
        "        \"\"\" Metrics calculation \"\"\"\n",
        "        score = calculate_metrics(mask, pred_y)\n",
        "        metrics_score = list(map(add, metrics_score, score))\n",
        "\n",
        "        \"\"\" Saving masks \"\"\"\n",
        "        pred_y = pred_y[0] > 0.5\n",
        "        pred_y = pred_y * 255\n",
        "        pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "        ori_img = ori_img\n",
        "        ori_mask = mask_parse(ori_mask)\n",
        "        pred_y = mask_parse(pred_y)\n",
        "        sep_line = np.ones((size[0], 10, 3)) * 255\n",
        "\n",
        "        tmp = [\n",
        "            ori_img, sep_line,\n",
        "            ori_mask, sep_line,\n",
        "            pred_y\n",
        "        ]\n",
        "\n",
        "        cat_images = np.concatenate(tmp, axis=1)\n",
        "        cv2.imwrite(f\"results/{model_name}/{name}.png\", cat_images)\n",
        "\n",
        "    jaccard = metrics_score[0]/len(test_x)\n",
        "    f1 = metrics_score[1]/len(test_x)\n",
        "    recall = metrics_score[2]/len(test_x)\n",
        "    precision = metrics_score[3]/len(test_x)\n",
        "    acc = metrics_score[4]/len(test_x)\n",
        "    f2 = metrics_score[5]/len(test_x)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - F2: {f2:1.4f}\")\n",
        "\n",
        "    mean_time_taken = np.mean(time_taken)\n",
        "    mean_fps = 1/mean_time_taken\n",
        "    print(\"Mean FPS: \", mean_fps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
