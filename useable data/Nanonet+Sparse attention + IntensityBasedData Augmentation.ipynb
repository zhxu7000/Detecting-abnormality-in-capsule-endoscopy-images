{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK2gE4IYpFqu",
        "outputId": "238d9f09-1b68-49db-920a-caca93754f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKetU_kznosl"
      },
      "source": [
        "#metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "OOyT5H-zmq5-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + 1e-15) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1e-15)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return dice_loss(y_true, y_pred) + tf.keras.losses.binary_crossentropy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKC0lspJn3Pd"
      },
      "source": [
        "#sparse attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ArI0wua-n2ch"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "class SparseAttention(layers.Layer):\n",
        "\n",
        "    def __init__(self, key_dim, num_heads=1, regularization_coeff=0.01, dropout_rate=0.1, name=None):\n",
        "        super(SparseAttention, self).__init__(name=name)\n",
        "        self.key_dim = key_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.regularization_coeff = regularization_coeff\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.q_dense = layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                    kernel_regularizer=regularizers.l2(self.regularization_coeff))\n",
        "        self.k_dense = layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                    kernel_regularizer=regularizers.l2(self.regularization_coeff))\n",
        "        self.v_dense = layers.Dense(self.key_dim * self.num_heads, use_bias=False,\n",
        "                                    kernel_regularizer=regularizers.l2(self.regularization_coeff))\n",
        "        self.output_dense = layers.Dense(input_shape[-1],\n",
        "                                         kernel_regularizer=regularizers.l2(self.regularization_coeff))\n",
        "\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout3 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout4 = layers.Dropout(0.2)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        q = self.dropout1(self.q_dense(x), training=training)\n",
        "        k = self.dropout2(self.k_dense(x), training=training)\n",
        "        v = self.dropout3(self.v_dense(x), training=training)\n",
        "\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        attn_score = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_score = attn_score / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
        "\n",
        "        attn_score = self.sparsemax(attn_score)\n",
        "        attn_score = self.dropout4(attn_score, training=training)\n",
        "\n",
        "        attn_values = tf.matmul(attn_score, v)\n",
        "        attn_values = self.combine_heads(attn_values)\n",
        "\n",
        "        attn_values = self.layer_norm1(attn_values)\n",
        "        output = self.output_dense(attn_values)\n",
        "        output = self.layer_norm2(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def sparsemax(self, logits, axis=-1):\n",
        "        logits = tf.convert_to_tensor(logits)\n",
        "        num_features = tf.shape(logits)[axis]\n",
        "        z = tf.sort(logits, axis=axis, direction='DESCENDING')\n",
        "        z_cumsum = tf.cumsum(z, axis=axis)\n",
        "        k = tf.range(1, num_features + 1, dtype=tf.float32)\n",
        "        z_check = 1 + k * z >= z_cumsum\n",
        "        k_max = tf.reduce_sum(tf.cast(z_check, tf.float32), axis=axis, keepdims=True)\n",
        "        z_max = tf.gather(z, tf.cast(k_max - 1, tf.int32), batch_dims=len(logits.shape) - 1)\n",
        "        out = tf.nn.relu(logits - z_max)\n",
        "        return out\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.key_dim))\n",
        "        return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.transpose(x, [0, 2, 1, 3])\n",
        "        return tf.reshape(x, (batch_size, -1, self.key_dim * self.num_heads))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9XDaRgoCQs"
      },
      "source": [
        "#Se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "csdWOzRcoB5d"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, Add, Permute, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def squeeze_excite_block(input, ratio=16):\n",
        "    ''' Create a channel-wise squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def spatial_squeeze_excite_block(input):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    se = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False,\n",
        "                kernel_initializer='he_normal')(input)\n",
        "\n",
        "    x = Multiply([input, se])\n",
        "    return x\n",
        "\n",
        "\n",
        "def channel_spatial_squeeze_excite(input, ratio=16):\n",
        "    ''' Create a spatial squeeze-excite block\n",
        "\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "\n",
        "    Returns: a keras tensor\n",
        "\n",
        "    References\n",
        "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
        "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
        "    '''\n",
        "\n",
        "    cse = squeeze_excite_block(input, ratio)\n",
        "    sse = spatial_squeeze_excite_block(input)\n",
        "\n",
        "    x = Add([cse, sse])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ZIULAUm6TD"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyi9W__ngsl",
        "outputId": "31728274-e932-47c7-dc86-c7a59e628162"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 32)         864       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_380 (Conv2D)         (None, 32, 32, 49)           9457      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_361 (B  (None, 32, 32, 49)           196       ['conv2d_380[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_304 (Activation  (None, 32, 32, 49)           0         ['batch_normalization_361[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_381 (Conv2D)         (None, 32, 32, 49)           21658     ['activation_304[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_362 (B  (None, 32, 32, 49)           196       ['conv2d_381[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_305 (Activation  (None, 32, 32, 49)           0         ['batch_normalization_362[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_382 (Conv2D)         (None, 32, 32, 196)          86632     ['activation_305[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_363 (B  (None, 32, 32, 196)          784       ['conv2d_382[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_364 (B  (None, 32, 32, 196)          784       ['batch_normalization_363[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_76 (Add)                (None, 32, 32, 196)          0         ['batch_normalization_363[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_364[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_306 (Activation  (None, 32, 32, 196)          0         ['add_76[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 196)                  0         ['activation_306[0][0]']      \n",
            " 6 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_106 (Reshape)       (None, 1, 1, 196)            0         ['global_average_pooling2d_76[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_152 (Dense)           (None, 1, 1, 12)             2352      ['reshape_106[0][0]']         \n",
            "                                                                                                  \n",
            " dense_153 (Dense)           (None, 1, 1, 196)            2352      ['dense_152[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_76 (Multiply)      (None, 32, 32, 196)          0         ['activation_306[0][0]',      \n",
            "                                                                     'dense_153[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_107 (Reshape)       (None, 1024, 196)            0         ['multiply_76[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_21 (Spars  (None, None, 196)            308700    ['reshape_107[0][0]']         \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_384 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_108 (Reshape)       (None, 32, 32, 196)          0         ['sparse_attention_21[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_365 (B  (None, 64, 64, 128)          512       ['conv2d_384[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_57 (UpSampli  (None, 64, 64, 196)          0         ['reshape_108[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_307 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_365[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_57 (Concatenat  (None, 64, 64, 324)          0         ['up_sampling2d_57[0][0]',    \n",
            " e)                                                                  'activation_307[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_385 (Conv2D)         (None, 64, 64, 32)           10400     ['concatenate_57[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_366 (B  (None, 64, 64, 32)           128       ['conv2d_385[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_308 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_366[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_386 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_308[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_367 (B  (None, 64, 64, 32)           128       ['conv2d_386[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_309 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_367[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_387 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_309[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_368 (B  (None, 64, 64, 128)          512       ['conv2d_387[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_369 (B  (None, 64, 64, 128)          512       ['batch_normalization_368[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_77 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_368[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_369[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_310 (Activation  (None, 64, 64, 128)          0         ['add_77[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 128)                  0         ['activation_310[0][0]']      \n",
            " 7 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_109 (Reshape)       (None, 1, 1, 128)            0         ['global_average_pooling2d_77[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_154 (Dense)           (None, 1, 1, 8)              1024      ['reshape_109[0][0]']         \n",
            "                                                                                                  \n",
            " dense_155 (Dense)           (None, 1, 1, 128)            1024      ['dense_154[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_389 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_77 (Multiply)      (None, 64, 64, 128)          0         ['activation_310[0][0]',      \n",
            "                                                                     'dense_155[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_370 (B  (None, 128, 128, 64)         256       ['conv2d_389[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_58 (UpSampli  (None, 128, 128, 128)        0         ['multiply_77[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_311 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_370[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_58 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_58[0][0]',    \n",
            " e)                                                                  'activation_311[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_390 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_58[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_371 (B  (None, 128, 128, 16)         64        ['conv2d_390[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_312 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_371[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_391 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_312[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_372 (B  (None, 128, 128, 16)         64        ['conv2d_391[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_313 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_372[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_392 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_313[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_373 (B  (None, 128, 128, 64)         256       ['conv2d_392[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_374 (B  (None, 128, 128, 64)         256       ['batch_normalization_373[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_78 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_373[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_374[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_314 (Activation  (None, 128, 128, 64)         0         ['add_78[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 64)                   0         ['activation_314[0][0]']      \n",
            " 8 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_110 (Reshape)       (None, 1, 1, 64)             0         ['global_average_pooling2d_78[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_156 (Dense)           (None, 1, 1, 4)              256       ['reshape_110[0][0]']         \n",
            "                                                                                                  \n",
            " dense_157 (Dense)           (None, 1, 1, 64)             256       ['dense_156[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_394 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_78 (Multiply)      (None, 128, 128, 64)         0         ['activation_314[0][0]',      \n",
            "                                                                     'dense_157[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_375 (B  (None, 256, 256, 32)         128       ['conv2d_394[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_59 (UpSampli  (None, 256, 256, 64)         0         ['multiply_78[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_315 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_375[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_59 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_59[0][0]',    \n",
            " e)                                                                  'activation_315[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_395 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_59[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_376 (B  (None, 256, 256, 8)          32        ['conv2d_395[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_316 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_376[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_396 (Conv2D)         (None, 256, 256, 8)          584       ['activation_316[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_377 (B  (None, 256, 256, 8)          32        ['conv2d_396[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_317 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_377[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_397 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_317[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_378 (B  (None, 256, 256, 32)         128       ['conv2d_397[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_379 (B  (None, 256, 256, 32)         128       ['batch_normalization_378[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_79 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_378[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_379[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_318 (Activation  (None, 256, 256, 32)         0         ['add_79[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 32)                   0         ['activation_318[0][0]']      \n",
            " 9 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_111 (Reshape)       (None, 1, 1, 32)             0         ['global_average_pooling2d_79[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_158 (Dense)           (None, 1, 1, 2)              64        ['reshape_111[0][0]']         \n",
            "                                                                                                  \n",
            " dense_159 (Dense)           (None, 1, 1, 32)             64        ['dense_158[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_79 (Multiply)      (None, 256, 256, 32)         0         ['activation_318[0][0]',      \n",
            "                                                                     'dense_159[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_399 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_79[0][0]']         \n",
            "                                                                                                  \n",
            " activation_319 (Activation  (None, 256, 256, 1)          0         ['conv2d_399[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 604808 (2.31 MB)\n",
            "Trainable params: 598356 (2.28 MB)\n",
            "Non-trainable params: 6452 (25.20 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape\n",
        "os.environ['SSL_CERT_DIR'] = '/etc/ssl/certs'\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Multiply, Add, BatchNormalization, Activation\n",
        "from keras.layers import Cropping2D,UpSampling2D, Input, Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def residual_block(x, num_filters):\n",
        "    x_init = x\n",
        "    x = Conv2D(num_filters//4, (1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters//4, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, (3, 3), padding=\"same\", kernel_regularizer=l2(0.02))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    s = Conv2D(num_filters, (1, 1), padding=\"same\")(x_init)\n",
        "    s = BatchNormalization()(x)\n",
        "\n",
        "    x = Add()([x, s])\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = squeeze_excite_block(x)\n",
        "    return x\n",
        "\n",
        "def NanoNet_A_sparseAttention(input_shape):\n",
        "    f = [32, 64, 128]\n",
        "    inputs = Input(shape=input_shape, name=\"input_image\")\n",
        "\n",
        "    # Encoder: MobileNetV2\n",
        "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=1)\n",
        "    encoder_output = encoder.get_layer(name=\"block_6_expand_relu\").output\n",
        "    skip_connections_name = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\"]\n",
        "\n",
        "    x = residual_block(encoder_output, 196)  # Residual Block\n",
        "\n",
        "\n",
        "    # SparseAttention Layer\n",
        "    transformer_shape = (x.shape[1], x.shape[2], x.shape[3])\n",
        "    x = Reshape((transformer_shape[0] * transformer_shape[1], transformer_shape[2]))(x)\n",
        "    x = SparseAttention(key_dim=transformer_shape[2], num_heads=2)(x)\n",
        "    x = Reshape((transformer_shape[0], transformer_shape[1], transformer_shape[2]))(x)\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(1, len(skip_connections_name) + 1, 1):\n",
        "        x_skip = encoder.get_layer(skip_connections_name[-i]).output\n",
        "        x_skip = Conv2D(f[-i], (1, 1), padding=\"same\")(x_skip)\n",
        "        x_skip = BatchNormalization()(x_skip)\n",
        "        x_skip = Activation(\"relu\")(x_skip)\n",
        "\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "\n",
        "        try:\n",
        "            x = Concatenate()([x, x_skip])\n",
        "        except Exception as e:\n",
        "            x = Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "            x = Concatenate()([x, x_skip])\n",
        "\n",
        "        x = residual_block(x, f[-i])\n",
        "    # Output layer\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    params = {\"img_height\": 256, \"img_width\": 256, \"img_channels\": 3, \"mask_channels\": 1}\n",
        "    input_shape = (params[\"img_height\"], params[\"img_width\"], params[\"img_channels\"])\n",
        "    model = NanoNet_A_sparseAttention(input_shape)\n",
        "    model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaVq7EoOvQFH"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "IYnZY6mivWl3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from dimensionality_reduction import apply_pca_to_image,reduce_mask_dimension,save_image\n",
        "\n",
        "H = 256\n",
        "W = 256\n",
        "\n",
        "def load_names(path, file_path):\n",
        "    f = open(file_path, \"r\")\n",
        "    data = f.read().split(\"\\n\")[:-1]\n",
        "    images = [os.path.join(path, \"images\", name) + \".jpg\" for name in data]\n",
        "    masks = [os.path.join(path, \"masks\", name) + \".jpg\" for name in data]\n",
        "    return images, masks\n",
        "\n",
        "def load_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    valid_names_path = f\"{path}/val.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    valid_x, valid_y = load_names(path, valid_names_path)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y)\n",
        "\n",
        "def load_test_data(path):\n",
        "    train_names_path = f\"{path}/train.txt\"\n",
        "    test_names_path = f\"{path}/test.txt\"\n",
        "\n",
        "    train_x, train_y = load_names(path, train_names_path)\n",
        "    test_x, test_y = load_names(path, test_names_path)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "def read_image(path):\n",
        "\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_image2(img_path):\n",
        "    return np.load(img_path)\n",
        "\n",
        "def read_mask2(mask_path):\n",
        "    return np.load(mask_path)\n",
        "\n",
        "\n",
        "\n",
        "def augment_data(image, mask):\n",
        "    # Random horizontal flip\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "\n",
        "    # Random vertical flip\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_up_down(image)\n",
        "        mask = tf.image.flip_up_down(mask)\n",
        "\n",
        "    # Random rotation (in 90-degree increments)\n",
        "    num_rotations = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)\n",
        "    image = tf.image.rot90(image, k=num_rotations)\n",
        "    mask = tf.image.rot90(mask, k=num_rotations)\n",
        "\n",
        "    # Intensity-based augmentation\n",
        "\n",
        "    # RGB to HSV\n",
        "    image_hsv = tf.image.rgb_to_hsv(image)\n",
        "\n",
        "    # Do some operations in HSV space, adjust saturation\n",
        "\n",
        "    delta = 0.2\n",
        "    image_hsv = tf.stack([\n",
        "        image_hsv[:, :, 0],  # Hue\n",
        "        tf.clip_by_value(image_hsv[:, :, 1] + delta, 0, 1),  # Saturation\n",
        "        image_hsv[:, :, 2],  # Value\n",
        "    ], axis=-1)\n",
        "\n",
        "    # Convert back to RGB\n",
        "    image_rgb = tf.image.hsv_to_rgb(image_hsv)\n",
        "\n",
        "    return image_rgb, mask\n",
        "\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    x, y = augment_data(x, y)\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch_size=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1000)  # Scramble data\n",
        "    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.cache()  # Cache data\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "# def tf_dataset(x, y, batch=8):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "#     dataset = dataset.map(tf_parse)\n",
        "#     dataset = dataset.batch(batch)\n",
        "#     dataset = dataset.repeat()\n",
        "#     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "#     return dataset\n",
        "\n",
        "\n",
        "def process_dataset(image_paths, mask_paths, save_image_dir, save_mask_dir):\n",
        "    processed_image_paths = []\n",
        "    processed_mask_paths = []\n",
        "\n",
        "    for i, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
        "        img = read_image2(img_path)\n",
        "        mask = read_mask2(mask_path)\n",
        "\n",
        "        processed_img = apply_pca_to_image(img)\n",
        "        processed_mask = reduce_mask_dimension(mask)\n",
        "\n",
        "        processed_img_path = os.path.join(save_image_dir, f\"processed_image_{i}.jpg\")\n",
        "        processed_mask_path = os.path.join(save_mask_dir, f\"processed_mask_{i}.jpg\")\n",
        "\n",
        "        save_image(processed_img, processed_img_path)\n",
        "        save_image(processed_mask, processed_mask_path)\n",
        "\n",
        "        processed_image_paths.append(processed_img_path)\n",
        "        processed_mask_paths.append(processed_mask_path)\n",
        "\n",
        "    return processed_image_paths, processed_mask_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHOMge78ve6a"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FkXytfLvvkPW"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import CustomObjectScope\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import load_model\n",
        "from keras.utils import custom_object_scope\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "    except OSError:\n",
        "        print(f\"Error: creating directory with name {path}\")\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def load_model_file(path):\n",
        "    with CustomObjectScope({\n",
        "            'iou': iou,\n",
        "            'dice_coef': dice_coef,\n",
        "            'dice_loss': dice_loss,\n",
        "            'bce_dice_loss': bce_dice_loss,\n",
        "            'SparseAttention': SparseAttention  #  SparseAttention\n",
        "        }):\n",
        "        model = tf.keras.models.load_model(path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URswsGAAv4pO"
      },
      "source": [
        "# sgdr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "DFBfW-Gev7Lo"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "class SGDRScheduler(Callback):\n",
        "\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DLQuj1MvtK4"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejrVNs_vzm6",
        "outputId": "39c0aabf-205c-4df8-b354-cf36672d88f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_image (InputLayer)    [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 128, 128, 32)         864       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 128, 128, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 128, 128, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 128, 128, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 128, 128, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 128, 128, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 128, 128, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 128, 128, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 128, 128, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 128, 128, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 128, 128, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 129, 129, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 64, 64, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 64, 64, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 64, 64, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 64, 64, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 64, 64, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 64, 64, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 64, 64, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 64, 64, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 64, 64, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 64, 64, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 64, 64, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 64, 64, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 64, 64, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 65, 65, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 32, 32, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 32, 32, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 32, 32, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 32, 32, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 32, 32, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 32, 32, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 32, 32, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 32, 32, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 32, 32, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 32, 32, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 32, 32, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " conv2d_400 (Conv2D)         (None, 32, 32, 49)           9457      ['block_6_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_380 (B  (None, 32, 32, 49)           196       ['conv2d_400[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_320 (Activation  (None, 32, 32, 49)           0         ['batch_normalization_380[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_401 (Conv2D)         (None, 32, 32, 49)           21658     ['activation_320[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_381 (B  (None, 32, 32, 49)           196       ['conv2d_401[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_321 (Activation  (None, 32, 32, 49)           0         ['batch_normalization_381[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_402 (Conv2D)         (None, 32, 32, 196)          86632     ['activation_321[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_382 (B  (None, 32, 32, 196)          784       ['conv2d_402[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_383 (B  (None, 32, 32, 196)          784       ['batch_normalization_382[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_80 (Add)                (None, 32, 32, 196)          0         ['batch_normalization_382[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_383[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_322 (Activation  (None, 32, 32, 196)          0         ['add_80[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 196)                  0         ['activation_322[0][0]']      \n",
            " 0 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_112 (Reshape)       (None, 1, 1, 196)            0         ['global_average_pooling2d_80[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_160 (Dense)           (None, 1, 1, 12)             2352      ['reshape_112[0][0]']         \n",
            "                                                                                                  \n",
            " dense_161 (Dense)           (None, 1, 1, 196)            2352      ['dense_160[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_80 (Multiply)      (None, 32, 32, 196)          0         ['activation_322[0][0]',      \n",
            "                                                                     'dense_161[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_113 (Reshape)       (None, 1024, 196)            0         ['multiply_80[0][0]']         \n",
            "                                                                                                  \n",
            " sparse_attention_22 (Spars  (None, None, 196)            308700    ['reshape_113[0][0]']         \n",
            " eAttention)                                                                                      \n",
            "                                                                                                  \n",
            " conv2d_404 (Conv2D)         (None, 64, 64, 128)          18560     ['block_3_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_114 (Reshape)       (None, 32, 32, 196)          0         ['sparse_attention_22[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_384 (B  (None, 64, 64, 128)          512       ['conv2d_404[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_60 (UpSampli  (None, 64, 64, 196)          0         ['reshape_114[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_323 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_384[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_60 (Concatenat  (None, 64, 64, 324)          0         ['up_sampling2d_60[0][0]',    \n",
            " e)                                                                  'activation_323[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_405 (Conv2D)         (None, 64, 64, 32)           10400     ['concatenate_60[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_385 (B  (None, 64, 64, 32)           128       ['conv2d_405[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_324 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_385[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_406 (Conv2D)         (None, 64, 64, 32)           9248      ['activation_324[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_386 (B  (None, 64, 64, 32)           128       ['conv2d_406[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_325 (Activation  (None, 64, 64, 32)           0         ['batch_normalization_386[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_407 (Conv2D)         (None, 64, 64, 128)          36992     ['activation_325[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_387 (B  (None, 64, 64, 128)          512       ['conv2d_407[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_388 (B  (None, 64, 64, 128)          512       ['batch_normalization_387[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_81 (Add)                (None, 64, 64, 128)          0         ['batch_normalization_387[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_388[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_326 (Activation  (None, 64, 64, 128)          0         ['add_81[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 128)                  0         ['activation_326[0][0]']      \n",
            " 1 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_115 (Reshape)       (None, 1, 1, 128)            0         ['global_average_pooling2d_81[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_162 (Dense)           (None, 1, 1, 8)              1024      ['reshape_115[0][0]']         \n",
            "                                                                                                  \n",
            " dense_163 (Dense)           (None, 1, 1, 128)            1024      ['dense_162[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_409 (Conv2D)         (None, 128, 128, 64)         6208      ['block_1_expand_relu[0][0]'] \n",
            "                                                                                                  \n",
            " multiply_81 (Multiply)      (None, 64, 64, 128)          0         ['activation_326[0][0]',      \n",
            "                                                                     'dense_163[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_389 (B  (None, 128, 128, 64)         256       ['conv2d_409[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_61 (UpSampli  (None, 128, 128, 128)        0         ['multiply_81[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_327 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_389[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_61 (Concatenat  (None, 128, 128, 192)        0         ['up_sampling2d_61[0][0]',    \n",
            " e)                                                                  'activation_327[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_410 (Conv2D)         (None, 128, 128, 16)         3088      ['concatenate_61[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_390 (B  (None, 128, 128, 16)         64        ['conv2d_410[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_328 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_390[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_411 (Conv2D)         (None, 128, 128, 16)         2320      ['activation_328[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_391 (B  (None, 128, 128, 16)         64        ['conv2d_411[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_329 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_391[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_412 (Conv2D)         (None, 128, 128, 64)         9280      ['activation_329[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_392 (B  (None, 128, 128, 64)         256       ['conv2d_412[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_393 (B  (None, 128, 128, 64)         256       ['batch_normalization_392[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_82 (Add)                (None, 128, 128, 64)         0         ['batch_normalization_392[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_393[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_330 (Activation  (None, 128, 128, 64)         0         ['add_82[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 64)                   0         ['activation_330[0][0]']      \n",
            " 2 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_116 (Reshape)       (None, 1, 1, 64)             0         ['global_average_pooling2d_82[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_164 (Dense)           (None, 1, 1, 4)              256       ['reshape_116[0][0]']         \n",
            "                                                                                                  \n",
            " dense_165 (Dense)           (None, 1, 1, 64)             256       ['dense_164[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_414 (Conv2D)         (None, 256, 256, 32)         128       ['input_image[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_82 (Multiply)      (None, 128, 128, 64)         0         ['activation_330[0][0]',      \n",
            "                                                                     'dense_165[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_394 (B  (None, 256, 256, 32)         128       ['conv2d_414[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " up_sampling2d_62 (UpSampli  (None, 256, 256, 64)         0         ['multiply_82[0][0]']         \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " activation_331 (Activation  (None, 256, 256, 32)         0         ['batch_normalization_394[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_62 (Concatenat  (None, 256, 256, 96)         0         ['up_sampling2d_62[0][0]',    \n",
            " e)                                                                  'activation_331[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_415 (Conv2D)         (None, 256, 256, 8)          776       ['concatenate_62[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_395 (B  (None, 256, 256, 8)          32        ['conv2d_415[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_332 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_395[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_416 (Conv2D)         (None, 256, 256, 8)          584       ['activation_332[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_396 (B  (None, 256, 256, 8)          32        ['conv2d_416[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_333 (Activation  (None, 256, 256, 8)          0         ['batch_normalization_396[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_417 (Conv2D)         (None, 256, 256, 32)         2336      ['activation_333[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_397 (B  (None, 256, 256, 32)         128       ['conv2d_417[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_398 (B  (None, 256, 256, 32)         128       ['batch_normalization_397[0][0\n",
            " atchNormalization)                                                 ]']                           \n",
            "                                                                                                  \n",
            " add_83 (Add)                (None, 256, 256, 32)         0         ['batch_normalization_397[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_398[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_334 (Activation  (None, 256, 256, 32)         0         ['add_83[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 32)                   0         ['activation_334[0][0]']      \n",
            " 3 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " reshape_117 (Reshape)       (None, 1, 1, 32)             0         ['global_average_pooling2d_83[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_166 (Dense)           (None, 1, 1, 2)              64        ['reshape_117[0][0]']         \n",
            "                                                                                                  \n",
            " dense_167 (Dense)           (None, 1, 1, 32)             64        ['dense_166[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_83 (Multiply)      (None, 256, 256, 32)         0         ['activation_334[0][0]',      \n",
            "                                                                     'dense_167[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_419 (Conv2D)         (None, 256, 256, 1)          33        ['multiply_83[0][0]']         \n",
            "                                                                                                  \n",
            " activation_335 (Activation  (None, 256, 256, 1)          0         ['conv2d_419[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 604808 (2.31 MB)\n",
            "Trainable params: 598356 (2.28 MB)\n",
            "Non-trainable params: 6452 (25.20 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 13.6250 - dice_coef: 0.2788 - iou: 0.1641 - recall_9: 0.6515 - precision_9: 0.2569\n",
            "Epoch 1: val_loss improved from inf to 12.22052, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 47s 181ms/step - loss: 13.6250 - dice_coef: 0.2788 - iou: 0.1641 - recall_9: 0.6515 - precision_9: 0.2569 - val_loss: 12.2205 - val_dice_coef: 0.2246 - val_iou: 0.1284 - val_recall_9: 0.5120 - val_precision_9: 0.2373 - lr: 1.0000e-04\n",
            "Epoch 2/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 10.6496 - dice_coef: 0.3751 - iou: 0.2337 - recall_9: 0.6832 - precision_9: 0.4445\n",
            "Epoch 2: val_loss improved from 12.22052 to 9.83654, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 10.6496 - dice_coef: 0.3751 - iou: 0.2337 - recall_9: 0.6832 - precision_9: 0.4445 - val_loss: 9.8365 - val_dice_coef: 0.2682 - val_iou: 0.1576 - val_recall_9: 0.6061 - val_precision_9: 0.3773 - lr: 1.0000e-04\n",
            "Epoch 3/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 8.5018 - dice_coef: 0.4757 - iou: 0.3152 - recall_9: 0.7004 - precision_9: 0.5992\n",
            "Epoch 3: val_loss improved from 9.83654 to 7.96187, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 8.5018 - dice_coef: 0.4757 - iou: 0.3152 - recall_9: 0.7004 - precision_9: 0.5992 - val_loss: 7.9619 - val_dice_coef: 0.3171 - val_iou: 0.1913 - val_recall_9: 0.4688 - val_precision_9: 0.6368 - lr: 1.0000e-04\n",
            "Epoch 4/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 6.9543 - dice_coef: 0.5495 - iou: 0.3821 - recall_9: 0.7296 - precision_9: 0.6831\n",
            "Epoch 4: val_loss improved from 7.96187 to 6.64487, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 6.9543 - dice_coef: 0.5495 - iou: 0.3821 - recall_9: 0.7296 - precision_9: 0.6831 - val_loss: 6.6449 - val_dice_coef: 0.3777 - val_iou: 0.2363 - val_recall_9: 0.6048 - val_precision_9: 0.5308 - lr: 1.0000e-04\n",
            "Epoch 5/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 5.7673 - dice_coef: 0.6096 - iou: 0.4418 - recall_9: 0.7534 - precision_9: 0.7477\n",
            "Epoch 5: val_loss improved from 6.64487 to 5.75415, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 5.7673 - dice_coef: 0.6096 - iou: 0.4418 - recall_9: 0.7534 - precision_9: 0.7477 - val_loss: 5.7541 - val_dice_coef: 0.4187 - val_iou: 0.2698 - val_recall_9: 0.7747 - val_precision_9: 0.3921 - lr: 1.0000e-04\n",
            "Epoch 6/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.8387 - dice_coef: 0.6571 - iou: 0.4928 - recall_9: 0.7712 - precision_9: 0.7911\n",
            "Epoch 6: val_loss improved from 5.75415 to 4.79798, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 4.8387 - dice_coef: 0.6571 - iou: 0.4928 - recall_9: 0.7712 - precision_9: 0.7911 - val_loss: 4.7980 - val_dice_coef: 0.5130 - val_iou: 0.3497 - val_recall_9: 0.7289 - val_precision_9: 0.5305 - lr: 1.0000e-04\n",
            "Epoch 7/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.1180 - dice_coef: 0.6866 - iou: 0.5263 - recall_9: 0.7769 - precision_9: 0.8136\n",
            "Epoch 7: val_loss improved from 4.79798 to 4.11707, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 4.1180 - dice_coef: 0.6866 - iou: 0.5263 - recall_9: 0.7769 - precision_9: 0.8136 - val_loss: 4.1171 - val_dice_coef: 0.5075 - val_iou: 0.3428 - val_recall_9: 0.5735 - val_precision_9: 0.6393 - lr: 1.0000e-04\n",
            "Epoch 8/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 3.5243 - dice_coef: 0.7202 - iou: 0.5661 - recall_9: 0.7909 - precision_9: 0.8441\n",
            "Epoch 8: val_loss improved from 4.11707 to 3.63098, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 3.5243 - dice_coef: 0.7202 - iou: 0.5661 - recall_9: 0.7909 - precision_9: 0.8441 - val_loss: 3.6310 - val_dice_coef: 0.5415 - val_iou: 0.3761 - val_recall_9: 0.7218 - val_precision_9: 0.5521 - lr: 1.0000e-04\n",
            "Epoch 9/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 3.0504 - dice_coef: 0.7466 - iou: 0.5990 - recall_9: 0.8068 - precision_9: 0.8614\n",
            "Epoch 9: val_loss improved from 3.63098 to 3.28721, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 3.0504 - dice_coef: 0.7466 - iou: 0.5990 - recall_9: 0.8068 - precision_9: 0.8614 - val_loss: 3.2872 - val_dice_coef: 0.5322 - val_iou: 0.3697 - val_recall_9: 0.7991 - val_precision_9: 0.4936 - lr: 1.0000e-04\n",
            "Epoch 10/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 2.6568 - dice_coef: 0.7708 - iou: 0.6303 - recall_9: 0.8208 - precision_9: 0.8798\n",
            "Epoch 10: val_loss improved from 3.28721 to 2.87270, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 2.6568 - dice_coef: 0.7708 - iou: 0.6303 - recall_9: 0.8208 - precision_9: 0.8798 - val_loss: 2.8727 - val_dice_coef: 0.5608 - val_iou: 0.3929 - val_recall_9: 0.5783 - val_precision_9: 0.6780 - lr: 1.0000e-04\n",
            "Epoch 11/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 2.3394 - dice_coef: 0.7907 - iou: 0.6568 - recall_9: 0.8280 - precision_9: 0.8935\n",
            "Epoch 11: val_loss improved from 2.87270 to 2.58790, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 2.3394 - dice_coef: 0.7907 - iou: 0.6568 - recall_9: 0.8280 - precision_9: 0.8935 - val_loss: 2.5879 - val_dice_coef: 0.6203 - val_iou: 0.4546 - val_recall_9: 0.6836 - val_precision_9: 0.6592 - lr: 1.0000e-04\n",
            "Epoch 12/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 2.0754 - dice_coef: 0.8078 - iou: 0.6806 - recall_9: 0.8393 - precision_9: 0.9019\n",
            "Epoch 12: val_loss improved from 2.58790 to 2.30622, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 2.0754 - dice_coef: 0.8078 - iou: 0.6806 - recall_9: 0.8393 - precision_9: 0.9019 - val_loss: 2.3062 - val_dice_coef: 0.6181 - val_iou: 0.4517 - val_recall_9: 0.6599 - val_precision_9: 0.7019 - lr: 1.0000e-04\n",
            "Epoch 13/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.8664 - dice_coef: 0.8174 - iou: 0.6941 - recall_9: 0.8411 - precision_9: 0.9026\n",
            "Epoch 13: val_loss improved from 2.30622 to 2.16183, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 121ms/step - loss: 1.8664 - dice_coef: 0.8174 - iou: 0.6941 - recall_9: 0.8411 - precision_9: 0.9026 - val_loss: 2.1618 - val_dice_coef: 0.6051 - val_iou: 0.4386 - val_recall_9: 0.7020 - val_precision_9: 0.6303 - lr: 1.0000e-04\n",
            "Epoch 14/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.6828 - dice_coef: 0.8303 - iou: 0.7123 - recall_9: 0.8490 - precision_9: 0.9083\n",
            "Epoch 14: val_loss improved from 2.16183 to 2.06058, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 118ms/step - loss: 1.6828 - dice_coef: 0.8303 - iou: 0.7123 - recall_9: 0.8490 - precision_9: 0.9083 - val_loss: 2.0606 - val_dice_coef: 0.6054 - val_iou: 0.4419 - val_recall_9: 0.7967 - val_precision_9: 0.5588 - lr: 1.0000e-04\n",
            "Epoch 15/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.5020 - dice_coef: 0.8536 - iou: 0.7466 - recall_9: 0.8657 - precision_9: 0.9291\n",
            "Epoch 15: val_loss improved from 2.06058 to 1.93662, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 1.5020 - dice_coef: 0.8536 - iou: 0.7466 - recall_9: 0.8657 - precision_9: 0.9291 - val_loss: 1.9366 - val_dice_coef: 0.5799 - val_iou: 0.4153 - val_recall_9: 0.7498 - val_precision_9: 0.5498 - lr: 1.0000e-04\n",
            "Epoch 16/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.3661 - dice_coef: 0.8627 - iou: 0.7601 - recall_9: 0.8722 - precision_9: 0.9300\n",
            "Epoch 16: val_loss improved from 1.93662 to 1.75161, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 1.3661 - dice_coef: 0.8627 - iou: 0.7601 - recall_9: 0.8722 - precision_9: 0.9300 - val_loss: 1.7516 - val_dice_coef: 0.6167 - val_iou: 0.4516 - val_recall_9: 0.7780 - val_precision_9: 0.6003 - lr: 1.0000e-04\n",
            "Epoch 17/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.2505 - dice_coef: 0.8686 - iou: 0.7694 - recall_9: 0.8758 - precision_9: 0.9316\n",
            "Epoch 17: val_loss improved from 1.75161 to 1.65755, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 1.2505 - dice_coef: 0.8686 - iou: 0.7694 - recall_9: 0.8758 - precision_9: 0.9316 - val_loss: 1.6575 - val_dice_coef: 0.6505 - val_iou: 0.4874 - val_recall_9: 0.7531 - val_precision_9: 0.6472 - lr: 1.0000e-04\n",
            "Epoch 18/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.1547 - dice_coef: 0.8719 - iou: 0.7743 - recall_9: 0.8747 - precision_9: 0.9346\n",
            "Epoch 18: val_loss improved from 1.65755 to 1.54344, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 1.1547 - dice_coef: 0.8719 - iou: 0.7743 - recall_9: 0.8747 - precision_9: 0.9346 - val_loss: 1.5434 - val_dice_coef: 0.6558 - val_iou: 0.4919 - val_recall_9: 0.6952 - val_precision_9: 0.6759 - lr: 1.0000e-04\n",
            "Epoch 19/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0663 - dice_coef: 0.8789 - iou: 0.7853 - recall_9: 0.8797 - precision_9: 0.9355\n",
            "Epoch 19: val_loss did not improve from 1.54344\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 1.0663 - dice_coef: 0.8789 - iou: 0.7853 - recall_9: 0.8797 - precision_9: 0.9355 - val_loss: 1.5587 - val_dice_coef: 0.6246 - val_iou: 0.4635 - val_recall_9: 0.7285 - val_precision_9: 0.6073 - lr: 1.0000e-04\n",
            "Epoch 20/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 1.0295 - dice_coef: 0.8625 - iou: 0.7605 - recall_9: 0.8605 - precision_9: 0.9213\n",
            "Epoch 20: val_loss improved from 1.54344 to 1.46594, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 1.0295 - dice_coef: 0.8625 - iou: 0.7605 - recall_9: 0.8605 - precision_9: 0.9213 - val_loss: 1.4659 - val_dice_coef: 0.6263 - val_iou: 0.4602 - val_recall_9: 0.7724 - val_precision_9: 0.5857 - lr: 1.0000e-04\n",
            "Epoch 21/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.9341 - dice_coef: 0.8849 - iou: 0.7949 - recall_9: 0.8812 - precision_9: 0.9406\n",
            "Epoch 21: val_loss improved from 1.46594 to 1.28600, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.9341 - dice_coef: 0.8849 - iou: 0.7949 - recall_9: 0.8812 - precision_9: 0.9406 - val_loss: 1.2860 - val_dice_coef: 0.6751 - val_iou: 0.5135 - val_recall_9: 0.7284 - val_precision_9: 0.7003 - lr: 1.0000e-04\n",
            "Epoch 22/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.8535 - dice_coef: 0.8995 - iou: 0.8182 - recall_9: 0.8938 - precision_9: 0.9485\n",
            "Epoch 22: val_loss did not improve from 1.28600\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.8535 - dice_coef: 0.8995 - iou: 0.8182 - recall_9: 0.8938 - precision_9: 0.9485 - val_loss: 1.3704 - val_dice_coef: 0.6224 - val_iou: 0.4622 - val_recall_9: 0.8208 - val_precision_9: 0.5538 - lr: 1.0000e-04\n",
            "Epoch 23/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7956 - dice_coef: 0.9047 - iou: 0.8267 - recall_9: 0.8974 - precision_9: 0.9522\n",
            "Epoch 23: val_loss improved from 1.28600 to 1.19795, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.7956 - dice_coef: 0.9047 - iou: 0.8267 - recall_9: 0.8974 - precision_9: 0.9522 - val_loss: 1.1980 - val_dice_coef: 0.6941 - val_iou: 0.5371 - val_recall_9: 0.7231 - val_precision_9: 0.7155 - lr: 1.0000e-04\n",
            "Epoch 24/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7551 - dice_coef: 0.9041 - iou: 0.8258 - recall_9: 0.8950 - precision_9: 0.9491\n",
            "Epoch 24: val_loss did not improve from 1.19795\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.7551 - dice_coef: 0.9041 - iou: 0.8258 - recall_9: 0.8950 - precision_9: 0.9491 - val_loss: 1.2035 - val_dice_coef: 0.6865 - val_iou: 0.5275 - val_recall_9: 0.7276 - val_precision_9: 0.6854 - lr: 1.0000e-04\n",
            "Epoch 25/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.7268 - dice_coef: 0.8983 - iou: 0.8167 - recall_9: 0.8866 - precision_9: 0.9449\n",
            "Epoch 25: val_loss improved from 1.19795 to 1.08619, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.7268 - dice_coef: 0.8983 - iou: 0.8167 - recall_9: 0.8866 - precision_9: 0.9449 - val_loss: 1.0862 - val_dice_coef: 0.7061 - val_iou: 0.5513 - val_recall_9: 0.7181 - val_precision_9: 0.7544 - lr: 1.0000e-04\n",
            "Epoch 26/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6844 - dice_coef: 0.9035 - iou: 0.8247 - recall_9: 0.8922 - precision_9: 0.9476\n",
            "Epoch 26: val_loss did not improve from 1.08619\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.6844 - dice_coef: 0.9035 - iou: 0.8247 - recall_9: 0.8922 - precision_9: 0.9476 - val_loss: 1.1104 - val_dice_coef: 0.6990 - val_iou: 0.5421 - val_recall_9: 0.7628 - val_precision_9: 0.6954 - lr: 1.0000e-04\n",
            "Epoch 27/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6512 - dice_coef: 0.9048 - iou: 0.8270 - recall_9: 0.8924 - precision_9: 0.9485\n",
            "Epoch 27: val_loss did not improve from 1.08619\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.6512 - dice_coef: 0.9048 - iou: 0.8270 - recall_9: 0.8924 - precision_9: 0.9485 - val_loss: 1.1469 - val_dice_coef: 0.6437 - val_iou: 0.4829 - val_recall_9: 0.7188 - val_precision_9: 0.6435 - lr: 1.0000e-04\n",
            "Epoch 28/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.6040 - dice_coef: 0.9144 - iou: 0.8429 - recall_9: 0.9027 - precision_9: 0.9554\n",
            "Epoch 28: val_loss did not improve from 1.08619\n",
            "98/98 [==============================] - 10s 106ms/step - loss: 0.6040 - dice_coef: 0.9144 - iou: 0.8429 - recall_9: 0.9027 - precision_9: 0.9554 - val_loss: 1.1579 - val_dice_coef: 0.6492 - val_iou: 0.4896 - val_recall_9: 0.4929 - val_precision_9: 0.8954 - lr: 1.0000e-04\n",
            "Epoch 29/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5687 - dice_coef: 0.9192 - iou: 0.8515 - recall_9: 0.9059 - precision_9: 0.9580\n",
            "Epoch 29: val_loss did not improve from 1.08619\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.5687 - dice_coef: 0.9192 - iou: 0.8515 - recall_9: 0.9059 - precision_9: 0.9580 - val_loss: 1.2715 - val_dice_coef: 0.5609 - val_iou: 0.3956 - val_recall_9: 0.3985 - val_precision_9: 0.8799 - lr: 1.0000e-04\n",
            "Epoch 30/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5445 - dice_coef: 0.9196 - iou: 0.8518 - recall_9: 0.9051 - precision_9: 0.9566\n",
            "Epoch 30: val_loss did not improve from 1.08619\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.5445 - dice_coef: 0.9196 - iou: 0.8518 - recall_9: 0.9051 - precision_9: 0.9566 - val_loss: 1.2617 - val_dice_coef: 0.5625 - val_iou: 0.4000 - val_recall_9: 0.4035 - val_precision_9: 0.8516 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5287 - dice_coef: 0.9160 - iou: 0.8461 - recall_9: 0.9026 - precision_9: 0.9544\n",
            "Epoch 31: val_loss improved from 1.08619 to 1.03893, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 118ms/step - loss: 0.5287 - dice_coef: 0.9160 - iou: 0.8461 - recall_9: 0.9026 - precision_9: 0.9544 - val_loss: 1.0389 - val_dice_coef: 0.6851 - val_iou: 0.5301 - val_recall_9: 0.6147 - val_precision_9: 0.7882 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.5058 - dice_coef: 0.9198 - iou: 0.8523 - recall_9: 0.9026 - precision_9: 0.9567\n",
            "Epoch 32: val_loss improved from 1.03893 to 0.99696, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.5058 - dice_coef: 0.9198 - iou: 0.8523 - recall_9: 0.9026 - precision_9: 0.9567 - val_loss: 0.9970 - val_dice_coef: 0.6945 - val_iou: 0.5378 - val_recall_9: 0.5737 - val_precision_9: 0.8709 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4803 - dice_coef: 0.9227 - iou: 0.8572 - recall_9: 0.9062 - precision_9: 0.9593\n",
            "Epoch 33: val_loss did not improve from 0.99696\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.4803 - dice_coef: 0.9227 - iou: 0.8572 - recall_9: 0.9062 - precision_9: 0.9593 - val_loss: 1.0346 - val_dice_coef: 0.6562 - val_iou: 0.4933 - val_recall_9: 0.5059 - val_precision_9: 0.9112 - lr: 1.0000e-04\n",
            "Epoch 34/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4619 - dice_coef: 0.9245 - iou: 0.8605 - recall_9: 0.9063 - precision_9: 0.9579\n",
            "Epoch 34: val_loss did not improve from 0.99696\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.4619 - dice_coef: 0.9245 - iou: 0.8605 - recall_9: 0.9063 - precision_9: 0.9579 - val_loss: 1.0926 - val_dice_coef: 0.6171 - val_iou: 0.4518 - val_recall_9: 0.4503 - val_precision_9: 0.9279 - lr: 1.0000e-04\n",
            "Epoch 35/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4333 - dice_coef: 0.9305 - iou: 0.8705 - recall_9: 0.9124 - precision_9: 0.9633\n",
            "Epoch 35: val_loss did not improve from 0.99696\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.4333 - dice_coef: 0.9305 - iou: 0.8705 - recall_9: 0.9124 - precision_9: 0.9633 - val_loss: 1.1785 - val_dice_coef: 0.5469 - val_iou: 0.3853 - val_recall_9: 0.3861 - val_precision_9: 0.9414 - lr: 1.0000e-04\n",
            "Epoch 36/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4109 - dice_coef: 0.9340 - iou: 0.8769 - recall_9: 0.9153 - precision_9: 0.9648\n",
            "Epoch 36: val_loss improved from 0.99696 to 0.86150, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.4109 - dice_coef: 0.9340 - iou: 0.8769 - recall_9: 0.9153 - precision_9: 0.9648 - val_loss: 0.8615 - val_dice_coef: 0.7336 - val_iou: 0.5846 - val_recall_9: 0.6831 - val_precision_9: 0.8022 - lr: 1.0000e-04\n",
            "Epoch 37/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3913 - dice_coef: 0.9364 - iou: 0.8808 - recall_9: 0.9176 - precision_9: 0.9669\n",
            "Epoch 37: val_loss improved from 0.86150 to 0.85164, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.3913 - dice_coef: 0.9364 - iou: 0.8808 - recall_9: 0.9176 - precision_9: 0.9669 - val_loss: 0.8516 - val_dice_coef: 0.7399 - val_iou: 0.5921 - val_recall_9: 0.6898 - val_precision_9: 0.8015 - lr: 1.0000e-04\n",
            "Epoch 38/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3654 - dice_coef: 0.9421 - iou: 0.8909 - recall_9: 0.9236 - precision_9: 0.9713\n",
            "Epoch 38: val_loss improved from 0.85164 to 0.84522, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.3654 - dice_coef: 0.9421 - iou: 0.8909 - recall_9: 0.9236 - precision_9: 0.9713 - val_loss: 0.8452 - val_dice_coef: 0.7272 - val_iou: 0.5756 - val_recall_9: 0.7051 - val_precision_9: 0.7667 - lr: 1.0000e-04\n",
            "Epoch 39/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3524 - dice_coef: 0.9422 - iou: 0.8910 - recall_9: 0.9233 - precision_9: 0.9699\n",
            "Epoch 39: val_loss did not improve from 0.84522\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.3524 - dice_coef: 0.9422 - iou: 0.8910 - recall_9: 0.9233 - precision_9: 0.9699 - val_loss: 0.8508 - val_dice_coef: 0.7127 - val_iou: 0.5615 - val_recall_9: 0.6239 - val_precision_9: 0.8261 - lr: 1.0000e-04\n",
            "Epoch 40/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3381 - dice_coef: 0.9432 - iou: 0.8929 - recall_9: 0.9240 - precision_9: 0.9710\n",
            "Epoch 40: val_loss improved from 0.84522 to 0.81197, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.3381 - dice_coef: 0.9432 - iou: 0.8929 - recall_9: 0.9240 - precision_9: 0.9710 - val_loss: 0.8120 - val_dice_coef: 0.7343 - val_iou: 0.5849 - val_recall_9: 0.6841 - val_precision_9: 0.8026 - lr: 1.0000e-04\n",
            "Epoch 41/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3397 - dice_coef: 0.9366 - iou: 0.8812 - recall_9: 0.9182 - precision_9: 0.9644\n",
            "Epoch 41: val_loss did not improve from 0.81197\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.3397 - dice_coef: 0.9366 - iou: 0.8812 - recall_9: 0.9182 - precision_9: 0.9644 - val_loss: 0.8172 - val_dice_coef: 0.7302 - val_iou: 0.5818 - val_recall_9: 0.6532 - val_precision_9: 0.8225 - lr: 1.0000e-04\n",
            "Epoch 42/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3187 - dice_coef: 0.9425 - iou: 0.8916 - recall_9: 0.9255 - precision_9: 0.9687\n",
            "Epoch 42: val_loss improved from 0.81197 to 0.81094, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.3187 - dice_coef: 0.9425 - iou: 0.8916 - recall_9: 0.9255 - precision_9: 0.9687 - val_loss: 0.8109 - val_dice_coef: 0.7333 - val_iou: 0.5850 - val_recall_9: 0.7098 - val_precision_9: 0.7670 - lr: 1.0000e-04\n",
            "Epoch 43/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.3064 - dice_coef: 0.9442 - iou: 0.8946 - recall_9: 0.9244 - precision_9: 0.9707\n",
            "Epoch 43: val_loss did not improve from 0.81094\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.3064 - dice_coef: 0.9442 - iou: 0.8946 - recall_9: 0.9244 - precision_9: 0.9707 - val_loss: 0.8181 - val_dice_coef: 0.7278 - val_iou: 0.5783 - val_recall_9: 0.6397 - val_precision_9: 0.8310 - lr: 1.0000e-04\n",
            "Epoch 44/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2958 - dice_coef: 0.9444 - iou: 0.8949 - recall_9: 0.9245 - precision_9: 0.9712\n",
            "Epoch 44: val_loss improved from 0.81094 to 0.77905, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 0.2958 - dice_coef: 0.9444 - iou: 0.8949 - recall_9: 0.9245 - precision_9: 0.9712 - val_loss: 0.7791 - val_dice_coef: 0.7484 - val_iou: 0.6031 - val_recall_9: 0.6798 - val_precision_9: 0.8221 - lr: 1.0000e-04\n",
            "Epoch 45/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2831 - dice_coef: 0.9462 - iou: 0.8983 - recall_9: 0.9271 - precision_9: 0.9726\n",
            "Epoch 45: val_loss did not improve from 0.77905\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.2831 - dice_coef: 0.9462 - iou: 0.8983 - recall_9: 0.9271 - precision_9: 0.9726 - val_loss: 0.7895 - val_dice_coef: 0.7382 - val_iou: 0.5896 - val_recall_9: 0.6543 - val_precision_9: 0.8344 - lr: 1.0000e-04\n",
            "Epoch 46/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2708 - dice_coef: 0.9489 - iou: 0.9030 - recall_9: 0.9285 - precision_9: 0.9739\n",
            "Epoch 46: val_loss did not improve from 0.77905\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.2708 - dice_coef: 0.9489 - iou: 0.9030 - recall_9: 0.9285 - precision_9: 0.9739 - val_loss: 0.8145 - val_dice_coef: 0.7343 - val_iou: 0.5861 - val_recall_9: 0.6453 - val_precision_9: 0.8287 - lr: 1.0000e-04\n",
            "Epoch 47/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2633 - dice_coef: 0.9490 - iou: 0.9032 - recall_9: 0.9280 - precision_9: 0.9738\n",
            "Epoch 47: val_loss improved from 0.77905 to 0.76857, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 123ms/step - loss: 0.2633 - dice_coef: 0.9490 - iou: 0.9032 - recall_9: 0.9280 - precision_9: 0.9738 - val_loss: 0.7686 - val_dice_coef: 0.7236 - val_iou: 0.5731 - val_recall_9: 0.7066 - val_precision_9: 0.7544 - lr: 1.0000e-04\n",
            "Epoch 48/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2548 - dice_coef: 0.9499 - iou: 0.9049 - recall_9: 0.9303 - precision_9: 0.9728\n",
            "Epoch 48: val_loss did not improve from 0.76857\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 0.2548 - dice_coef: 0.9499 - iou: 0.9049 - recall_9: 0.9303 - precision_9: 0.9728 - val_loss: 0.7949 - val_dice_coef: 0.7290 - val_iou: 0.5777 - val_recall_9: 0.7280 - val_precision_9: 0.7437 - lr: 1.0000e-04\n",
            "Epoch 49/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2799 - dice_coef: 0.9352 - iou: 0.8793 - recall_9: 0.9132 - precision_9: 0.9645\n",
            "Epoch 49: val_loss did not improve from 0.76857\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.2799 - dice_coef: 0.9352 - iou: 0.8793 - recall_9: 0.9132 - precision_9: 0.9645 - val_loss: 0.8470 - val_dice_coef: 0.7193 - val_iou: 0.5663 - val_recall_9: 0.7619 - val_precision_9: 0.6989 - lr: 1.0000e-04\n",
            "Epoch 50/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2598 - dice_coef: 0.9435 - iou: 0.8935 - recall_9: 0.9226 - precision_9: 0.9687\n",
            "Epoch 50: val_loss improved from 0.76857 to 0.75908, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.2598 - dice_coef: 0.9435 - iou: 0.8935 - recall_9: 0.9226 - precision_9: 0.9687 - val_loss: 0.7591 - val_dice_coef: 0.7252 - val_iou: 0.5744 - val_recall_9: 0.6371 - val_precision_9: 0.8346 - lr: 1.0000e-04\n",
            "Epoch 51/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2348 - dice_coef: 0.9528 - iou: 0.9101 - recall_9: 0.9331 - precision_9: 0.9757\n",
            "Epoch 51: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.2348 - dice_coef: 0.9528 - iou: 0.9101 - recall_9: 0.9331 - precision_9: 0.9757 - val_loss: 0.7897 - val_dice_coef: 0.7290 - val_iou: 0.5813 - val_recall_9: 0.6011 - val_precision_9: 0.8815 - lr: 1.0000e-04\n",
            "Epoch 52/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2355 - dice_coef: 0.9500 - iou: 0.9049 - recall_9: 0.9272 - precision_9: 0.9742\n",
            "Epoch 52: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 0.2355 - dice_coef: 0.9500 - iou: 0.9049 - recall_9: 0.9272 - precision_9: 0.9742 - val_loss: 0.7881 - val_dice_coef: 0.7181 - val_iou: 0.5665 - val_recall_9: 0.5825 - val_precision_9: 0.8893 - lr: 1.0000e-04\n",
            "Epoch 53/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2234 - dice_coef: 0.9524 - iou: 0.9096 - recall_9: 0.9316 - precision_9: 0.9766\n",
            "Epoch 53: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.2234 - dice_coef: 0.9524 - iou: 0.9096 - recall_9: 0.9316 - precision_9: 0.9766 - val_loss: 1.0462 - val_dice_coef: 0.6087 - val_iou: 0.4452 - val_recall_9: 0.4234 - val_precision_9: 0.9174 - lr: 1.0000e-04\n",
            "Epoch 54/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2084 - dice_coef: 0.9580 - iou: 0.9195 - recall_9: 0.9373 - precision_9: 0.9791\n",
            "Epoch 54: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.2084 - dice_coef: 0.9580 - iou: 0.9195 - recall_9: 0.9373 - precision_9: 0.9791 - val_loss: 0.9182 - val_dice_coef: 0.6324 - val_iou: 0.4699 - val_recall_9: 0.4577 - val_precision_9: 0.9279 - lr: 1.0000e-04\n",
            "Epoch 55/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1948 - dice_coef: 0.9616 - iou: 0.9262 - recall_9: 0.9422 - precision_9: 0.9814\n",
            "Epoch 55: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1948 - dice_coef: 0.9616 - iou: 0.9262 - recall_9: 0.9422 - precision_9: 0.9814 - val_loss: 0.7629 - val_dice_coef: 0.7163 - val_iou: 0.5641 - val_recall_9: 0.5851 - val_precision_9: 0.8777 - lr: 1.0000e-04\n",
            "Epoch 56/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2025 - dice_coef: 0.9553 - iou: 0.9148 - recall_9: 0.9332 - precision_9: 0.9779\n",
            "Epoch 56: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.2025 - dice_coef: 0.9553 - iou: 0.9148 - recall_9: 0.9332 - precision_9: 0.9779 - val_loss: 0.7629 - val_dice_coef: 0.7311 - val_iou: 0.5807 - val_recall_9: 0.6086 - val_precision_9: 0.8797 - lr: 1.0000e-04\n",
            "Epoch 57/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2181 - dice_coef: 0.9461 - iou: 0.8984 - recall_9: 0.9230 - precision_9: 0.9703\n",
            "Epoch 57: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.2181 - dice_coef: 0.9461 - iou: 0.8984 - recall_9: 0.9230 - precision_9: 0.9703 - val_loss: 0.8064 - val_dice_coef: 0.7189 - val_iou: 0.5662 - val_recall_9: 0.6224 - val_precision_9: 0.8297 - lr: 1.0000e-04\n",
            "Epoch 58/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2147 - dice_coef: 0.9464 - iou: 0.8989 - recall_9: 0.9249 - precision_9: 0.9708\n",
            "Epoch 58: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.2147 - dice_coef: 0.9464 - iou: 0.8989 - recall_9: 0.9249 - precision_9: 0.9708 - val_loss: 0.7621 - val_dice_coef: 0.7316 - val_iou: 0.5800 - val_recall_9: 0.7212 - val_precision_9: 0.7528 - lr: 1.0000e-04\n",
            "Epoch 59/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1920 - dice_coef: 0.9572 - iou: 0.9182 - recall_9: 0.9348 - precision_9: 0.9789\n",
            "Epoch 59: val_loss did not improve from 0.75908\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1920 - dice_coef: 0.9572 - iou: 0.9182 - recall_9: 0.9348 - precision_9: 0.9789 - val_loss: 0.8041 - val_dice_coef: 0.6991 - val_iou: 0.5435 - val_recall_9: 0.5598 - val_precision_9: 0.9049 - lr: 1.0000e-04\n",
            "Epoch 60/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1834 - dice_coef: 0.9590 - iou: 0.9214 - recall_9: 0.9372 - precision_9: 0.9796\n",
            "Epoch 60: val_loss improved from 0.75908 to 0.71686, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1834 - dice_coef: 0.9590 - iou: 0.9214 - recall_9: 0.9372 - precision_9: 0.9796 - val_loss: 0.7169 - val_dice_coef: 0.7434 - val_iou: 0.5968 - val_recall_9: 0.6467 - val_precision_9: 0.8472 - lr: 1.0000e-04\n",
            "Epoch 61/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1761 - dice_coef: 0.9605 - iou: 0.9241 - recall_9: 0.9388 - precision_9: 0.9804\n",
            "Epoch 61: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1761 - dice_coef: 0.9605 - iou: 0.9241 - recall_9: 0.9388 - precision_9: 0.9804 - val_loss: 0.7180 - val_dice_coef: 0.7454 - val_iou: 0.5990 - val_recall_9: 0.7110 - val_precision_9: 0.7767 - lr: 1.0000e-04\n",
            "Epoch 62/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1651 - dice_coef: 0.9640 - iou: 0.9306 - recall_9: 0.9417 - precision_9: 0.9832\n",
            "Epoch 62: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 107ms/step - loss: 0.1651 - dice_coef: 0.9640 - iou: 0.9306 - recall_9: 0.9417 - precision_9: 0.9832 - val_loss: 0.7640 - val_dice_coef: 0.7231 - val_iou: 0.5727 - val_recall_9: 0.5937 - val_precision_9: 0.8765 - lr: 1.0000e-04\n",
            "Epoch 63/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1656 - dice_coef: 0.9619 - iou: 0.9269 - recall_9: 0.9382 - precision_9: 0.9814\n",
            "Epoch 63: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1656 - dice_coef: 0.9619 - iou: 0.9269 - recall_9: 0.9382 - precision_9: 0.9814 - val_loss: 0.7509 - val_dice_coef: 0.7344 - val_iou: 0.5861 - val_recall_9: 0.6123 - val_precision_9: 0.8755 - lr: 1.0000e-04\n",
            "Epoch 64/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1581 - dice_coef: 0.9635 - iou: 0.9296 - recall_9: 0.9414 - precision_9: 0.9822\n",
            "Epoch 64: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1581 - dice_coef: 0.9635 - iou: 0.9296 - recall_9: 0.9414 - precision_9: 0.9822 - val_loss: 0.7505 - val_dice_coef: 0.7249 - val_iou: 0.5760 - val_recall_9: 0.5971 - val_precision_9: 0.8702 - lr: 1.0000e-04\n",
            "Epoch 65/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1502 - dice_coef: 0.9656 - iou: 0.9336 - recall_9: 0.9434 - precision_9: 0.9839\n",
            "Epoch 65: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1502 - dice_coef: 0.9656 - iou: 0.9336 - recall_9: 0.9434 - precision_9: 0.9839 - val_loss: 0.7503 - val_dice_coef: 0.7184 - val_iou: 0.5683 - val_recall_9: 0.5779 - val_precision_9: 0.8791 - lr: 1.0000e-04\n",
            "Epoch 66/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1476 - dice_coef: 0.9652 - iou: 0.9328 - recall_9: 0.9433 - precision_9: 0.9828\n",
            "Epoch 66: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.1476 - dice_coef: 0.9652 - iou: 0.9328 - recall_9: 0.9433 - precision_9: 0.9828 - val_loss: 0.7547 - val_dice_coef: 0.7207 - val_iou: 0.5709 - val_recall_9: 0.5866 - val_precision_9: 0.8754 - lr: 1.0000e-04\n",
            "Epoch 67/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1452 - dice_coef: 0.9647 - iou: 0.9319 - recall_9: 0.9427 - precision_9: 0.9829\n",
            "Epoch 67: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1452 - dice_coef: 0.9647 - iou: 0.9319 - recall_9: 0.9427 - precision_9: 0.9829 - val_loss: 0.7292 - val_dice_coef: 0.7375 - val_iou: 0.5890 - val_recall_9: 0.7077 - val_precision_9: 0.7574 - lr: 1.0000e-04\n",
            "Epoch 68/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1614 - dice_coef: 0.9552 - iou: 0.9147 - recall_9: 0.9330 - precision_9: 0.9753\n",
            "Epoch 68: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1614 - dice_coef: 0.9552 - iou: 0.9147 - recall_9: 0.9330 - precision_9: 0.9753 - val_loss: 0.8959 - val_dice_coef: 0.6868 - val_iou: 0.5272 - val_recall_9: 0.5827 - val_precision_9: 0.7960 - lr: 1.0000e-04\n",
            "Epoch 69/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2418 - dice_coef: 0.9166 - iou: 0.8475 - recall_9: 0.8931 - precision_9: 0.9493\n",
            "Epoch 69: val_loss did not improve from 0.71686\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.2418 - dice_coef: 0.9166 - iou: 0.8475 - recall_9: 0.8931 - precision_9: 0.9493 - val_loss: 0.7982 - val_dice_coef: 0.7027 - val_iou: 0.5461 - val_recall_9: 0.7927 - val_precision_9: 0.6666 - lr: 1.0000e-04\n",
            "Epoch 70/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2645 - dice_coef: 0.9082 - iou: 0.8345 - recall_9: 0.8889 - precision_9: 0.9431\n",
            "Epoch 70: val_loss improved from 0.71686 to 0.71437, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.2645 - dice_coef: 0.9082 - iou: 0.8345 - recall_9: 0.8889 - precision_9: 0.9431 - val_loss: 0.7144 - val_dice_coef: 0.7190 - val_iou: 0.5660 - val_recall_9: 0.7313 - val_precision_9: 0.7375 - lr: 1.0000e-04\n",
            "Epoch 71/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2151 - dice_coef: 0.9347 - iou: 0.8784 - recall_9: 0.9112 - precision_9: 0.9633\n",
            "Epoch 71: val_loss did not improve from 0.71437\n",
            "98/98 [==============================] - 11s 108ms/step - loss: 0.2151 - dice_coef: 0.9347 - iou: 0.8784 - recall_9: 0.9112 - precision_9: 0.9633 - val_loss: 0.7895 - val_dice_coef: 0.7169 - val_iou: 0.5640 - val_recall_9: 0.7860 - val_precision_9: 0.6843 - lr: 1.0000e-04\n",
            "Epoch 72/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1763 - dice_coef: 0.9545 - iou: 0.9131 - recall_9: 0.9333 - precision_9: 0.9747\n",
            "Epoch 72: val_loss improved from 0.71437 to 0.69120, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 126ms/step - loss: 0.1763 - dice_coef: 0.9545 - iou: 0.9131 - recall_9: 0.9333 - precision_9: 0.9747 - val_loss: 0.6912 - val_dice_coef: 0.7458 - val_iou: 0.6006 - val_recall_9: 0.6901 - val_precision_9: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 73/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1529 - dice_coef: 0.9643 - iou: 0.9311 - recall_9: 0.9426 - precision_9: 0.9826\n",
            "Epoch 73: val_loss improved from 0.69120 to 0.67515, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1529 - dice_coef: 0.9643 - iou: 0.9311 - recall_9: 0.9426 - precision_9: 0.9826 - val_loss: 0.6751 - val_dice_coef: 0.7516 - val_iou: 0.6076 - val_recall_9: 0.6918 - val_precision_9: 0.8056 - lr: 1.0000e-04\n",
            "Epoch 74/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1398 - dice_coef: 0.9685 - iou: 0.9391 - recall_9: 0.9462 - precision_9: 0.9859\n",
            "Epoch 74: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1398 - dice_coef: 0.9685 - iou: 0.9391 - recall_9: 0.9462 - precision_9: 0.9859 - val_loss: 0.6772 - val_dice_coef: 0.7524 - val_iou: 0.6082 - val_recall_9: 0.6825 - val_precision_9: 0.8180 - lr: 1.0000e-04\n",
            "Epoch 75/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1309 - dice_coef: 0.9707 - iou: 0.9431 - recall_9: 0.9480 - precision_9: 0.9875\n",
            "Epoch 75: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.1309 - dice_coef: 0.9707 - iou: 0.9431 - recall_9: 0.9480 - precision_9: 0.9875 - val_loss: 0.6885 - val_dice_coef: 0.7493 - val_iou: 0.6045 - val_recall_9: 0.6786 - val_precision_9: 0.8163 - lr: 1.0000e-04\n",
            "Epoch 76/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1237 - dice_coef: 0.9722 - iou: 0.9460 - recall_9: 0.9489 - precision_9: 0.9887\n",
            "Epoch 76: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1237 - dice_coef: 0.9722 - iou: 0.9460 - recall_9: 0.9489 - precision_9: 0.9887 - val_loss: 0.6939 - val_dice_coef: 0.7522 - val_iou: 0.6078 - val_recall_9: 0.6812 - val_precision_9: 0.8166 - lr: 1.0000e-04\n",
            "Epoch 77/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1183 - dice_coef: 0.9729 - iou: 0.9473 - recall_9: 0.9496 - precision_9: 0.9889\n",
            "Epoch 77: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1183 - dice_coef: 0.9729 - iou: 0.9473 - recall_9: 0.9496 - precision_9: 0.9889 - val_loss: 0.7005 - val_dice_coef: 0.7518 - val_iou: 0.6074 - val_recall_9: 0.6852 - val_precision_9: 0.8099 - lr: 1.0000e-04\n",
            "Epoch 78/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1142 - dice_coef: 0.9731 - iou: 0.9476 - recall_9: 0.9499 - precision_9: 0.9890\n",
            "Epoch 78: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1142 - dice_coef: 0.9731 - iou: 0.9476 - recall_9: 0.9499 - precision_9: 0.9890 - val_loss: 0.7048 - val_dice_coef: 0.7534 - val_iou: 0.6093 - val_recall_9: 0.6741 - val_precision_9: 0.8278 - lr: 1.0000e-04\n",
            "Epoch 79/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1104 - dice_coef: 0.9733 - iou: 0.9480 - recall_9: 0.9493 - precision_9: 0.9893\n",
            "Epoch 79: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1104 - dice_coef: 0.9733 - iou: 0.9480 - recall_9: 0.9493 - precision_9: 0.9893 - val_loss: 0.7246 - val_dice_coef: 0.7464 - val_iou: 0.6005 - val_recall_9: 0.7278 - val_precision_9: 0.7621 - lr: 1.0000e-04\n",
            "Epoch 80/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1071 - dice_coef: 0.9734 - iou: 0.9483 - recall_9: 0.9495 - precision_9: 0.9891\n",
            "Epoch 80: val_loss did not improve from 0.67515\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.1071 - dice_coef: 0.9734 - iou: 0.9483 - recall_9: 0.9495 - precision_9: 0.9891 - val_loss: 0.7208 - val_dice_coef: 0.7467 - val_iou: 0.6017 - val_recall_9: 0.6525 - val_precision_9: 0.8407 - lr: 1.0000e-04\n",
            "Epoch 81/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1064 - dice_coef: 0.9724 - iou: 0.9463 - recall_9: 0.9491 - precision_9: 0.9879\n",
            "Epoch 81: val_loss improved from 0.67515 to 0.67250, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 119ms/step - loss: 0.1064 - dice_coef: 0.9724 - iou: 0.9463 - recall_9: 0.9491 - precision_9: 0.9879 - val_loss: 0.6725 - val_dice_coef: 0.7520 - val_iou: 0.6069 - val_recall_9: 0.6913 - val_precision_9: 0.8072 - lr: 1.0000e-04\n",
            "Epoch 82/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1195 - dice_coef: 0.9644 - iou: 0.9313 - recall_9: 0.9414 - precision_9: 0.9835\n",
            "Epoch 82: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1195 - dice_coef: 0.9644 - iou: 0.9313 - recall_9: 0.9414 - precision_9: 0.9835 - val_loss: 0.7965 - val_dice_coef: 0.7439 - val_iou: 0.5966 - val_recall_9: 0.7483 - val_precision_9: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 83/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1164 - dice_coef: 0.9663 - iou: 0.9350 - recall_9: 0.9434 - precision_9: 0.9838\n",
            "Epoch 83: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1164 - dice_coef: 0.9663 - iou: 0.9350 - recall_9: 0.9434 - precision_9: 0.9838 - val_loss: 0.7132 - val_dice_coef: 0.7464 - val_iou: 0.5999 - val_recall_9: 0.6933 - val_precision_9: 0.7913 - lr: 1.0000e-04\n",
            "Epoch 84/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1704 - dice_coef: 0.9378 - iou: 0.8845 - recall_9: 0.9157 - precision_9: 0.9620\n",
            "Epoch 84: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1704 - dice_coef: 0.9378 - iou: 0.8845 - recall_9: 0.9157 - precision_9: 0.9620 - val_loss: 0.8050 - val_dice_coef: 0.7006 - val_iou: 0.5444 - val_recall_9: 0.7964 - val_precision_9: 0.6683 - lr: 1.0000e-04\n",
            "Epoch 85/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1752 - dice_coef: 0.9395 - iou: 0.8869 - recall_9: 0.9181 - precision_9: 0.9659\n",
            "Epoch 85: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1752 - dice_coef: 0.9395 - iou: 0.8869 - recall_9: 0.9181 - precision_9: 0.9659 - val_loss: 0.7185 - val_dice_coef: 0.7075 - val_iou: 0.5539 - val_recall_9: 0.7572 - val_precision_9: 0.6930 - lr: 1.0000e-04\n",
            "Epoch 86/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1601 - dice_coef: 0.9480 - iou: 0.9017 - recall_9: 0.9253 - precision_9: 0.9707\n",
            "Epoch 86: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.1601 - dice_coef: 0.9480 - iou: 0.9017 - recall_9: 0.9253 - precision_9: 0.9707 - val_loss: 0.7510 - val_dice_coef: 0.7286 - val_iou: 0.5779 - val_recall_9: 0.7315 - val_precision_9: 0.7387 - lr: 1.0000e-04\n",
            "Epoch 87/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1236 - dice_coef: 0.9660 - iou: 0.9344 - recall_9: 0.9434 - precision_9: 0.9845\n",
            "Epoch 87: val_loss did not improve from 0.67250\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1236 - dice_coef: 0.9660 - iou: 0.9344 - recall_9: 0.9434 - precision_9: 0.9845 - val_loss: 0.6766 - val_dice_coef: 0.7450 - val_iou: 0.5979 - val_recall_9: 0.7058 - val_precision_9: 0.7880 - lr: 1.0000e-04\n",
            "Epoch 88/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1110 - dice_coef: 0.9710 - iou: 0.9437 - recall_9: 0.9480 - precision_9: 0.9870\n",
            "Epoch 88: val_loss improved from 0.67250 to 0.66353, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.1110 - dice_coef: 0.9710 - iou: 0.9437 - recall_9: 0.9480 - precision_9: 0.9870 - val_loss: 0.6635 - val_dice_coef: 0.7509 - val_iou: 0.6057 - val_recall_9: 0.7061 - val_precision_9: 0.7950 - lr: 1.0000e-04\n",
            "Epoch 89/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1040 - dice_coef: 0.9727 - iou: 0.9470 - recall_9: 0.9495 - precision_9: 0.9883\n",
            "Epoch 89: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1040 - dice_coef: 0.9727 - iou: 0.9470 - recall_9: 0.9495 - precision_9: 0.9883 - val_loss: 0.6727 - val_dice_coef: 0.7509 - val_iou: 0.6054 - val_recall_9: 0.6830 - val_precision_9: 0.8162 - lr: 1.0000e-04\n",
            "Epoch 90/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0976 - dice_coef: 0.9743 - iou: 0.9500 - recall_9: 0.9507 - precision_9: 0.9896\n",
            "Epoch 90: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.0976 - dice_coef: 0.9743 - iou: 0.9500 - recall_9: 0.9507 - precision_9: 0.9896 - val_loss: 0.6787 - val_dice_coef: 0.7490 - val_iou: 0.6033 - val_recall_9: 0.6674 - val_precision_9: 0.8276 - lr: 1.0000e-04\n",
            "Epoch 91/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0931 - dice_coef: 0.9752 - iou: 0.9517 - recall_9: 0.9507 - precision_9: 0.9903\n",
            "Epoch 91: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 116ms/step - loss: 0.0931 - dice_coef: 0.9752 - iou: 0.9517 - recall_9: 0.9507 - precision_9: 0.9903 - val_loss: 0.6917 - val_dice_coef: 0.7481 - val_iou: 0.6030 - val_recall_9: 0.6560 - val_precision_9: 0.8394 - lr: 1.0000e-04\n",
            "Epoch 92/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0893 - dice_coef: 0.9758 - iou: 0.9528 - recall_9: 0.9513 - precision_9: 0.9906\n",
            "Epoch 92: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 113ms/step - loss: 0.0893 - dice_coef: 0.9758 - iou: 0.9528 - recall_9: 0.9513 - precision_9: 0.9906 - val_loss: 0.6892 - val_dice_coef: 0.7492 - val_iou: 0.6041 - val_recall_9: 0.6487 - val_precision_9: 0.8508 - lr: 1.0000e-04\n",
            "Epoch 93/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0874 - dice_coef: 0.9756 - iou: 0.9524 - recall_9: 0.9514 - precision_9: 0.9903\n",
            "Epoch 93: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.0874 - dice_coef: 0.9756 - iou: 0.9524 - recall_9: 0.9514 - precision_9: 0.9903 - val_loss: 0.6640 - val_dice_coef: 0.7474 - val_iou: 0.6003 - val_recall_9: 0.7273 - val_precision_9: 0.7706 - lr: 1.0000e-04\n",
            "Epoch 94/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0868 - dice_coef: 0.9747 - iou: 0.9507 - recall_9: 0.9505 - precision_9: 0.9898\n",
            "Epoch 94: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 108ms/step - loss: 0.0868 - dice_coef: 0.9747 - iou: 0.9507 - recall_9: 0.9505 - precision_9: 0.9898 - val_loss: 0.7087 - val_dice_coef: 0.7456 - val_iou: 0.6008 - val_recall_9: 0.6502 - val_precision_9: 0.8436 - lr: 1.0000e-04\n",
            "Epoch 95/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.0891 - dice_coef: 0.9728 - iou: 0.9471 - recall_9: 0.9484 - precision_9: 0.9886\n",
            "Epoch 95: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.0891 - dice_coef: 0.9728 - iou: 0.9471 - recall_9: 0.9484 - precision_9: 0.9886 - val_loss: 0.7260 - val_dice_coef: 0.7469 - val_iou: 0.6004 - val_recall_9: 0.7166 - val_precision_9: 0.7762 - lr: 1.0000e-04\n",
            "Epoch 96/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1043 - dice_coef: 0.9645 - iou: 0.9316 - recall_9: 0.9411 - precision_9: 0.9823\n",
            "Epoch 96: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1043 - dice_coef: 0.9645 - iou: 0.9316 - recall_9: 0.9411 - precision_9: 0.9823 - val_loss: 0.9321 - val_dice_coef: 0.6912 - val_iou: 0.5329 - val_recall_9: 0.7595 - val_precision_9: 0.6442 - lr: 1.0000e-04\n",
            "Epoch 97/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2233 - dice_coef: 0.9082 - iou: 0.8352 - recall_9: 0.8853 - precision_9: 0.9398\n",
            "Epoch 97: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.2233 - dice_coef: 0.9082 - iou: 0.8352 - recall_9: 0.8853 - precision_9: 0.9398 - val_loss: 2.1471 - val_dice_coef: 0.0798 - val_iou: 0.0428 - val_recall_9: 0.0489 - val_precision_9: 0.9683 - lr: 1.0000e-04\n",
            "Epoch 98/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2425 - dice_coef: 0.9056 - iou: 0.8314 - recall_9: 0.8868 - precision_9: 0.9409\n",
            "Epoch 98: val_loss did not improve from 0.66353\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "98/98 [==============================] - 11s 108ms/step - loss: 0.2425 - dice_coef: 0.9056 - iou: 0.8314 - recall_9: 0.8868 - precision_9: 0.9409 - val_loss: 0.7786 - val_dice_coef: 0.7196 - val_iou: 0.5653 - val_recall_9: 0.6225 - val_precision_9: 0.8257 - lr: 1.0000e-04\n",
            "Epoch 99/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1623 - dice_coef: 0.9462 - iou: 0.9015 - recall_9: 0.9336 - precision_9: 0.9670\n",
            "Epoch 99: val_loss did not improve from 0.66353\n",
            "98/98 [==============================] - 11s 109ms/step - loss: 0.1623 - dice_coef: 0.9462 - iou: 0.9015 - recall_9: 0.9336 - precision_9: 0.9670 - val_loss: 0.6814 - val_dice_coef: 0.7375 - val_iou: 0.5884 - val_recall_9: 0.7085 - val_precision_9: 0.7712 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1453 - dice_coef: 0.9563 - iou: 0.9165 - recall_9: 0.9363 - precision_9: 0.9785\n",
            "Epoch 100: val_loss improved from 0.66353 to 0.65546, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.1453 - dice_coef: 0.9563 - iou: 0.9165 - recall_9: 0.9363 - precision_9: 0.9785 - val_loss: 0.6555 - val_dice_coef: 0.7441 - val_iou: 0.5967 - val_recall_9: 0.7043 - val_precision_9: 0.7896 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1390 - dice_coef: 0.9597 - iou: 0.9226 - recall_9: 0.9388 - precision_9: 0.9809\n",
            "Epoch 101: val_loss improved from 0.65546 to 0.64780, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 117ms/step - loss: 0.1390 - dice_coef: 0.9597 - iou: 0.9226 - recall_9: 0.9388 - precision_9: 0.9809 - val_loss: 0.6478 - val_dice_coef: 0.7475 - val_iou: 0.6011 - val_recall_9: 0.7086 - val_precision_9: 0.7916 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1347 - dice_coef: 0.9618 - iou: 0.9266 - recall_9: 0.9405 - precision_9: 0.9820\n",
            "Epoch 102: val_loss improved from 0.64780 to 0.64740, saving model to files/NanoNet_A_sparseAttention/model.h5\n",
            "98/98 [==============================] - 11s 118ms/step - loss: 0.1347 - dice_coef: 0.9618 - iou: 0.9266 - recall_9: 0.9405 - precision_9: 0.9820 - val_loss: 0.6474 - val_dice_coef: 0.7496 - val_iou: 0.6038 - val_recall_9: 0.7092 - val_precision_9: 0.7941 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1310 - dice_coef: 0.9635 - iou: 0.9297 - recall_9: 0.9422 - precision_9: 0.9831\n",
            "Epoch 103: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1310 - dice_coef: 0.9635 - iou: 0.9297 - recall_9: 0.9422 - precision_9: 0.9831 - val_loss: 0.6489 - val_dice_coef: 0.7508 - val_iou: 0.6053 - val_recall_9: 0.7124 - val_precision_9: 0.7930 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1279 - dice_coef: 0.9649 - iou: 0.9322 - recall_9: 0.9432 - precision_9: 0.9838\n",
            "Epoch 104: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 108ms/step - loss: 0.1279 - dice_coef: 0.9649 - iou: 0.9322 - recall_9: 0.9432 - precision_9: 0.9838 - val_loss: 0.6518 - val_dice_coef: 0.7513 - val_iou: 0.6059 - val_recall_9: 0.7167 - val_precision_9: 0.7898 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1252 - dice_coef: 0.9660 - iou: 0.9343 - recall_9: 0.9442 - precision_9: 0.9844\n",
            "Epoch 105: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1252 - dice_coef: 0.9660 - iou: 0.9343 - recall_9: 0.9442 - precision_9: 0.9844 - val_loss: 0.6534 - val_dice_coef: 0.7521 - val_iou: 0.6070 - val_recall_9: 0.7194 - val_precision_9: 0.7884 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1224 - dice_coef: 0.9671 - iou: 0.9364 - recall_9: 0.9453 - precision_9: 0.9849\n",
            "Epoch 106: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1224 - dice_coef: 0.9671 - iou: 0.9364 - recall_9: 0.9453 - precision_9: 0.9849 - val_loss: 0.6587 - val_dice_coef: 0.7523 - val_iou: 0.6073 - val_recall_9: 0.7207 - val_precision_9: 0.7869 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1200 - dice_coef: 0.9680 - iou: 0.9381 - recall_9: 0.9461 - precision_9: 0.9855\n",
            "Epoch 107: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1200 - dice_coef: 0.9680 - iou: 0.9381 - recall_9: 0.9461 - precision_9: 0.9855 - val_loss: 0.6620 - val_dice_coef: 0.7525 - val_iou: 0.6075 - val_recall_9: 0.7208 - val_precision_9: 0.7867 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1177 - dice_coef: 0.9689 - iou: 0.9398 - recall_9: 0.9468 - precision_9: 0.9862\n",
            "Epoch 108: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 107ms/step - loss: 0.1177 - dice_coef: 0.9689 - iou: 0.9398 - recall_9: 0.9468 - precision_9: 0.9862 - val_loss: 0.6652 - val_dice_coef: 0.7533 - val_iou: 0.6086 - val_recall_9: 0.7211 - val_precision_9: 0.7879 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1157 - dice_coef: 0.9696 - iou: 0.9411 - recall_9: 0.9475 - precision_9: 0.9864\n",
            "Epoch 109: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1157 - dice_coef: 0.9696 - iou: 0.9411 - recall_9: 0.9475 - precision_9: 0.9864 - val_loss: 0.6661 - val_dice_coef: 0.7535 - val_iou: 0.6089 - val_recall_9: 0.7207 - val_precision_9: 0.7886 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1135 - dice_coef: 0.9704 - iou: 0.9426 - recall_9: 0.9482 - precision_9: 0.9869\n",
            "Epoch 110: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1135 - dice_coef: 0.9704 - iou: 0.9426 - recall_9: 0.9482 - precision_9: 0.9869 - val_loss: 0.6708 - val_dice_coef: 0.7530 - val_iou: 0.6083 - val_recall_9: 0.7225 - val_precision_9: 0.7860 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1117 - dice_coef: 0.9710 - iou: 0.9437 - recall_9: 0.9486 - precision_9: 0.9873\n",
            "Epoch 111: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 12s 118ms/step - loss: 0.1117 - dice_coef: 0.9710 - iou: 0.9437 - recall_9: 0.9486 - precision_9: 0.9873 - val_loss: 0.6705 - val_dice_coef: 0.7544 - val_iou: 0.6101 - val_recall_9: 0.7207 - val_precision_9: 0.7903 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1099 - dice_coef: 0.9715 - iou: 0.9446 - recall_9: 0.9490 - precision_9: 0.9876\n",
            "Epoch 112: val_loss did not improve from 0.64740\n",
            "\n",
            "Epoch 112: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1099 - dice_coef: 0.9715 - iou: 0.9446 - recall_9: 0.9490 - precision_9: 0.9876 - val_loss: 0.6694 - val_dice_coef: 0.7551 - val_iou: 0.6110 - val_recall_9: 0.7195 - val_precision_9: 0.7926 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1086 - dice_coef: 0.9720 - iou: 0.9456 - recall_9: 0.9505 - precision_9: 0.9873\n",
            "Epoch 113: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1086 - dice_coef: 0.9720 - iou: 0.9456 - recall_9: 0.9505 - precision_9: 0.9873 - val_loss: 0.6732 - val_dice_coef: 0.7550 - val_iou: 0.6109 - val_recall_9: 0.7216 - val_precision_9: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 114/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1084 - dice_coef: 0.9721 - iou: 0.9457 - recall_9: 0.9493 - precision_9: 0.9880\n",
            "Epoch 114: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 115ms/step - loss: 0.1084 - dice_coef: 0.9721 - iou: 0.9457 - recall_9: 0.9493 - precision_9: 0.9880 - val_loss: 0.6742 - val_dice_coef: 0.7551 - val_iou: 0.6109 - val_recall_9: 0.7225 - val_precision_9: 0.7900 - lr: 1.0000e-06\n",
            "Epoch 115/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1081 - dice_coef: 0.9722 - iou: 0.9459 - recall_9: 0.9497 - precision_9: 0.9880\n",
            "Epoch 115: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1081 - dice_coef: 0.9722 - iou: 0.9459 - recall_9: 0.9497 - precision_9: 0.9880 - val_loss: 0.6748 - val_dice_coef: 0.7551 - val_iou: 0.6110 - val_recall_9: 0.7233 - val_precision_9: 0.7892 - lr: 1.0000e-06\n",
            "Epoch 116/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1078 - dice_coef: 0.9723 - iou: 0.9461 - recall_9: 0.9493 - precision_9: 0.9882\n",
            "Epoch 116: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1078 - dice_coef: 0.9723 - iou: 0.9461 - recall_9: 0.9493 - precision_9: 0.9882 - val_loss: 0.6755 - val_dice_coef: 0.7550 - val_iou: 0.6108 - val_recall_9: 0.7245 - val_precision_9: 0.7879 - lr: 1.0000e-06\n",
            "Epoch 117/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1075 - dice_coef: 0.9724 - iou: 0.9462 - recall_9: 0.9494 - precision_9: 0.9882\n",
            "Epoch 117: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1075 - dice_coef: 0.9724 - iou: 0.9462 - recall_9: 0.9494 - precision_9: 0.9882 - val_loss: 0.6754 - val_dice_coef: 0.7551 - val_iou: 0.6109 - val_recall_9: 0.7247 - val_precision_9: 0.7878 - lr: 1.0000e-06\n",
            "Epoch 118/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1073 - dice_coef: 0.9725 - iou: 0.9464 - recall_9: 0.9497 - precision_9: 0.9882\n",
            "Epoch 118: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1073 - dice_coef: 0.9725 - iou: 0.9464 - recall_9: 0.9497 - precision_9: 0.9882 - val_loss: 0.6755 - val_dice_coef: 0.7551 - val_iou: 0.6109 - val_recall_9: 0.7255 - val_precision_9: 0.7870 - lr: 1.0000e-06\n",
            "Epoch 119/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1072 - dice_coef: 0.9724 - iou: 0.9464 - recall_9: 0.9497 - precision_9: 0.9882\n",
            "Epoch 119: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 111ms/step - loss: 0.1072 - dice_coef: 0.9724 - iou: 0.9464 - recall_9: 0.9497 - precision_9: 0.9882 - val_loss: 0.6754 - val_dice_coef: 0.7552 - val_iou: 0.6111 - val_recall_9: 0.7255 - val_precision_9: 0.7872 - lr: 1.0000e-06\n",
            "Epoch 120/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1069 - dice_coef: 0.9725 - iou: 0.9466 - recall_9: 0.9496 - precision_9: 0.9884\n",
            "Epoch 120: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1069 - dice_coef: 0.9725 - iou: 0.9466 - recall_9: 0.9496 - precision_9: 0.9884 - val_loss: 0.6757 - val_dice_coef: 0.7551 - val_iou: 0.6110 - val_recall_9: 0.7268 - val_precision_9: 0.7860 - lr: 1.0000e-06\n",
            "Epoch 121/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1067 - dice_coef: 0.9726 - iou: 0.9467 - recall_9: 0.9496 - precision_9: 0.9884\n",
            "Epoch 121: val_loss did not improve from 0.64740\n",
            "98/98 [==============================] - 11s 110ms/step - loss: 0.1067 - dice_coef: 0.9726 - iou: 0.9467 - recall_9: 0.9496 - precision_9: 0.9884 - val_loss: 0.6761 - val_dice_coef: 0.7552 - val_iou: 0.6110 - val_recall_9: 0.7266 - val_precision_9: 0.7862 - lr: 1.0000e-06\n",
            "Epoch 122/200\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.1066 - dice_coef: 0.9726 - iou: 0.9468 - recall_9: 0.9499 - precision_9: 0.9883\n",
            "Epoch 122: val_loss did not improve from 0.64740\n",
            "\n",
            "Epoch 122: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "98/98 [==============================] - 11s 112ms/step - loss: 0.1066 - dice_coef: 0.9726 - iou: 0.9468 - recall_9: 0.9499 - precision_9: 0.9883 - val_loss: 0.6766 - val_dice_coef: 0.7551 - val_iou: 0.6109 - val_recall_9: 0.7271 - val_precision_9: 0.7855 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from keras.metrics import Recall, Precision, MeanIoU\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    \"\"\" Remove folders and files \"\"\"\n",
        "    # os.system(\"rm files/files.csv\")\n",
        "    # os.system(\"rm -r logs\")\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    input_shape = (256, 256, 3)\n",
        "    batch_size = 8\n",
        "    lr = 1e-4\n",
        "    epochs = 200\n",
        "    model_name = \"NanoNet_A_sparseAttention\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "    csv_path = f\"files/{model_name}/model.csv\"\n",
        "    log_path = f\"logs/{model_name}/\"\n",
        "    \"\"\" Creating folders \"\"\"\n",
        "    create_dir(f\"files/{model_name}\")\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    path = '/content/drive/MyDrive/capstone/Kvasir-SEG'\n",
        "\n",
        "    (train_x, train_y), (valid_x, valid_y) = load_data(path)\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
        "\n",
        "    # \"\"\" Model \"\"\"\n",
        "    model = NanoNet_A_sparseAttention(input_shape)\n",
        "\n",
        "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "    model.compile(loss=bce_dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
        "    model.summary()\n",
        "\n",
        "    #\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(log_dir=log_path),\n",
        "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
        "    ]\n",
        "\n",
        "    train_steps = (len(train_x)//batch_size)\n",
        "    valid_steps = (len(valid_x)//batch_size)\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_steps += 1\n",
        "\n",
        "    model.fit(train_dataset,\n",
        "            epochs=epochs,\n",
        "            validation_data=valid_dataset,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_steps=valid_steps,\n",
        "            callbacks=callbacks,\n",
        "            shuffle= False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0qoXyNT9zJX"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJg8vXU79w8y",
        "outputId": "e6ddf2d2-a6a3-458b-c5b9-faa1a7c6647b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju87vqa0ndwg0850onjdz7ol: 0.07498\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju87xn2snfmv0987sc3d9xnq: 0.07686\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju87z6o6nh73085045bzsx6o: 0.07717\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju87zv8lni0o0850hbbecbq6: 0.07728\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8828oxnool0801qno9luhr: 0.07936\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju884985nlmx0817vzpax3y4: 0.07303\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7dp3dw2k4n0755zhe003ad: 0.07468\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dqcwi2dz00850gcmr2ert: 0.07327\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7druhp2gp308715i6km7be: 0.07616\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju7dsrtb2f8i085064kwugfk: 0.07840\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju7dtb1e2j0t0818deq51ib3: 0.07382\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju7dubap2g0w0801fgl42mg9: 0.07709\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju7dvl5m2n4t0755hlnnjjet: 0.08532\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju7dwe282dc309876rco45ts: 0.07158\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju7dxffn2eam0817qxosfwch: 0.08446\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7dymur2od30755eg8yv2ht: 0.07320\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju7dz5yy2i7z0801ausi7rna: 0.11421\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju7ea4om2l910801bohqjccy: 0.07739\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju7ebe962hr409872ovibahw: 0.11182\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "cju7ecl9i2i060987xawjp4l0: 0.11151\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju7eea9b2m0z0801ynqv1fqu: 0.11166\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju88oh0po9gq0801nge4tgr1: 0.10693\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju88q6h6obpd0871ckmiabbo: 0.10535\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju88rl5eo94l0850kf5wtrm1: 0.11495\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju88t4fvokxf07558ymyh281: 0.11268\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju88trl3ogi208716qvti51b: 0.14159\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju88v2f9oi8w0871hx9auh01: 0.10352\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "cju88vx2uoocy075531lc63n3: 0.14446\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju88y1mwoln50871emyfny1g: 0.14100\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju88z8bson4h0871nnd7fdxo: 0.10012\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "cju890guyoiti098753yg6cdu: 0.11093\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju8914beokbf0850isxpocrk: 0.10573\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "cju892fesoq2g0801n0e0jyia: 0.10992\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju893jmdompz0817xn3g1w4h: 0.07118\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju89y9h0puti0818i5yw29e6: 0.07289\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju89z6pqpqfx0817mfv8ixjc: 0.07635\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8a1jtvpt9m081712iwkca7: 0.09935\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8a2itsq4dv0755ntlovpxe: 0.07316\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8a3nhbpwnb0850d37fo2na: 0.07132\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "cju8a56vxpy780850r45yu4wk: 0.06949\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8a84g0q76m0818hwiggkod: 0.07483\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "cju8abobpqbir08189u01huru: 0.07803\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8adb60qbiu080188mxpf8d: 0.06996\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8aeei7q8k308173n9y4klv: 0.06859\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8aj01yqeqm0850lhdz3xdw: 0.11612\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8alhigqn2h0801zksudldd: 0.07475\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8amfdtqi4x09871tygrgqe: 0.07097\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8ando2qqdo0818ck7i1be1: 0.08310\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8apjewqrk00801k5d71gky: 0.07306\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8aqq8uqmoq0987hphto9gg: 0.07413\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8arof2qpf20850ifr1bnqj: 0.07053\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8ashhnquqr0801rwduzt7d: 0.07563\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8at3s1qqqx0850hcq8nmnq: 0.07858\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8auylgqx0z0871u4o4db7o: 0.07958\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8aw9n1qyg10801jkjlmors: 0.07327\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8axq24r4an0755yhv9d4ly: 0.07340\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8ayeq7r1fb0818z1junacy: 0.06950\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8azmhcr66e0755t61atz72: 0.07717\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b0jr0r2oi0801jiquetd5: 0.07184\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8b1v3br45u087189kku66u: 0.07465\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju8b2rmgr52s0801p54eyflx: 0.11924\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8b3ka8r64u0801fh18hk7l: 0.07575\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b4ja9r2s808509d45ma86: 0.07047\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b542nr81x0871uxnkm9ih: 0.07337\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8b5p40r2c60987ofa0mu03: 0.07025\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8b6rp0r5st0850184f79xt: 0.07695\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8b7aqtr4a00987coba14b7: 0.07296\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8b8yair65w09878pyqtr96: 0.07125\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8bafgqrf4x0818twisk3ea: 0.07601\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bbznkrf5g0871jncffynk: 0.06997\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bff9nrfi10850fmfzbf8v: 0.07223\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8bgdmqrksy0801tozdmraa: 0.07163\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bh8surexp0987o5pzklk1: 0.06951\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8bi8q7rlmn0871abc5ch8k: 0.07164\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bj2ssrmlm0871gc2ug2rs: 0.07110\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bk8oirjhw0817hgkua2w8: 0.07210\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8bljw9rqk20801kr54akrl: 0.08029\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8bm24yrrdp081829mbo8ic: 0.07134\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bn7m2rmm70817hgxpb1uq: 0.07329\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bop5jrsid08716i24fqda: 0.07315\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bpctzrqkr0850zeldv9kt: 0.07404\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bqxxurs6i0850mu7mtef9: 0.07473\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "cju8brv16rx7f0818uf5n89pv: 0.07599\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju8bssulrrcy0987h1vq5060: 0.07594\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8buos5rz9b08715lfr0f4f: 0.07924\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "cju8bw697rwg308177tg8huas: 0.08023\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju8bysfgrzkl081786jwac09: 0.07406\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8bzzy2s66m08016z6mouqt: 0.07583\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju8c1a0ws7o208181c6lbsom: 0.07565\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju8c2rqzs5t80850d0zky5dy: 0.11274\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju8c3xs7sauj0801ieyzezr5: 0.06912\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "cju8c5223s8j80850b4kealt4: 0.09018\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "cju8c5mxls96t0850wvkvsity: 0.11111\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju8c5zcbsdfz0801o5t6jag1: 0.11070\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju8c6hnxsdvr0801wn0vrsa6: 0.11221\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "cju8c82iosagu0817l74s4m5g: 0.11286\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "cju8c9akjsdjj0850s67uzlxq: 0.11590\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju8ca4geseia0850i2ru11hw: 0.10446\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju8cattbsivm0818p446wgel: 0.10609\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "cju8cbsyssiqj0871gr4jedjp: 0.10514\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju7efffp2ivf0817etg3jehl: 0.12032\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6x35ervu2808015c7eoqe4: 0.10532\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "cju6x4t13vyw60755gtcf9ndu: 0.11764\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "cju6x97w4vwua0850x0997r0a: 0.10620\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "cju6xa0qmvzun0818xjukgncj: 0.13712\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "cju6xifswvwbo0987nibtdr50: 0.10791\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju6xlygpw7bs0818n691jsq4: 0.07162\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju6xmqd9w0250817l5kxfnsk: 0.06825\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju6ywm40wdbo0987pbftsvtg: 0.11884\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "cju6yxyt0wh080871sqpepu47: 0.07342\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju6yywx1whbb0871ksgfgf9f: 0.07297\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju6z1bzbwfq50817b2alatvr: 0.07272\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju6z2616wqbk07555bvnuyr1: 0.07399\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju6z600qwh4z081700qimgl9: 0.07383\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cju6z7e4bwgdd0987ogkzq9kt: 0.07875\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "cju6z9a9kwsl007552s49rx6i: 0.07224\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "cju76erapykj30871x5eaxh4q: 0.08291\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "cju76l27oyrw907551ri2a7fl: 0.07448\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "cju76lsehyia10987u54vn8rb: 0.07329\n",
            "\n",
            "Jaccard: 0.7108 - F1: 0.8064 - Recall: 0.8174 - Precision: 0.8574 - Acc: 0.9505 - F2: 0.8051\n",
            "Mean FPS:  11.60702818398892\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import time\n",
        "from operator import add\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    jaccard_score, f1_score, recall_score, precision_score, accuracy_score, fbeta_score)\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.reshape(-1)\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "\n",
        "    ## Score\n",
        "    score_jaccard = jaccard_score(y_true, y_pred, average='binary')\n",
        "    score_f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    score_recall = recall_score(y_true, y_pred, average='binary')\n",
        "    score_precision = precision_score(y_true, y_pred, average='binary', zero_division=1)\n",
        "    score_acc = accuracy_score(y_true, y_pred)\n",
        "    score_fbeta = fbeta_score(y_true, y_pred, beta=2.0, average='binary', zero_division=1)\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc, score_fbeta]\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Load dataset \"\"\"\n",
        "    path = \"/content/drive/MyDrive/capstone/Kvasir-SEG\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_test_data(path)\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    size = (256, 256)\n",
        "    input_shape = (256, 256, 3)\n",
        "    model_name = \"NanoNet_A_sparseAttention\"\n",
        "    model_path = f\"files/{model_name}/model.h5\"\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    create_dir(f\"results/{model_name}\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    model = load_model_file(model_path)\n",
        "\n",
        "    \"\"\" Sample prediction: To improve FPS \"\"\"\n",
        "    image = np.zeros((1, 256, 256, 3))\n",
        "    mask = model.predict(image)\n",
        "\n",
        "    \"\"\" Testing \"\"\"\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    time_taken = []\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(test_x, test_y)):\n",
        "        name = y.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Image \"\"\"\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        image = cv2.resize(image, size)\n",
        "        ori_img = image\n",
        "        image = image/255.0\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image.astype(np.float32)\n",
        "\n",
        "        \"\"\" Mask \"\"\"\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, size)\n",
        "        ori_mask = mask\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        mask = mask/255.0\n",
        "        mask = mask.astype(np.float32)\n",
        "\n",
        "        \"\"\" Time taken \"\"\"\n",
        "        start_time = time.time()\n",
        "        pred_y = model.predict(image)\n",
        "        total_time = time.time() - start_time\n",
        "        time_taken.append(total_time)\n",
        "        print(f\"{name}: {total_time:1.5f}\")\n",
        "\n",
        "        \"\"\" Metrics calculation \"\"\"\n",
        "        score = calculate_metrics(mask, pred_y)\n",
        "        metrics_score = list(map(add, metrics_score, score))\n",
        "\n",
        "        \"\"\" Saving masks \"\"\"\n",
        "        pred_y = pred_y[0] > 0.5\n",
        "        pred_y = pred_y * 255\n",
        "        pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "        ori_img = ori_img\n",
        "        ori_mask = mask_parse(ori_mask)\n",
        "        pred_y = mask_parse(pred_y)\n",
        "        sep_line = np.ones((size[0], 10, 3)) * 255\n",
        "\n",
        "        tmp = [\n",
        "            ori_img, sep_line,\n",
        "            ori_mask, sep_line,\n",
        "            pred_y\n",
        "        ]\n",
        "\n",
        "        cat_images = np.concatenate(tmp, axis=1)\n",
        "        cv2.imwrite(f\"results/{model_name}/{name}.png\", cat_images)\n",
        "\n",
        "    jaccard = metrics_score[0]/len(test_x)\n",
        "    f1 = metrics_score[1]/len(test_x)\n",
        "    recall = metrics_score[2]/len(test_x)\n",
        "    precision = metrics_score[3]/len(test_x)\n",
        "    acc = metrics_score[4]/len(test_x)\n",
        "    f2 = metrics_score[5]/len(test_x)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f} - F2: {f2:1.4f}\")\n",
        "\n",
        "    mean_time_taken = np.mean(time_taken)\n",
        "    mean_fps = 1/mean_time_taken\n",
        "    print(\"Mean FPS: \", mean_fps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
